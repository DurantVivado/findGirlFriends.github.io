

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=&quot;auto&quot;>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png">
  <link rel="icon" type="image/png" href="/img/favicon.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="博主：来自华中科技大学国家光电研究中心的">
  <meta name="author" content="Durant">
  <meta name="keywords" content="">
  <title>CEPH 基础介绍 - Durant Thorvalds 的米奇妙妙屋</title>

  <link  rel="stylesheet" href="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.staticfile.org/github-markdown-css/4.0.0/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.staticfile.org/highlight.js/10.0.0/styles/github-gist.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.staticfile.org/gitalk/1.6.2/gitalk.css" />
  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_pf9vaxs7x7b.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 5.2.0"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>Fluid</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                联系我
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/links/">
                <i class="iconfont icon-link-fill"></i>
                友链
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;</a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" href="javascript:">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner intro-2" id="background" parallax=true
         style="background: url('/img/default.png') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="container page-header text-center fade-in-up">
            <span class="h2" id="subtitle">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2020-11-22 00:00" pubdate>
        2020年11月22日 凌晨
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      12k 字
    </span>
  

  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      136
       分钟
    </span>
  

  
  
    
      <!-- 不蒜子统计文章PV -->
      <span id="busuanzi_container_page_pv" style="display: none">
        <i class="iconfont icon-eye" aria-hidden="true"></i>
        <span id="busuanzi_value_page_pv"></span> 次
      </span>
    
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid">
  <div class="row">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-md">
      <div class="container nopadding-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto" id="post">
            <!-- SEO header -->
            <h1 style="display: none">CEPH 基础介绍</h1>
            
            <div class="markdown-body" id="post-body">
              <h1 id="CEPH基础理论学习"><a href="#CEPH基础理论学习" class="headerlink" title="CEPH基础理论学习"></a>CEPH基础理论学习</h1><hr>
<h1 id="第一部分"><a href="#第一部分" class="headerlink" title="第一部分"></a>第一部分</h1><h2 id="1-Ceph简介"><a href="#1-Ceph简介" class="headerlink" title="1 Ceph简介"></a><strong>1 Ceph简介</strong></h2><blockquote>
<p>Ceph是一个统一的分布式存储系统，设计初衷是提供较好的性能、可靠性和可扩展性。它是一个统一的存储系统，既支持传统的块、文件存储协议，例如SAN和NAS，也支持新兴的对象存储协议，如S3和Swift，这使得Ceph理论上可以满足时下一切主流的存储应用的要求。</p>
</blockquote>
<p>Ceph项目最早起源于Sage就读博士期间的工作（最早的成果于2004年发表），并随后贡献给开源社区。在经过了数年的发展之后，目前已得到众多云计算厂商的支持并被广泛应用。RedHat及OpenStack都可与Ceph整合以支持虚拟机镜像的后端存储。</p>
<ul>
<li>Ceph摒弃了传统的集中式存储元数据的方案，采用CRUSH算法，数据分布均衡，并行度高。</li>
<li>考虑了容灾区的隔离，能够实现各类负载的副本放置规则，例如跨机房，机架感知。</li>
<li>能够支持上千个存储节点的规模，支持TB到PB级的数据。</li>
</ul>
<p><strong>高可用性</strong></p>
<ul>
<li>a. 副本数可以灵活控制。</li>
<li>b. 支持故障域分隔，数据强一致性。</li>
<li>c. 多种故障场景自动进行修复自愈。</li>
<li>d. 没有单点故障，自动管理。</li>
</ul>
<p><strong>高可扩展性</strong></p>
<ul>
<li>a. 去中心化。</li>
<li>b. 扩展灵活。</li>
<li>c. 随着节点增加而线性增长。</li>
</ul>
<p><strong>特性丰富</strong></p>
<p>a. 支持三种存储接口：块存储、文件存储、对象存储。</p>
<p>b. 支持自定义接口，支持多种语言驱动。</p>
<p>特点：</p>
<ul>
<li><p>高性能</p>
</li>
<li><p>高可用性</p>
</li>
<li><p>高可扩展性</p>
</li>
<li><p>特性丰富</p>
</li>
</ul>
<p><strong>支持三种接口</strong>：</p>
<ul>
<li>Object：有原生的API，而且也兼容Swift和S3的API。</li>
<li>Block：支持精简配置、快照、克隆。</li>
<li>File：Posix接口，支持快照。</li>
</ul>
<p><img src="\img\ceph-st1.jpg" srcset="/img/loading.gif" alt="640?wx_fmt=png"></p>
<ul>
<li><p>Monitor</p>
<p><code>ceph-mon</code>，一个Ceph集群需要多个Monitor组成的小集群，它们通过Paxos同步数据，用来保存OSD的元数据。<a target="_blank" rel="noopener" href="http://docs.ceph.org.cn/glossary/#term-ceph-monitor"><em>Ceph Monitor</em></a>维护着展示集群状态的各种图表，包括监视器图、 OSD 图、归置组（ PG ）图、和 CRUSH 图。 Ceph 保存着发生在Monitors 、 OSD 和 PG上的每一次状态变更的历史信息（称为 epoch ）。通常至少需要三个监视器才能实现冗余和高可用性。</p>
</li>
<li><p>Manager</p>
<p><a target="_blank" rel="noopener" href="https://docs.ceph.com/en/latest/glossary/#term-Ceph-Manager">Ceph Manager</a>守护进程（<code>ceph-mgr</code>）负责跟踪运行时指标和Ceph集群的当前状态，包括存储利用率，当前性能指标和系统负载。Ceph Manager守护进程还托管基于python的模块，以管理和公开Ceph集群信息，包括基于Web的<a target="_blank" rel="noopener" href="https://docs.ceph.com/en/latest/mgr/dashboard/#mgr-dashboard">Ceph仪表板</a>和 <a target="_blank" rel="noopener" href="https://docs.ceph.com/en/latest/mgr/restful">REST API</a>。通常，至少需要两个管理器才能实现高可用性。</p>
</li>
<li><p>OSD</p>
<p>OSD全称Object Storage Daemon（<code>ceph-osd</code>），也就是负责响应客户端请求返回具体数据的进程。一个Ceph集群一般都有很多个OSD。<a target="_blank" rel="noopener" href="http://docs.ceph.org.cn/glossary/#term-56"><em>Ceph OSD 守护进程</em></a>（ Ceph OSD ）的功能是存储数据，处理数据的复制、恢复、回填、再均衡，并通过检查其他OSD 守护进程的心跳来向 Ceph Monitors 提供一些监控信息。当 Ceph 存储集群设定为有2个副本时，至少需要2个 OSD 守护进程，集群才能达到 <code>active+clean</code> 状态（ Ceph 默认有3个副本，但你可以调整副本数）</p>
</li>
</ul>
<ul>
<li><p>MDS</p>
<p>MDS全称Ceph Metadata Server（<code>ceph-mds</code>），是CephFS服务依赖的元数据服务。<a target="_blank" rel="noopener" href="http://docs.ceph.org.cn/glossary/#term-63"><em>Ceph 元数据服务器</em></a>（ MDS ）为 <a target="_blank" rel="noopener" href="http://docs.ceph.org.cn/glossary/#term-45"><em>Ceph 文件系统</em></a>存储元数据（也就是说，Ceph 块设备和 Ceph 对象存储不使用MDS ）。元数据服务器使得 POSIX 文件系统的用户们，可以在不对 Ceph 存储集群造成负担的前提下，执行诸如 <code>ls</code>、<code>find</code> 等基本命令。</p>
</li>
</ul>
<ul>
<li><p>Object</p>
<p>Ceph最底层的存储单元是Object对象，每个Object包含元数据和原始数据。</p>
</li>
</ul>
<ul>
<li><p>PG</p>
<p>PG全称Placement Groups归置组，是一个逻辑的概念，一个PG包含多个OSD。引入PG这一层其实是为了更好的分配数据和定位数据。</p>
</li>
</ul>
<ul>
<li><p>RADOS</p>
<p>RADOS全称Reliable Autonomic Distributed Object Store （可靠自治的分布式对象存储），是Ceph集群的<strong>精华</strong>，用户实现数据分配、Failover等集群操作。具有自愈，自管理能力的智能存储节点构建的高可靠，自治，分布式对象存储系统。</p>
</li>
</ul>
<ul>
<li><p>Libradio</p>
<p>Librados是Rados提供库，因为RADOS是协议很难直接访问，因此上层的RBD、RGW和CephFS都是通过librados访问的，目前提供PHP、Ruby、Java、Python、C和C++支持。</p>
</li>
</ul>
<ul>
<li><p>CRUSH<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="  Weil S A Brandt S A , Miller E L , et al. CRUSH: Controlled, Scalable, Decentralized Placement of Replicated Data[C]// IEEE Sc Conference. ACM, 2006.
">[1]</span></a></sup></p>
<p>CRUSH是Ceph使用的数据分布算法，类似一致性哈希，让数据分配到预期的地方。</p>
</li>
</ul>
<ul>
<li><p>RBD</p>
<p>RBD全称RADOS block device，是Ceph对外提供的块设备服务。采用全分布式，可靠的块设备访问接口，同时提供Linux内核态和用户态客户端访问支持，以及QEMU/KVM驱动。</p>
</li>
</ul>
<ul>
<li><p>RGW</p>
<p>RGW全称RADOS gateway，基于Bucket的REST网关，是Ceph对外提供的对象存储服务，接口与S3和Swift兼容。</p>
</li>
</ul>
<ul>
<li><p>CephFS</p>
<p>CephFS全称Ceph File System，是Ceph对外提供的文件系统服务。与POSIX兼容，同时提供Linux内核态用户端和FUSE访问支持。</p>
</li>
</ul>
<blockquote>
<p>基于 RADOS 的 Ceph 对象存储集群包括两类守护进程：term:对象存储守护进程（ OSD ）把存储节点上的数据存储为对象； term:Ceph 监视器（ MON ）维护集群运行图的主拷贝。一个 Ceph 集群可以包含数千个存储节点，最简系统至少需要一个监视器和两个 OSD 才能做到数据复制。</p>
</blockquote>
<h3 id="ceph读写流程"><a href="#ceph读写流程" class="headerlink" title="ceph读写流程"></a>ceph读写流程</h3><p>Read：</p>
<ul>
<li>Client app 发送读请求，RADOS将请求发送给Primary OSD。</li>
<li>主要OSD在本地磁盘读数据并完成读请求。</li>
</ul>
<p>Write:</p>
<ul>
<li><p>Client App 写数据，RADOS将数据发送给Primary OSD。</p>
</li>
<li><p>Primary OSD识别Replica OSDs并且向他们发送数据，由他们写数据到本地磁盘。</p>
</li>
<li>Replica OSDs 完成写并通知Primary OSD。</li>
<li>Primary OSDs 通知client APP 写完成。</li>
<li><img src="\img\ceph-b.png" srcset="/img/loading.gif" alt="image-20201122180847737"></li>
</ul>
<h3 id="三种存储方式"><a href="#三种存储方式" class="headerlink" title="三种存储方式"></a>三种存储方式</h3><h4 id="1-块设备"><a href="#1-块设备" class="headerlink" title="1. 块设备"></a>1. 块设备</h4><p><strong>典型设备：</strong> 磁盘阵列，硬盘</p>
<p>主要是将裸磁盘空间映射给主机使用的。</p>
<p><strong>优点：</strong></p>
<ul>
<li>通过RAID与LVM（逻辑卷管理）等手段，对数据提供了保护。</li>
<li>多块廉价的硬盘组合起来，提高容量。</li>
<li>多块磁盘组合出来的逻辑盘，提升读写效率。</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li>采用SAN架构组网时，光纤交换机，造价成本高。</li>
<li>主机之间无法共享数据。</li>
</ul>
<p><strong>使用场景：</strong></p>
<ul>
<li>docker容器、虚拟机磁盘存储分配。</li>
<li>日志存储。</li>
<li>文件存储。</li>
<li>…</li>
</ul>
<h4 id="2-文件存储"><a href="#2-文件存储" class="headerlink" title="2.文件存储"></a>2.文件存储</h4><p><strong>典型设备：</strong> FTP、NFS服务器<br> 为了克服块存储文件无法共享的问题，所以有了文件存储。<br> 在服务器上架设FTP与NFS服务，就是文件存储。</p>
<p><strong>优点：</strong></p>
<ul>
<li>造价低，随便一台机器就可以了。</li>
<li>方便文件共享。</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li>读写速率低。</li>
<li>传输速率慢。</li>
</ul>
<p><strong>使用场景：</strong></p>
<ul>
<li>日志存储。</li>
<li>有目录结构的文件存储。</li>
<li>…</li>
</ul>
<h4 id="3-对象存储"><a href="#3-对象存储" class="headerlink" title="3.对象存储"></a>3.对象存储</h4><p><img src="\img\ceph-c.jpg" srcset="/img/loading.gif" alt="img"></p>
<p><strong>典型设备：</strong> 内置大容量硬盘的分布式服务器(swift, s3)<br> 多台服务器内置大容量硬盘，安装上对象存储管理软件，对外提供读写访问功能。</p>
<p><strong>优点：</strong></p>
<ul>
<li>具备块存储的读写高速。</li>
<li>具备文件存储的共享等特性。</li>
</ul>
<p><strong>使用场景：</strong> (适合更新变动较少的数据)</p>
<ul>
<li>图片存储。</li>
<li>视频存储。</li>
<li>…</li>
</ul>
<h3 id="Ceph硬件要求"><a href="#Ceph硬件要求" class="headerlink" title="Ceph硬件要求"></a>Ceph硬件要求</h3><p>Ceph 为普通硬件设计，这可使构建、维护 PB 级数据集群的费用相对低廉。规划集群硬件时，需要均衡几方面的因素，包括区域失效和潜在的性能问题。硬件规划要包含把使用 Ceph 集群的 Ceph 守护进程和其他进程恰当分布。通常，我们推荐在一台机器上只运行一种类型的守护进程。我们推荐把使用数据集群的进程（如 OpenStack 、 CloudStack 等）安装在别的机器上。</p>
<blockquote>
<p>关于 Ceph 的高品质博客文章也值得参考，比如 <a target="_blank" rel="noopener" href="http://ceph.com/community/ceph-performance-part-1-disk-controller-write-throughput/">Ceph Write Throughput 1</a> 、 <a target="_blank" rel="noopener" href="http://ceph.com/community/ceph-performance-part-2-write-throughput-without-ssd-journals/">Ceph Write Throughput 2</a> 、 <a target="_blank" rel="noopener" href="http://ceph.com/uncategorized/argonaut-vs-bobtail-performance-preview/">Argonaut v. Bobtail Performance Preview</a> 、 <a target="_blank" rel="noopener" href="http://ceph.com/community/ceph-bobtail-performance-io-scheduler-comparison/">Bobtail Performance - I/O Scheduler Comparison</a> 。</p>
</blockquote>
<h4 id="CPU"><a href="#CPU" class="headerlink" title="CPU"></a>CPU</h4><p>Ceph 元数据服务器对 CPU 敏感，它会动态地重分布它们的负载，所以你的元数据服务器应该有足够的处理能力（如 4 核或更强悍的 CPU ）。 Ceph 的 OSD 运行着 <a target="_blank" rel="noopener" href="http://docs.ceph.org.cn/glossary/#term-rados"><em>RADOS</em></a> 服务、用 <a target="_blank" rel="noopener" href="http://docs.ceph.org.cn/glossary/#term-crush"><em>CRUSH</em></a> 计算数据存放位置、复制数据、维护它自己的集群运行图副本，因此 OSD 需要一定的处理能力（如双核 CPU ）。监视器只简单地维护着集群运行图的副本，因此对 CPU 不敏感；但必须考虑机器以后是否还会运行 Ceph 监视器以外的 CPU 密集型任务。例如，如果服务器以后要运行用于计算的虚拟机（如 OpenStack Nova ），你就要确保给 Ceph 进程保留了足够的处理能力，所以我们推荐在其他机器上运行 CPU 密集型任务。</p>
<h4 id="RAM内存"><a href="#RAM内存" class="headerlink" title="RAM内存"></a>RAM内存</h4><p>元数据服务器和监视器必须可以尽快地提供它们的数据，所以他们应该有足够的内存，至少每进程 1GB 。 OSD 的日常运行不需要那么多内存（如每进程 500MB ）差不多了；然而在恢复期间它们占用内存比较大（如每进程每 TB 数据需要约 1GB 内存）。通常内存越多越好。</p>
<p>元数据服务器和监视器必须可以尽快地提供它们的数据，所以他们应该有足够的内存，至少每进程 1GB 。 OSD 的日常运行不需要那么多内存（如每进程 500MB ）差不多了；然而在恢复期间它们占用内存比较大（如每进程每 TB 数据需要约 1GB 内存）。通常内存越多越好。</p>
<blockquote>
<p>因为 Ceph 发送 ACK 前必须把所有数据写入日志（至少对 xfs 和 ext4 来说是），因此均衡日志和 OSD 性能相当重要。</p>
</blockquote>
<h4 id="硬盘驱动器"><a href="#硬盘驱动器" class="headerlink" title="硬盘驱动器"></a>硬盘驱动器</h4><p>OSD 应该有足够的空间用于存储对象数据。考虑到大硬盘的每 GB 成本，我们建议用容量大于 1TB 的硬盘。</p>
<ul>
<li><p>不顾分区而在单个硬盘上运行多个OSD，这样<strong>不明智</strong>！</p>
</li>
<li><p>不顾分区而在运行了OSD的硬盘上同时运行监视器或元数据服务器也<strong>不明智</strong>！</p>
</li>
</ul>
<p>存储驱动器受限于寻道时间、访问时间、读写时间、还有总吞吐量，这些物理局限性影响着整体系统性能，尤其在系统恢复期间。因此我们推荐独立的驱动器用于安装操作系统和软件，另外每个 OSD 守护进程占用一个驱动器。大多数 “slow OSD”问题的起因都是在相同的硬盘上运行了操作系统、多个 OSD 、和/或多个日志文件。鉴于解决性能问题的成本差不多会超过另外增加磁盘驱动器，你应该在设计时就避免增加 OSD 存储驱动器的负担来提升性能。</p>
<p>Ceph 允许你在每块硬盘驱动器上运行多个 OSD ，但这会导致资源竞争并降低总体吞吐量； Ceph 也允许把日志和对象数据存储在相同驱动器上，但这会增加记录写日志并回应客户端的延时，因为 Ceph 必须先写入日志才会回应确认了写动作。 btrfs 文件系统能同时写入日志数据和对象数据， xfs 和 ext4 却不能。</p>
<p>SSD总体性能优于HDD</p>
<ul>
<li><strong>写密集语义：</strong> 记日志涉及写密集语义，所以你要确保选用的 SSD 写入性能和硬盘相当或好于硬盘。廉价 SSD 可能在加速访问的同时引入写延时，有时候高性能硬盘的写入速度可以和便宜 SSD 相媲美。</li>
<li><strong>顺序写入：</strong> 在一个 SSD 上为多个 OSD 存储多个日志时也必须考虑 SSD 的顺序写入极限，因为它们要同时处理多个 OSD 日志的写入请求。</li>
<li><strong>分区对齐：</strong> 采用了 SSD 的一个常见问题是人们喜欢分区，却常常忽略了分区对齐，这会导致 SSD 的数据传输速率慢很多，所以请确保分区对齐了。</li>
</ul>
<p>SSD 用于对象存储太昂贵了，但是把 OSD 的日志存到 SSD 、把对象数据存储到独立的硬盘可以明显提升性能。 <code>osd journal</code> 选项的默认值是 <code>/var/lib/ceph/osd/$cluster-$id/journal</code> ，你可以把它挂载到一个 SSD 或 SSD 分区，这样它就不再是和对象数据一样存储在同一个硬盘上的文件了。</p>
<p>提升 CephFS 文件系统性能的一种方法是从 CephFS 文件内容里分离出元数据。 Ceph 提供了默认的 <code>metadata</code> 存储池来存储 CephFS 元数据，所以你不需要给 CephFS 元数据创建存储池，但是可以给它创建一个仅指向某主机 SSD 的 CRUSH 运行图。详情见<a target="_blank" rel="noopener" href="http://ceph.com/docs/master/rados/operations/crush-map/#placing-different-pools-on-different-osds">给存储池指定 OSD</a> 。</p>
<p>你可以在同一主机上运行多个 OSD ，但要确保 OSD 硬盘总吞吐量不超过为客户端提供读写服务所需的网络带宽；还要考虑集群在每台主机上所存储的数据占总体的百分比，如果一台主机所占百分比太大而它挂了，就可能导致诸如超过 <code>full ratio</code> 的问题，此问题会使 Ceph 中止运作以防数据丢失。</p>
<p>如果每台主机运行多个 OSD ，也得保证内核是最新的。参阅<a target="_blank" rel="noopener" href="http://docs.ceph.org.cn/start/os-recommendations">操作系统推荐</a>里关于 <code>glibc</code> 和 <code>syncfs(2)</code> 的部分，确保硬件性能可达期望值。</p>
<p>OSD 数量较多（如 20 个以上）的主机会派生出大量线程，尤其是在恢复和重均衡期间。很多 Linux 内核默认的最大线程数较小（如 32k 个），如果您遇到了这类问题，可以把 <code>kernel.pid_max</code> 值调高些。理论最大值是 4194303 。例如把下列这行加入 <code>/etc/sysctl.conf</code> 文件：</p>
<pre><code class="hljs ini"><span class="hljs-attr">kernel.pid_max</span> = <span class="hljs-number">4194303</span></code></pre>
<h4 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h4><p>​        建议每台机器最少两个千兆网卡，现在大多数机械硬盘都能达到大概 100MB/s 的吞吐量，网卡应该能处理所有 OSD 硬盘总吞吐量，所以推荐最少两个千兆网卡，分别用于公网（前端）和集群网络（后端）。集群网络（最好别连接到国际互联网）用于处理由数据复制产生的额外负载，而且可防止拒绝服务攻击，拒绝服务攻击会干扰数据归置组，使之在 OSD 数据复制时不能回到 <code>active + clean</code> 状态。请考虑部署万兆网卡。通过 1Gbps 网络复制 1TB 数据耗时 3 小时，而 3TB （典型配置）需要 9 小时，相比之下，如果使用 10Gbps 复制时间可分别缩减到 20 分钟和 1 小时。在一个 PB 级集群中， OSD 磁盘失败是常态，而非异常；在性价比合理的的前提下，系统管理员想让 PG 尽快从 <code>degraded</code> （降级）状态恢复到 <code>active + clean</code> 状态。另外，一些部署工具（如 Dell 的 Crowbar ）部署了 5 个不同的网络，但使用了 VLAN 以提高网络和硬件可管理性。 VLAN 使用 802.1q 协议，还需要采用支持 VLAN 功能的网卡和交换机，增加的硬件成本可用节省的运营（网络安装、维护）成本抵消。使用 VLAN 来处理集群和计算栈（如 OpenStack 、 CloudStack 等等）之间的 VM 流量时，采用 10G 网卡仍然值得。每个网络的机架路由器到核心路由器应该有更大的带宽，如 40Gbps 到 100Gbps 。</p>
<p>​        服务器应配置底板管理控制器（ Baseboard Management Controller, BMC ），管理和部署工具也应该大规模使用 BMC ，所以请考虑带外网络管理的成本/效益平衡，此程序管理着 SSH 访问、 VM 映像上传、操作系统安装、端口管理、等等，会徒增网络负载。运营 3 个网络有点过分，但是每条流量路径都指示了部署一个大型数据集群前要仔细考虑的潜能力、吞吐量、性能瓶颈。</p>
<p>Ceph 可以运行在廉价的普通硬件上，小型生产集群和开发集群可以在一般的硬件上。</p>
<p>如果在只有一块硬盘的机器上运行 OSD ，要把数据和操作系统分别放到不同分区；一般来说，我们推荐操作系统和数据分别使用不同的硬盘。</p>
<h4 id="最低硬件要求"><a href="#最低硬件要求" class="headerlink" title="最低硬件要求"></a>最低硬件要求</h4><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">进程</th>
<th style="text-align:left">条件</th>
<th style="text-align:left">最低建议</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>ceph-osd</code></td>
<td style="text-align:left">Processor</td>
<td style="text-align:left">1x 64-bit AMD-641x 32-bit ARM dual-core or better1x i386 dual-core</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">RAM</td>
<td style="text-align:left">~1GB for 1TB of storage per daemon</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">Volume Storage</td>
<td style="text-align:left">1x storage drive per daemon</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">Journal</td>
<td style="text-align:left">1x SSD partition per daemon (optional)</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">Network</td>
<td style="text-align:left">2x 1GB Ethernet NICs</td>
</tr>
<tr>
<td style="text-align:left"><code>ceph-mon</code></td>
<td style="text-align:left">Processor</td>
<td style="text-align:left">1x 64-bit AMD-64/i3861x 32-bit ARM dual-core or better1x i386 dual-core</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">RAM</td>
<td style="text-align:left">1 GB per daemon</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">Disk Space</td>
<td style="text-align:left">10 GB per daemon</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">Network</td>
<td style="text-align:left">2x 1GB Ethernet NICs</td>
</tr>
<tr>
<td style="text-align:left"><code>ceph-mds</code></td>
<td style="text-align:left">Processor</td>
<td style="text-align:left">1x 64-bit AMD-64 quad-core1x 32-bit ARM quad-core1x i386 quad-core</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">RAM</td>
<td style="text-align:left">1 GB minimum per daemon</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">Disk Space</td>
<td style="text-align:left">1 MB per daemon</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">Network</td>
<td style="text-align:left">2x 1GB Ethernet NICs</td>
</tr>
</tbody>
</table>
</div>
<h4 id="生产实例"><a href="#生产实例" class="headerlink" title="生产实例"></a>生产实例</h4><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">Configuration</th>
<th style="text-align:left">Criteria</th>
<th style="text-align:left">Minimum Recommended</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Dell PE R510</td>
<td style="text-align:left">Processor</td>
<td style="text-align:left">2x 64-bit quad-core Xeon CPUs</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">RAM</td>
<td style="text-align:left">16 GB</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">Volume Storage</td>
<td style="text-align:left">8x 2TB drives. 1 OS, 7 Storage</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">Client Network</td>
<td style="text-align:left">2x 1GB Ethernet NICs</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">OSD Network</td>
<td style="text-align:left">2x 1GB Ethernet NICs</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">Mgmt. Network</td>
<td style="text-align:left">2x 1GB Ethernet NICs</td>
</tr>
<tr>
<td style="text-align:left">Dell PE R515</td>
<td style="text-align:left">Processor</td>
<td style="text-align:left">1x hex-core Opteron CPU</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">RAM</td>
<td style="text-align:left">16 GB</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">Volume Storage</td>
<td style="text-align:left">12x 3TB drives. Storage</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">OS Storage</td>
<td style="text-align:left">1x 500GB drive. Operating System.</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">Client Network</td>
<td style="text-align:left">2x 1GB Ethernet NICs</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">OSD Network</td>
<td style="text-align:left">2x 1GB Ethernet NICs</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left">Mgmt. Network</td>
<td style="text-align:left">2x 1GB Ethernet NICs</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h2 id="2-Ceph-I-O流程和数据分布"><a href="#2-Ceph-I-O流程和数据分布" class="headerlink" title="2 Ceph I/O流程和数据分布"></a>2 Ceph I/O流程和数据分布</h2><p><img src="\img\ceph-d.png" srcset="/img/loading.gif" alt="img"></p>
<p><img src="\img\ceph_io2.png" srcset="/img/loading.gif" alt="img"></p>
<p><strong>步骤：</strong></p>
<ol>
<li>client 创建cluster handler。</li>
<li>client 读取配置文件。</li>
<li>client 连接上monitor，获取集群map信息。</li>
<li>client 读写io 根据crushmap 算法请求对应的主osd数据节点。</li>
<li>主osd数据节点同时写入另外两个副本节点数据。</li>
<li>等待主节点以及另外两个副本节点写完数据状态。</li>
<li>主节点及副本节点写入状态都成功后，返回给client，io写入完成。</li>
</ol>
<h3 id="新主I-O流程图"><a href="#新主I-O流程图" class="headerlink" title="新主I/O流程图"></a>新主I/O流程图</h3><p><img src="\img\ceph-e.jpg" srcset="/img/loading.gif" alt="img"></p>
<p><strong>步骤：</strong></p>
<ol>
<li>client连接monitor获取集群map信息。</li>
<li>同时新主osd1由于没有pg数据会主动上报monitor告知让osd2临时接替为主。</li>
<li>临时主osd2会把数据全量同步给新主osd1。</li>
<li>client IO读写直接连接临时主osd2进行读写。</li>
<li>osd2收到读写io，同时写入另外两副本节点。</li>
<li>等待osd2以及另外两副本写入成功。</li>
<li>osd2三份数据都写入成功返回给client, 此时client io读写完毕。</li>
<li>如果osd1数据同步完毕，临时主osd2会交出主角色。</li>
<li>osd1成为主节点，osd2变成副本。</li>
</ol>
<h3 id="Ceph-I-O算法流程"><a href="#Ceph-I-O算法流程" class="headerlink" title="Ceph I/O算法流程"></a>Ceph I/O算法流程</h3><p><img src="\img\ceph-arch.png" srcset="/img/loading.gif" alt="img"></p>
<ol>
<li>File用户需要读写的文件。File-&gt;Object映射：</li>
</ol>
<ul>
<li>a. ino (File的元数据，File的唯一id)。</li>
<li>b. ono(File切分产生的某个object的序号，默认以4M切分一个块大小)。</li>
<li>c. oid(object id: ino + ono)。</li>
</ul>
<ol>
<li>Object是RADOS需要的对象。Ceph指定一个静态hash函数计算oid的值，将oid映射成一个近似均匀分布的伪随机值，然后和mask按位相与，得到pgid。Object-&gt;PG映射：</li>
</ol>
<ul>
<li>a. hash(oid) &amp; mask-&gt; pgid 。</li>
<li>b. mask = PG总数m(m为2的整数幂)-1 。</li>
</ul>
<ol>
<li>PG(Placement Group),用途是对object的存储进行组织和位置映射, (类似于redis cluster里面的slot的概念) 一个PG里面会有很多object。采用CRUSH算法，将pgid代入其中，然后得到一组OSD。PG-&gt;OSD映射：</li>
</ol>
<ul>
<li>a. CRUSH(pgid)-&gt;(osd1,osd2,osd3) 。</li>
</ul>
<pre><code class="hljs ini"><span class="hljs-attr">locator</span> = object_name
<span class="hljs-attr">obj_hash</span> =  hash(locator)
<span class="hljs-attr">pg</span> = obj_hash % num_pg
<span class="hljs-attr">osds_for_pg</span> = crush(pg)  <span class="hljs-comment"># returns a list of osds</span>
<span class="hljs-attr">primary</span> = osds_for_pg[<span class="hljs-number">0</span>]
<span class="hljs-attr">replicas</span> = osds_for_pg[<span class="hljs-number">1</span>:]</code></pre>
<h3 id="Ceph-RBD-IO流程"><a href="#Ceph-RBD-IO流程" class="headerlink" title="Ceph RBD IO流程"></a>Ceph RBD IO流程</h3><p><img src="/img/ceph-f.png" srcset="/img/loading.gif" alt="img"></p>
<ol>
<li><p>客户端创建一个pool，需要为这个pool指定pg的数量。</p>
</li>
<li><p>创建pool/image rbd设备进行挂载。</p>
</li>
<li><p>用户写入的数据进行切块，每个块的大小默认为4M，并且每个块都有一个名字，名字就是object+序号。</p>
</li>
<li><p>将每个object通过pg进行副本位置的分配。</p>
</li>
<li><p>pg根据cursh算法会寻找3个osd，把这个object分别保存在这三个osd上。</p>
</li>
<li><p>osd上实际是把底层的disk进行了格式化操作，一般部署工具会将它格式化为xfs文件系统。</p>
</li>
<li><p>object的存储就变成了存储一个文rbd0.object1.file。</p>
</li>
</ol>
<p><img src="/img/ceph-g.png" srcset="/img/loading.gif" alt="img"></p>
<p><strong>客户端写数据osd过程：</strong></p>
<ol>
<li>采用的是librbd的形式，使用librbd创建一个块设备，向这个块设备中写入数据。</li>
<li>在客户端本地同过调用librados接口，然后经过pool，rbd，object、pg进行层层映射,在PG这一层中，可以知道数据保存在哪3个OSD上，这3个OSD分为主从的关系。</li>
<li>客户端与primay OSD建立SOCKET 通信，将要写入的数据传给primary OSD，由primary OSD再将数据发送给其他replica OSD数据节点。</li>
</ol>
<h3 id="Ceph-Pool和PG分布情况"><a href="#Ceph-Pool和PG分布情况" class="headerlink" title="Ceph Pool和PG分布情况"></a>Ceph Pool和PG分布情况</h3><p><img src="/img/ceph-h.jpg" srcset="/img/loading.gif" alt="img"></p>
<ul>
<li><p>pool是ceph存储数据时的逻辑分区，它起到namespace的作用。</p>
</li>
<li><p>每个pool包含一定数量(可配置)的PG。</p>
</li>
<li><p>PG里的对象被映射到不同的Object上。</p>
</li>
<li><p>pool是分布到整个集群的。</p>
</li>
<li><p>pool可以做故障隔离域，根据不同的用户场景不一进行隔离。</p>
</li>
</ul>
<h3 id="Ceph-数据扩容PG分布"><a href="#Ceph-数据扩容PG分布" class="headerlink" title="Ceph 数据扩容PG分布"></a>Ceph 数据扩容PG分布</h3><p><strong>场景数据迁移流程：</strong></p>
<ul>
<li>现状3个OSD, 4个PG</li>
<li>扩容到4个OSD, 4个PG</li>
</ul>
<p><strong>扩容前</strong></p>
<p><img src="/img/ceph-i.jpg" srcset="/img/loading.gif" alt="img"></p>
<p><strong>扩容后</strong></p>
<p><img src="/img/ceph-j.png" srcset="/img/loading.gif" alt="img"></p>
<p><strong>说明</strong><br>每个OSD上分布很多PG, 并且每个PG会自动散落在不同的OSD上。如果扩容那么相应的PG会进行迁移到新的OSD上，保证PG数量的均衡。</p>
<h3 id="PG状态及其含义"><a href="#PG状态及其含义" class="headerlink" title="PG状态及其含义"></a>PG状态及其含义</h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">PG状态</th>
<th style="text-align:center">含义</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">down</td>
<td style="text-align:center">PG处于失效状态，PG处于离线状态</td>
</tr>
<tr>
<td style="text-align:center">repair</td>
<td style="text-align:center">PG正在被检查，被发现的任何不一致都将尽可能的被修复</td>
</tr>
<tr>
<td style="text-align:center">peering（等待互联）</td>
<td style="text-align:center">PG处于 peering 过程中, peering 由主 osd 发起的使存放 PG 副本的所有 OSD 就 PG 的所有对象和元素数据的状态达成一致的过程, peering 过程完成后, 主 OSD 就可以接受客户端写请求.</td>
</tr>
<tr>
<td style="text-align:center">active</td>
<td style="text-align:center">当 ceph 完成 peering 过程, pg 将会变成 active, active 状态意味着 pg 中的数据变得可用, 主 pg 将可执行读写操作 .</td>
</tr>
<tr>
<td style="text-align:center">clean</td>
<td style="text-align:center">干净态。前不存在待修复的对象， Acting Set和Up Set内容一致，并且大小等于存储池的副本数</td>
</tr>
<tr>
<td style="text-align:center">replay（重做）</td>
<td style="text-align:center">某OSD崩溃后，PG正在等待客户端重新发起操作</td>
</tr>
<tr>
<td style="text-align:center">degraded</td>
<td style="text-align:center">1.PG 处于 active+degraded 原因是因为 OSD 是处于活跃, 但并没有完成所有的对象副本写入, PG 中部分对象的副本数量未达到规定的数量.当 OSD 重新上线, OSD 将会重新恢复,  假如 OSD DOWN 并且 degraded 状态持续, CEPH 会标记 DOWN OSD, 并会对集群迁移相关 OSD 的数据, 对应时间由<code>mon osd down out interval</code> 参数决定 .</td>
</tr>
<tr>
<td style="text-align:center">inconsistent</td>
<td style="text-align:center">PG副本出现不一致, 对象大小不正确或者恢复借宿后某个副本出现对象丢失现象</td>
</tr>
<tr>
<td style="text-align:center">recovering</td>
<td style="text-align:center">ceph 设备故障容忍在一定范围的软件与硬件问题, 当 OSD 变 DOWN, 那么包含该 OSD 的 PG 副本都会有问题, 当 OSD 恢复, OSD 对应的 PG 将会更新 并反映出当前状态, 在一段时间周期后, OSD 将会恢复 recovering 状态</td>
</tr>
<tr>
<td style="text-align:center">back filling</td>
<td style="text-align:center">当新 OSD 加入集群, CRUSH 将会为集群新添加的 OSD 重新分配 PG, 强制新的 OSD 接受重新分配的 PG 并把一定数量的负载转移到新 OSD 中,back filling OSD 会在后台处理, 当 backfilling 完成, 新的 OSD 完成后, 将开始对请求进行服务</td>
</tr>
<tr>
<td style="text-align:center">remapped</td>
<td style="text-align:center">当 pg 改变, 数据从旧的 osd 迁移到新的 osd, 新的主 osd 应该请求将会花费一段时间, 在这段时间内, 将会继续向旧主 osd 请求服务, 直到 PG 迁移完成, 当数据迁移完成, mapping 将会使用新的 OSD 响应主 OSD 服务</td>
</tr>
<tr>
<td style="text-align:center">stale（旧）</td>
<td style="text-align:center">当 ceph 使用 heartbeat 确认主机与进程是否运行, ceph osd daemon 可能由于网络临时故障, 获得一个卡住状态 (stuck state) 没有得到心跳回应 默认, osd daemon 会每 0.5 秒报告 PG, up 状态, 启动与故障分析, 假如 PG 中主 OSD 因为故障没有回应 monitor 或者其他 OSD 报告 主 osd down, 那么 monitor 将会标记 PG stale</td>
</tr>
<tr>
<td style="text-align:center">scrubbing</td>
<td style="text-align:center">scrubbing（清理中）, PG 在做一致性校验</td>
</tr>
<tr>
<td style="text-align:center">inactive</td>
<td style="text-align:center">inactive ：PG 很长时间没有显示为 active 状态, (不可执行读写请求), PG 不可以执行读写, 因为等待 OSD 更新数据到最新的备份状态</td>
</tr>
<tr>
<td style="text-align:center">unclean</td>
<td style="text-align:center">unclean：PG 很长时间都不是 clean 状态 (不可以完成之前恢复的操作), PG 包含对象没有完成相应的复制副本数量, 通常都要执行恢复操作。</td>
</tr>
<tr>
<td style="text-align:center">stale（新）</td>
<td style="text-align:center">stale：PG 状态很长时间没有被 ceph-osd 更新过, 标识存储在该 GP 中的节点显示为 DOWN, PG 处于 unknown 状态, 因为 OSD 没有报告 monitor 由 mon osd report timeout 定义超时时间</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h2 id="3-Ceph心跳机制"><a href="#3-Ceph心跳机制" class="headerlink" title="3 Ceph心跳机制"></a>3 Ceph心跳机制</h2><p>心跳是用于节点间检测对方是否故障的，以便及时发现故障节点进入相应的故障处理流程。</p>
<p><strong>问题：</strong></p>
<ul>
<li>故障检测时间和心跳报文带来的负载之间做权衡。</li>
<li>心跳频率太高则过多的心跳报文会影响系统性能。</li>
<li>心跳频率过低则会延长发现故障节点的时间，从而影响系统的可用性。</li>
</ul>
<p><strong>故障检测策略应该能够做到：</strong></p>
<ul>
<li><strong>及时</strong>：节点发生异常如宕机或网络中断时，集群可以在可接受的时间范围内感知。</li>
<li><strong>适当的压力</strong>：包括对节点的压力，和对网络的压力。</li>
<li><strong>容忍网络抖动</strong>：网络偶尔延迟。</li>
<li><strong>扩散机制</strong>：节点存活状态改变导致的元信息变化需要通过某种机制扩展到整个集群。</li>
</ul>
<h3 id="心跳检测"><a href="#心跳检测" class="headerlink" title="心跳检测"></a>心跳检测</h3><p><img src="\img\ceph-webp.png" srcset="/img/loading.gif" alt="img"></p>
<p><strong>OSD节点会监听public、cluster、front和back四个端口</strong></p>
<ul>
<li><strong>public端口</strong>：监听来自Monitor和Client的连接。</li>
<li><strong>cluster端口</strong>：监听来自OSD Peer的连接。</li>
<li><strong>front端口</strong>：供客户端连接集群使用的网卡, 这里临时给集群内部之间进行心跳。</li>
<li><strong>back端口</strong>：供客集群内部使用的网卡。集群内部之间进行心跳。</li>
<li><strong>hbclient</strong>：发送ping心跳的messenger。</li>
</ul>
<h3 id="Ceph-OSD之间相互心跳检测"><a href="#Ceph-OSD之间相互心跳检测" class="headerlink" title="Ceph OSD之间相互心跳检测"></a>Ceph OSD之间相互心跳检测</h3><p><img src="/img/ceph-k.jpg" srcset="/img/loading.gif" alt="img"></p>
<ul>
<li>同一个PG内OSD互相心跳，他们互相发送PING/PONG信息。</li>
<li>每隔6s检测一次(实际会在这个基础上加一个随机时间来避免峰值)。</li>
<li>20s没有检测到心跳回复，加入failure队列。</li>
</ul>
<h3 id="Ceph-OSD与Mon心跳检测"><a href="#Ceph-OSD与Mon心跳检测" class="headerlink" title="Ceph OSD与Mon心跳检测"></a>Ceph OSD与Mon心跳检测</h3><p><img src="/img/ceph-l.jpg" srcset="/img/loading.gif" alt="img"></p>
<p><strong>OSD报告给Monitor：</strong></p>
<ul>
<li>OSD有事件发生时（比如故障、PG变更）。</li>
<li>自身启动5秒内。</li>
<li>OSD周期性的上报给Monito<ul>
<li>OSD检查failure_queue中的伙伴OSD失败信息。</li>
<li>向Monitor发送失效报告，并将失败信息加入failure_pending队列，然后将其从failure_queue移除。</li>
<li>收到来自failure_queue或者failure_pending中的OSD的心跳时，将其从两个队列中移除，并告知Monitor取消之前的失效报告。</li>
<li>当发生与Monitor网络重连时，会将failure_pending中的错误报告加回到failure_queue中，并再次发送给Monitor。</li>
</ul>
</li>
</ul>
<p>Monitor统计下线OSD</p>
<ul>
<li>Monitor收集来自OSD的伙伴失效报告。</li>
<li>当错误报告指向的OSD失效超过一定阈值，且有足够多的OSD报告其失效时，将该OSD下线。</li>
</ul>
<h3 id="Ceph心跳检测总结"><a href="#Ceph心跳检测总结" class="headerlink" title="Ceph心跳检测总结"></a>Ceph心跳检测总结</h3><p>Ceph通过伙伴OSD汇报失效节点和Monitor统计来自OSD的心跳两种方式判定OSD节点失效。</p>
<ul>
<li><strong>及时</strong>：伙伴OSD可以在秒级发现节点失效并汇报Monitor，并在几分钟内由Monitor将失效OSD下线。</li>
<li><strong>适当的压力</strong>：由于有伙伴OSD汇报机制，Monitor与OSD之间的心跳统计更像是一种保险措施，因此OSD向Monitor发送心跳的间隔可以长达600秒，Monitor的检测阈值也可以长达900秒。Ceph实际上是将故障检测过程中中心节点的压力分散到所有的OSD上，以此提高中心节点Monitor的可靠性，进而提高整个集群的可扩展性。</li>
<li><strong>容忍网络抖动</strong>：Monitor收到OSD对其伙伴OSD的汇报后，并没有马上将目标OSD下线，而是周期性的等待几个条件：<ul>
<li>目标OSD的失效时间大于通过固定量osd_heartbeat_grace和历史网络条件动态确定的阈值。</li>
<li>来自不同主机的汇报达到mon_osd_min_down_reporters。</li>
<li>满足前两个条件前失效汇报没有被源OSD取消。</li>
</ul>
</li>
<li><strong>扩散</strong>：作为中心节点的Monitor并没有在更新OSDMap后尝试广播通知所有的OSD和Client，而是惰性的等待OSD和Client来获取。以此来减少Monitor压力并简化交互逻辑。</li>
</ul>
<h2 id="4-Ceph通信框架"><a href="#4-Ceph通信框架" class="headerlink" title="4 Ceph通信框架"></a>4 Ceph通信框架</h2><p><strong>Simple线程模式</strong></p>
<ul>
<li><strong>特点</strong>：每一个网络链接，都会创建两个线程，一个用于接收，一个用于发送。</li>
<li><strong>缺点</strong>：大量的链接会产生大量的线程，会消耗CPU资源，影响性能。</li>
</ul>
<p><strong>Async事件的I/O多路复用模式</strong></p>
<ul>
<li><strong>特点</strong>：这种是目前网络通信中广泛采用的方式。k版默认已经使用Asnyc了。</li>
</ul>
<p><strong>XIO方式使用了开源的网络通信库accelio来实现</strong></p>
<ul>
<li><strong>特点</strong>：这种方式需要依赖第三方的库accelio稳定性，目前处于试验阶段。</li>
</ul>
<h3 id="Ceph通信框架设计模式"><a href="#Ceph通信框架设计模式" class="headerlink" title="Ceph通信框架设计模式"></a>Ceph通信框架设计模式</h3><p><strong>设计模式(Subscribe/Publish)</strong></p>
<p>订阅发布模式又名观察者模式，它意图是“定义对象间的一种一对多的依赖关系，<br> 当一个对象的状态发生改变时，所有依赖于它的对象都得到通知并被自动更新”。</p>
<p><img src="/img/ceph-m.jpg" srcset="/img/loading.gif" alt="img"></p>
<p>Accepter监听peer的请求, 调用 SimpleMessenger::add_accept_pipe() 创建新的 Pipe 到 SimpleMessenger::pipes 来处理该请求。</p>
<p>Pipe用于消息的读取和发送。该类主要有两个组件，Pipe::Reader，Pipe::Writer用来处理消息读取和发送。</p>
<p>Messenger作为消息的发布者, 各个 Dispatcher 子类作为消息的订阅者, Messenger 收到消息之后，  通过 Pipe 读取消息，然后转给 Dispatcher 处理。</p>
<p>Dispatcher调度员是订阅者的基类，具体的订阅后端继承该类,初始化的时候通过 Messenger::add_dispatcher_tail/head 注册到 Messenger::dispatchers. 收到消息后，通知该类处理。</p>
<p>DispatchQueue该类用来缓存收到的消息, 然后唤醒 DispatchQueue::dispatch_thread 线程找到后端的 Dispatch 处理消息。</p>
<p><img src="/img/ceph-u.png" srcset="/img/loading.gif" alt=""></p>
<h3 id="通信类框架图"><a href="#通信类框架图" class="headerlink" title="通信类框架图"></a>通信类框架图</h3><p><img src="/img/ceph-n.jpg" srcset="/img/loading.gif" alt="img"></p>
<h3 id="通信数据格式"><a href="#通信数据格式" class="headerlink" title="通信数据格式"></a>通信数据格式</h3><p>通信协议格式需要双方约定数据格式。</p>
<p><strong>消息的内容主要分为三部分：</strong></p>
<ul>
<li>header //消息头类型消息的信封</li>
<li>user data //需要发送的实际数据<ul>
<li>payload     //操作保存元数据</li>
<li>middle      //预留字段</li>
<li>data       //读写数据</li>
</ul>
</li>
<li>footer       //消息的结束标记</li>
</ul>
<pre><code class="hljs cpp"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Message</span> :</span> <span class="hljs-keyword">public</span> RefCountedObject &#123;
<span class="hljs-keyword">protected</span>:
  ceph_msg_header  header;      <span class="hljs-comment">// 消息头</span>
  ceph_msg_footer  footer;      <span class="hljs-comment">// 消息尾</span>
  bufferlist       payload;  <span class="hljs-comment">// &quot;front&quot; unaligned blob</span>
  bufferlist       middle;   <span class="hljs-comment">// &quot;middle&quot; unaligned blob</span>
  bufferlist       data;     <span class="hljs-comment">// data payload (page-alignment will be preserved where possible)</span>

  <span class="hljs-comment">/* recv_stamp is set when the Messenger starts reading the</span>
<span class="hljs-comment">   * Message off the wire */</span>
  <span class="hljs-keyword">utime_t</span> recv_stamp;       <span class="hljs-comment">//开始接收数据的时间戳</span>
  <span class="hljs-comment">/* dispatch_stamp is set when the Messenger starts calling dispatch() on</span>
<span class="hljs-comment">   * its endpoints */</span>
  <span class="hljs-keyword">utime_t</span> dispatch_stamp;   <span class="hljs-comment">//dispatch 的时间戳</span>
  <span class="hljs-comment">/* throttle_stamp is the point at which we got throttle */</span>
  <span class="hljs-keyword">utime_t</span> throttle_stamp;   <span class="hljs-comment">//获取throttle 的slot的时间戳</span>
  <span class="hljs-comment">/* time at which message was fully read */</span>
  <span class="hljs-keyword">utime_t</span> recv_complete_stamp;  <span class="hljs-comment">//接收完成的时间戳</span>

  ConnectionRef connection;     <span class="hljs-comment">//网络连接</span>

  <span class="hljs-keyword">uint32_t</span> magic = <span class="hljs-number">0</span>;           <span class="hljs-comment">//消息的魔术字</span>

  bi::list_member_hook&lt;&gt; dispatch_q;    <span class="hljs-comment">//boost::intrusive 成员字段</span>
&#125;;

<span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">ceph_msg_header</span> &#123;</span>
    __le64 seq;       <span class="hljs-comment">// 当前session内 消息的唯一 序号</span>
    __le64 tid;       <span class="hljs-comment">// 消息的全局唯一的 id</span>
    __le16 type;      <span class="hljs-comment">// 消息类型</span>
    __le16 priority;  <span class="hljs-comment">// 优先级</span>
    __le16 version;   <span class="hljs-comment">// 版本号</span>

    __le32 front_len; <span class="hljs-comment">// payload 的长度</span>
    __le32 middle_len;<span class="hljs-comment">// middle 的长度</span>
    __le32 data_len;  <span class="hljs-comment">// data 的 长度</span>
    __le16 data_off;  <span class="hljs-comment">// 对象的数据偏移量</span>


    <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">ceph_entity_name</span> <span class="hljs-title">src</span>;</span> <span class="hljs-comment">//消息源</span>

    <span class="hljs-comment">/* oldest code we think can decode this.  unknown if zero. */</span>
    __le16 compat_version;
    __le16 reserved;
    __le32 crc;       <span class="hljs-comment">/* header crc32c */</span>
&#125; __attribute__ ((packed));

<span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">ceph_msg_footer</span> &#123;</span>
    __le32 front_crc, middle_crc, data_crc; <span class="hljs-comment">//crc校验码</span>
    __le64  sig; <span class="hljs-comment">//消息的64位signature</span>
    __u8 flags; <span class="hljs-comment">//结束标志</span>
&#125; __attribute__ ((packed));</code></pre>
<hr>
<h2 id="5-Ceph-CRUSH算法"><a href="#5-Ceph-CRUSH算法" class="headerlink" title="5 Ceph CRUSH算法"></a>5 Ceph CRUSH算法</h2><blockquote>
<p>Controlled Replication Under Scalable Hashing, 可扩展哈希下的可控复制。以数据唯一标识符、当前存储集群的拓扑结构以及数据备份策略作为CRUSH输入，可以随时随地的通过计算获取数控所在的底层存储设备的位置并直接与其通信，从而避免查表操作，实现去中心化和高度并发。</p>
<p>CRUSH是一种伪随机算法，采用<strong>一致性哈希</strong>。</p>
<p>OSD MAP: 包含当前所有pool的状态，和所有OSD状态。</p>
<p>CRUSH MAP: 包含当前磁盘、服务器、机架的层次结构。</p>
<p>CRUSH Rules：数据映射的策略。以便灵活放置Object。</p>
</blockquote>
<h3 id="数据分布算法挑战"><a href="#数据分布算法挑战" class="headerlink" title="数据分布算法挑战"></a>数据分布算法挑战</h3><p><strong>数据分布和负载均衡</strong>：</p>
<ul>
<li>a. 数据分布均衡，使数据能均匀的分布到各个节点上。</li>
<li>b. 负载均衡，使数据访问读写操作的负载在各个节点和磁盘的负载均衡。</li>
</ul>
<p><strong>灵活应对集群伸缩</strong>：</p>
<ul>
<li>a. 系统可以方便的增加或者删除节点设备，并且对节点失效进行处理。</li>
<li>b. 增加或者删除节点设备后，能自动实现数据的均衡，并且尽可能少的迁移数据。</li>
</ul>
<p><strong>支持大规模集群</strong>：</p>
<ul>
<li>a. 要求数据分布算法维护的元数据相对较小，并且计算量不能太大。随着集群规模的增 加，数据分布算法开销相对比较小。</li>
</ul>
<h3 id="Ceph-CRUSH算法原理"><a href="#Ceph-CRUSH算法原理" class="headerlink" title="Ceph CRUSH算法原理"></a>Ceph CRUSH算法原理</h3><p><strong>CRUSH算法因子：</strong></p>
<ul>
<li>层次化的Cluster Map<br> 实际应用中设备具有形如“数据中心 → 机架→主机→磁盘”这样的树状层级，所以Cluster Map采用树来实现，每个叶子节点都是真实的最小物理存储设备，称为devices，而所有中间节点称为root，是整个集群的入口。每个节点都拥有唯一的数字ID和类型，但是只有叶子节点才拥有非负ID，表明它们是终端设备。</li>
</ul>
<blockquote>
<p>​                                                    下表展示了Cluster Map一些常见节点的层级</p>
</blockquote>
<ul>
<li>| 类型ID |  类型名称  |<br> | :——: | :————: |<br>|   0    |    osd     |<br> |   1    |    host    |<br> |   2    |  chassis   |<br> |   3    |    rack    |<br> |   4    |    row     |<br> |   5    |    pdu     |<br> |   6    |    pod     |<br> |   7    |    room    |<br> |   8    | datacenter |<br> |   9    |   region   |<br> |   10   |    root    |</li>
</ul>
<p><img src="/img/ceph-o.png" srcset="/img/loading.gif" alt=" "></p>
<ul>
<li>CRUSH Map是一个树形结构，OSDMap更多记录的是OSDMap的属性(epoch/fsid/pool信息以及osd的ip等等)。</li>
</ul>
<p>叶子节点是device（也就是osd），其他的节点称为bucket节点，这些bucket都是虚构的节点，可以根据物理结构进行抽象，当然树形结构只有一个最终的根节点称之为root节点，中间虚拟的bucket节点可以是数据中心抽象、机房抽象、机架抽象、主机抽象等。</p>
<h3 id="数据分布策略Placement-Rules"><a href="#数据分布策略Placement-Rules" class="headerlink" title="数据分布策略Placement Rules"></a>数据分布策略Placement Rules</h3><p>在完成了使用clustermap建立对应的集群的拓扑结构描述后，可以定义placement rule 来完成<strong>数据映射</strong>.</p>
<p>这些操作有三种类型：</p>
<ul>
<li><p>take</p>
<p>take从cluster map选择指定编号的bucket ，并以此作为后续步骤的输入。例如系统默认以root节点作为输入。</p>
</li>
<li><p>select*</p>
<p>select从输入的bucket中随机选择指定类型和数量的条目。Ceph支持两种类型的备份策略，多副本和<strong>纠删码</strong>，对应两种算法，firstn和<strong>indep</strong>。以上两种算法都是dfs，无明显区别，唯一区别是纠删码是要求结果是有序的，i.e.总是返回指定长度的结果，如果对应条目不存在，采用空穴进行填充。</p>
<p>select操作也支持容灾模式，例如设置为rack，select保证所有选出的副本位于不同的机架上，也可以设置为host，即所有选出的副本位于不同的主机的磁盘上。</p>
</li>
<li><p>emit</p>
<p>输出最终的选择结果给上级调用并返回。</p>
</li>
</ul>
<p><strong>数据分布策略Placement Rules主要有特点：</strong></p>
<ul>
<li>a. 从CRUSH Map中的哪个节点开始查找</li>
<li>b. 使用那个节点作为故障隔离域</li>
<li>c. 定位副本的搜索模式（广度优先 or 深度优先）</li>
</ul>
<pre><code class="hljs bash">rule replicated_ruleset  <span class="hljs-comment">#规则集的命名，创建pool时可以指定rule集</span>
&#123;
    ruleset 0                <span class="hljs-comment">#rules集的编号，顺序编即可   </span>
    <span class="hljs-built_in">type</span> replicated          <span class="hljs-comment">#定义pool类型为replicated(还有erasure模式)   </span>
    min_size 1                <span class="hljs-comment">#pool中最小指定的副本数量不能小1</span>
    max_size 10               <span class="hljs-comment">#pool中最大指定的副本数量不能大于10       </span>
    step take default         <span class="hljs-comment">#查找bucket入口点，一般是root类型的bucket    </span>
    step chooseleaf  firstn  0  <span class="hljs-built_in">type</span>  host <span class="hljs-comment">#选择一个host,并递归选择叶子节点osd     </span>
    step emit        <span class="hljs-comment">#结束</span>
&#125;</code></pre>
<h3 id="Bucket随机算法类型"><a href="#Bucket随机算法类型" class="headerlink" title="Bucket随机算法类型"></a>Bucket随机算法类型</h3><p><img src="/img/ceph-p.jpg" srcset="/img/loading.gif" alt=""></p>
<p><strong>一般的buckets</strong>：适合所有子节点权重相同，而且很少添加删除item。</p>
<p><strong>list buckets</strong>：适用于集群扩展类型。增加item，产生最优的数据移动，查找item，时间复杂度O(n)。</p>
<p><strong>tree buckets</strong>：查找负责度是O (log n), 添加删除叶子节点时，其他节点node_id不变。</p>
<p><strong>straw buckets</strong>：允许所有项通过类似抽签的方式来与其他项公平“竞争”。定位副本时，bucket中的每一项都对应一个随机长度的straw，且拥有最长长度的straw会获得胜利（被选中），添加或者重新计算，子树之间的数据移动提供最优的解决方案。</p>
<h3 id="STRAW算法"><a href="#STRAW算法" class="headerlink" title="STRAW算法"></a>STRAW算法</h3><p>straw将所有元素比喻成吸管，针对指定输入，为每个元素随机计算一个长度，最后选择长度最长的那个元素作为结果输出，这个过程也被形象地称为抽签（draw），对应元素的长度称为签长。因为存储设备随着时间推移会趋于异构化，随意我们引入权重来让容量大的设备分担更多的数据，而容量小的设备分担更少的数据，从而使得数据在异构网络中也能获得合理的分布。</p>
<p>将所有元素按逆序排列，我们设签长为$L$，用$\triangle W_{pre}$表示当前元素与之前元素权重的差值，$R$表示剩余元素个数，$\triangle W_{next}$表示下一个元素与现在元素的权重差值,$S$表示累计权重占有的比重。</p>
<script type="math/tex; mode=display">
S = \frac{\sum \triangle W_{pre}}{\sum \triangle W_{pre}+\triangle W_{next}}\\
L = \prod{(1/S)^{1/R}}</script><p>再把L乘上0x10000。</p>
<p>上述算法，最终选择结果不仅与每个元素自身权重有关，还与集合中其它元素相关，修正后的straw2算法，则更加简单，</p>
<pre><code class="hljs properties"><span class="hljs-attr">max_x</span> = <span class="hljs-string">-1</span>
<span class="hljs-attr">max_item</span> = <span class="hljs-string">-1</span>
<span class="hljs-attr">foreach</span> <span class="hljs-string">in item:</span>
	<span class="hljs-attr">x</span> = <span class="hljs-string">hash(input, r)</span>
	<span class="hljs-attr">x</span> = <span class="hljs-string">ln(x/65536)/weight</span>
	<span class="hljs-attr">if</span> <span class="hljs-string">x &gt; x_max:</span>
		<span class="hljs-attr">x_max</span> = <span class="hljs-string">x</span>
		<span class="hljs-attr">max_item</span> = <span class="hljs-string">item</span>
<span class="hljs-attr">return</span> <span class="hljs-string">max_item</span></code></pre>
<p>下面我们介绍firstn算法的select过程</p>
<ol>
<li>重置计数器rep</li>
<li>检查rep是否等于副本数，若等于，则返回，否则进行下一步</li>
<li>重置尝试次数</li>
<li>设置当前查找的bucket为函数输入的bucket</li>
<li></li>
</ol>
<h3 id="CRUSH算法案例"><a href="#CRUSH算法案例" class="headerlink" title="CRUSH算法案例"></a>CRUSH算法案例</h3><p>集群中有部分sas和ssd磁盘，现在有个业务线性能及可用性优先级高于其他业务线，能否让这个高优业务线的数据都存放在ssd磁盘上。</p>
<p><strong>普通用户：</strong></p>
<p><img src="/img/ceph-q.jpg" srcset="/img/loading.gif" alt="img"></p>
<p><strong>高优用户</strong></p>
<p><img src="/img/ceph-r.jpg" srcset="/img/loading.gif" alt="img"></p>
<p>配置规则</p>
<p>作者：<img src="/img/ceph-t.jpg" srcset="/img/loading.gif" alt="img"></p>
<hr>
<h2 id="6-定制化Ceph-RBD-QOS"><a href="#6-定制化Ceph-RBD-QOS" class="headerlink" title="6 定制化Ceph RBD QOS"></a>6 定制化Ceph RBD QOS</h2><p>QoS （Quality of Service，服务质量）起源于网络技术，它用来解决网络延迟和阻塞等问题，能够为指定的网络通信提供更好的服务能力。</p>
<p>我们总的Ceph集群的iIO能力是有限的，比如带宽，IOPS。如何避免用户争取资源，如果保证集群所有用户资源的高可用性，以及如何保证高优用户资源的可用性。所以我们需要把有限的IO能力合理分配。</p>
<h3 id="Ceph-IO操作类型"><a href="#Ceph-IO操作类型" class="headerlink" title="Ceph IO操作类型"></a>Ceph IO操作类型</h3><ul>
<li><p><strong>ClientOp</strong>：来自客户端的读写I/O请求。</p>
</li>
<li><p><strong>SubOp</strong>：osd之间的I/O请求。主要包括由客户端I/O产生的副本间数据读写请求，以及由数据同步、数据扫描、负载均衡等引起的I/O请求。</p>
</li>
<li><p><strong>SnapTrim</strong>：快照数据删除。从客户端发送快照删除命令后，删除相关元数据便直接返回，之后由后台线程删除真实的快照数据。通过控制snaptrim的速率间接控制删除速率。</p>
</li>
<li><p><strong>Scrub</strong>：用于发现对象的静默数据错误，扫描元数据的Scrub和对象整体扫描的deep Scrub。</p>
</li>
<li><p><strong>Recovery</strong>：数据恢复和迁移。集群扩/缩容、osd失效/从新加入等过程。</p>
</li>
</ul>
<h2 id="7-CephFS"><a href="#7-CephFS" class="headerlink" title="7.CephFS"></a>7.CephFS</h2><p><img src="/img/ceph-lec.jpg" srcset="/img/loading.gif" alt="image-20201122181939220"></p>
<h2 id="8-BlueStore-分布式对象存储"><a href="#8-BlueStore-分布式对象存储" class="headerlink" title="8.BlueStore 分布式对象存储"></a>8.BlueStore 分布式对象存储</h2><blockquote>
<p>与一般的FS相比，BlueStore绕过了系统的本地文件系统，由自身接管磁盘，所以其性能更好。并且充分考虑了对下一代全SSD以及全NVMe SSD闪存的支持。例如支持RocksDB。</p>
</blockquote>
<p>先介绍一些术语：</p>
<p>$ACID$，分别表示$Atomicity, Consistency,Isolation,Durability$。原子性，一致性，隔离性，持久性。一个支持事务$Transcation$d的系统必须支持这四种特性。</p>
<ol>
<li><strong>block-size</strong></li>
</ol>
<p>对磁盘进行操作的最小粒度（原子粒度），对普通的机械硬盘，最小粒度512字节，即一个扇区。现代SSD一般使用更大的块，4KB。</p>
<ol>
<li><strong>RMW（Read Modify Write）</strong></li>
</ol>
<p>指覆盖写，如果本次改写的内容不足一个磁盘块大小，那么需要先将对应的块读上来，然后将待修改的内容与原先的内容进行合并，最后将更新后的块重新写入原先的位置。 有两个问题：1. 额外的惩罚读 2. 因为要针对已有的内容执行覆盖写。解决方法是引入日志，数据线写入日志盘再更新数据盘。</p>
<ol>
<li><strong>COW（Copy On Write）</strong></li>
</ol>
<p>指当覆盖写发生时，不是直接更新磁盘对应位置的已有内容，而是重新在磁盘上分配一块新的空间，用于存放本次新写入的内容，这个过程也称为写时重定向。当新写完成，对应的地址更新时，即可释放原有数据对应的磁盘空间。</p>
<p>它自身的缺陷是：1. 破坏了数据在磁盘分布的物理连续性，经过多次COW后，前端任何大范围的顺序读后续都将变为随机读。在SSD普及后，这种情况有所好转。2. 将新的内容写入新块后，原有的块因为仍然保留了部分有效内容，所以COW之后不能释放。因为COW涉及空间重分配和地址指针重定向，所以COW将引入更多元数据。对存储系统而言，元数据的多少关乎功能的丰富与否。</p>
<p>BlueStore针对写操作综合运用了RMW和COW策略——任何一个写请求，根据磁盘的大小，将其切分为三个部分。<strong>首尾非块大小对齐部分</strong>，<strong>中间块大小对齐部分</strong>，然后针对中间块对齐部分采用<strong>COW策略</strong>，首尾非块对齐部分采用<strong>RMW策略</strong>。</p>
<p>BlueStore主要提供了读写两种类型的多线程访问接口，这些接口是基于PG粒度的。因为读请求可以并发，而写请求出于效率考虑一般被设计为异步，（所以PG内部使用读写锁来实现上述语义），实现上还需要为每个PG设计一个队列，用于对所有操作该PG的写请求进行保序。称为$OpSequencer$，不同类型的ObjectStore略有不同，在BlueStore实现中，$OpSequencer$包含两个FIFO。用于将所有进入覆盖写数据阶段的带日志写请求在线程池中再次进行排序。所有写请求通过标准的<code>queue_transcations</code>接口提交至BlueStore.</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://github.com/ceph/ceph/pull/9398">多对象事务语义支持参考</a></p>
</blockquote>
<h2 id="磁盘数据结构"><a href="#磁盘数据结构" class="headerlink" title="磁盘数据结构"></a>磁盘数据结构</h2><p>BlueStore习惯上磁盘格式用<code>_t</code>结尾而内存格式不以<code>_t</code>结尾并且只有首大写字母。所有元数据被设计成可以和用户数据分开存放，以键值对形式存放在$kvDB$中。</p>
<h3 id="PG"><a href="#PG" class="headerlink" title="PG"></a>PG</h3><p>ceph对所有存储资源进行池化管理。资源池(pool), 是一个虚拟概念，表示一组CRUSH规则的约束条件。比如我们可以针对不同的pool指定不同的备份策略，针对时延敏感的应用采用副本策略，而在一些不重要的应用采用纠删码。Ceph将任意类型的前端数据都抽象为对象，每一个对象 采用一定的策略可以生成一个全局唯一的对象标识符（$ObjectID,OID$）基于此策略的对象标识最终可以形成一个扁平的寻址空间，从而大大提高索引效率。</p>
<p>为了实现不同pool之间的策略隔离，Ceph引入了一个中间结构，称为PG，（归置组），实现两级映射。</p>
<ul>
<li>第一级映射是静态的，负责将任何前端类型 的应用数据按照固定的大小进行分割·编号作为伪随机哈希函数的输入，均匀映射至PG，以实现负载均衡。</li>
<li>第二集映射实现PG到OSD的映射，这级映射仍采用伪随机哈希函数，但是除输入的五安居唯一的PGID外，还引入了集群拓扑，并且使用CRUSH规则对计算过程进行调整，以帮助PG在不同OSD之间进行灵活迁移，进而实现数据可靠性和自动平衡等。最终pool以PG作为基本单位进行组织。</li>
</ul>
<p>为了维持扁平的寻址空间。PG也有一个全局唯一的ID——$PGID$，所有的pool由Monitor统一管理，集群内唯一pool-id。事实上，为了保证Monitor分布一致性，采用Paxos算法，我们只需要为pool内每个PG分配一个pool内唯一的编号即可。</p>
<blockquote>
<p>思考：如何使得同一个pool下的不同的PG低n位相同？</p>
<p>首先由特定类型的Client根据其操作的对象名计算出一个32位的哈希值，然后根据其操作的对象名计算出一个32位的哈希值，然后根据归属的pool及此时的哈希值，通过简单的计算，比如取模，即可找到最终承载该对象的PG。</p>
<p>我们发现如果pool内的PG数目如果能写成$2^n$形式，那么其低n比特都是相同的。我们将$2^n-1$称为PG的<strong>掩码</strong>。否则，若PG不能写成$2^n$的形式，则不能保证针对不同的输入低n比特相同这一“稳定”的性质。（比如有12个PG，那么对于属于同一个pool的PGID只有低两位相同）</p>
<p>因此一种行之有效的方法是用掩码代替取模。取hash低n-1位，即$hash\&amp;(2^n-1)$ .但这种映射存在问题，如果PG数目不能被2整除，那么采用这种方式会导致空穴，也就是取模结果没有实际PG对应。</p>
<p>比如一个pool有12个PG，n=4，但是12-15这些值都浪费了：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>0</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
<th>7</th>
<th>8</th>
<th>9</th>
<th>10</th>
<th>11</th>
<th>12</th>
<th>13</th>
<th>14</th>
<th>15</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>x</td>
<td>x</td>
<td>x</td>
<td>x</td>
</tr>
</tbody>
</table>
</div>
<p>我们可以想办法压缩空间 ，$hash\&amp;(2^{n-1}-1)$，使得不能被2整除的PGID仍能被合理映射。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>0</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
<th>7</th>
<th>8</th>
<th>9</th>
<th>10</th>
<th>11</th>
<th>12</th>
<th>13</th>
<th>14</th>
<th>15</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>0</td>
<td>1</td>
<td>2</td>
<td>3</td>
<td>4</td>
<td>5</td>
<td>6</td>
<td>7</td>
</tr>
</tbody>
</table>
</div>
<p>如果$hash\&amp;(2^{n}-1)&lt;pg_num$，那么可以直接返回$hash\&amp;(2^n-1)$.</p>
<p>否则，我们返回$hash\&amp;(2^{n-1}-1)$.</p>
<p>在参考书上被称为稳定哈希（stable hash）。</p>
</blockquote>
<p>Ceph主要设计理念之一是高扩展性。当集群中PG增加，新的PG会被随机均匀地映射至所有OSD上。作为stable hash的输入的PG数目已经发生变化，导致某些对象从旧PG重新映射至新PG，因此需要转移这部分对象，我们称为<strong>PG分裂</strong>。</p>
<h3 id="对象"><a href="#对象" class="headerlink" title="对象"></a>对象</h3><p>BlueStore中的对象非常类似于文件，其最基本单元是<strong>逻辑段</strong>（extent）.</p>
<p>可以写成 $\{offset, length , data\}$，</p>
<ul>
<li>offset表示对象逻辑偏移，从0开始编址</li>
<li>逻辑段长度</li>
<li>抽象数据类型</li>
</ul>
<p>—-更新中</p>
<p>参考资料</p>
<p><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/cc3ece850433">https://www.jianshu.com/p/cc3ece850433</a></p>
<hr>
<section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>Weil S A Brandt S A , Miller E L , et al. CRUSH: Controlled, Scalable, Decentralized Placement of Replicated Data[C]// IEEE Sc Conference. ACM, 2006.
<a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>
            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/ceph/">ceph</a>
                    
                      <a class="hover-with-bg" href="/categories/ceph/%E7%90%86%E8%AE%BA/">理论</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/ceph/">ceph</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！</p>
              
              
                <div class="post-prevnext row">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2020/11/19/Ceph%E6%95%B0%E6%8D%AE%E5%88%86%E5%B8%83%E5%8F%8A%E7%BC%93%E5%AD%98%E6%9B%BF%E6%8D%A2%E6%9C%BA%E5%88%B6/">
                        <span class="hidden-mobile">CEPH 数据分布及缓存替换机制</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
              <!-- Comments -->
              <article class="comments" id="comments">
                
                
  <div id="vcomments"></div>
  <script type="text/javascript">
    function loadValine() {
      addScript('https://cdn.staticfile.org/valine/1.4.14/Valine.min.js', function () {
        new Valine({
          el: "#vcomments",
          app_id: "8oX7VCxy9tyFQIvF8qwLorP0-gzGzoHsz",
          app_key: "F0939INl4cKXpCw2HgTIUb6B",
          placeholder: "说点什么,（支持Markdown）",
          path: window.location.pathname,
          avatar: "retro",
          meta: ["nick","mail","link"],
          pageSize: "10",
          lang: "zh-CN",
          highlight: false,
          recordIP: false,
          serverURLs: "https://8ox7vcxy.lc-cn-n1-shared.com",
        });
      });
    }
    waitElementVisible('vcomments', loadValine);
  </script>
  <noscript>Please enable JavaScript to view the <a target="_blank" href="https://valine.js.org" rel="nofollow noopener noopener">comments
      powered by Valine.</a></noscript>


              </article>
            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div id="tocbot"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    
  </main>

  
    <a id="scroll-top-button" href="#" role="button">
      <i class="iconfont icon-arrowup" aria-hidden="true"></i>
    </a>
  

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  

  

  <footer class="mt-5">
  <div class="text-center py-3">
    <div>
      <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a>
      <i class="iconfont icon-love"></i>
      <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener">
        <span>Fluid</span></a>
    </div>
    
  <div class="statistics">
    
    

    
      
        <!-- 不蒜子统计PV -->
        <span id="busuanzi_container_site_pv" style="display: none">
            总访问量 
            <span id="busuanzi_value_site_pv"></span>
             次
          </span>
      
      
        <!-- 不蒜子统计UV -->
        <span id="busuanzi_container_site_uv" style="display: none">
            总访客数 
            <span id="busuanzi_value_site_uv"></span>
             人
          </span>
      
    
  </div>


    

    
  </div>
</footer>

<!-- SCRIPTS -->
<script  src="https://cdn.staticfile.org/jquery/3.4.1/jquery.min.js" ></script>
<script  src="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/js/bootstrap.min.js" ></script>
<script  src="/js/debouncer.js" ></script>
<script  src="/js/main.js" ></script>

<!-- Plugins -->


  
    <script  src="/js/lazyload.js" ></script>
  



  



  <script defer src="https://cdn.staticfile.org/clipboard.js/2.0.6/clipboard.min.js" ></script>
  <script  src="/js/clipboard-use.js" ></script>



  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





  <script  src="https://cdn.staticfile.org/tocbot/4.11.1/tocbot.min.js" ></script>
  <script>
    $(document).ready(function () {
      var boardCtn = $('#board-ctn');
      var boardTop = boardCtn.offset().top;

      tocbot.init({
        tocSelector: '#tocbot',
        contentSelector: '#post-body',
        headingSelector: 'h1,h2,h3,h4,h5,h6',
        linkClass: 'tocbot-link',
        activeLinkClass: 'tocbot-active-link',
        listClass: 'tocbot-list',
        isCollapsedClass: 'tocbot-is-collapsed',
        collapsibleClass: 'tocbot-is-collapsible',
        collapseDepth: 0,
        scrollSmooth: true,
        headingsOffset: -boardTop
      });
      if ($('.toc-list-item').length > 0) {
        $('#toc').css('visibility', 'visible');
      }
    });
  </script>



  <script  src="https://cdn.staticfile.org/typed.js/2.0.11/typed.min.js" ></script>
  <script>
    var typed = new Typed('#subtitle', {
      strings: [
        '  ',
        "CEPH 基础介绍&nbsp;",
      ],
      cursorChar: "_",
      typeSpeed: 70,
      loop: false,
    });
    typed.stop();
    $(document).ready(function () {
      $(".typed-cursor").addClass("h2");
      typed.start();
    });
  </script>



  <script  src="https://cdn.staticfile.org/anchor-js/4.2.2/anchor.min.js" ></script>
  <script>
    anchors.options = {
      placement: "right",
      visible: "hover",
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  </script>



  <script  src="/js/local-search.js" ></script>
  <script>
    var path = "/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      searchFunc(path, 'local-search-input', 'local-search-result');
      this.onclick = null
    }
  </script>



  <script  src="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.css" />

  <script>
    $('#post img:not(.no-zoom img, img[no-zoom]), img[zoom]').each(
      function () {
        var element = document.createElement('a');
        $(element).attr('data-fancybox', 'images');
        $(element).attr('href', $(this).attr('src'));
        $(this).wrap(element);
      }
    );
  </script>







  
  
    <script>
      !function (e, t, a) {
        function r() {
          for (var e = 0; e < s.length; e++) s[e].alpha <= 0 ? (t.body.removeChild(s[e].el), s.splice(e, 1)) : (s[e].y--, s[e].scale += .004, s[e].alpha -= .013, s[e].el.style.cssText = "left:" + s[e].x + "px;top:" + s[e].y + "px;opacity:" + s[e].alpha + ";transform:scale(" + s[e].scale + "," + s[e].scale + ") rotate(45deg);background:" + s[e].color + ";z-index:99999");
          requestAnimationFrame(r)
        }

        function n() {
          var t = "function" == typeof e.onclick && e.onclick;
          e.onclick = function (e) {
            t && t(), o(e)
          }
        }

        function o(e) {
          var a = t.createElement("div");
          a.className = "heart", s.push({
            el: a,
            x: e.clientX - 5,
            y: e.clientY - 5,
            scale: 1,
            alpha: 1,
            color: c()
          }), t.body.appendChild(a)
        }

        function i(e) {
          var a = t.createElement("style");
          a.type = "text/css";
          try {
            a.appendChild(t.createTextNode(e))
          } catch (t) {
            a.styleSheet.cssText = e
          }
          t.getElementsByTagName("head")[0].appendChild(a)
        }

        function c() {
          return "rgb(" + ~~(255 * Math.random()) + "," + ~~(255 * Math.random()) + "," + ~~(255 * Math.random()) + ")"
        }

        var s = [];
        e.requestAnimationFrame = e.requestAnimationFrame || e.webkitRequestAnimationFrame || e.mozRequestAnimationFrame || e.oRequestAnimationFrame || e.msRequestAnimationFrame || function (e) {
          setTimeout(e, 1e3 / 60)
        }, i(".heart{width: 10px;height: 10px;position: fixed;background: #f00;transform: rotate(45deg);-webkit-transform: rotate(45deg);-moz-transform: rotate(45deg);}.heart:after,.heart:before{content: '';width: inherit;height: inherit;background: inherit;border-radius: 50%;-webkit-border-radius: 50%;-moz-border-radius: 50%;position: fixed;}.heart:after{top: -5px;}.heart:before{left: -5px;}"), n(), r()
      }(window, document);
    </script>
  








  <script  src="https://cdn.staticfile.org/mermaid/8.5.0/mermaid.min.js" ></script>
  <script>
    if (window.mermaid) {
      mermaid.initialize({"theme":"default"});
    }
  </script>




  

  

  

  

  

  





<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<!--<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>-->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

</body>
</html>
