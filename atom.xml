<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Durant Thorvalds 的米奇妙妙屋</title>
  
  <subtitle>在这里，您能感受算法和大数据及存储的魅力！</subtitle>
  <link href="http://durantthorvalds.top/atom.xml" rel="self"/>
  
  <link href="http://durantthorvalds.top/"/>
  <updated>2021-01-05T13:24:01.499Z</updated>
  <id>http://durantthorvalds.top/</id>
  
  <author>
    <name>Durant</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>「置顶」本博客导航！</title>
    <link href="http://durantthorvalds.top/2025/12/31/2020-10-8-article_navigator/"/>
    <id>http://durantthorvalds.top/2025/12/31/2020-10-8-article_navigator/</id>
    <published>2025-12-30T16:00:00.000Z</published>
    <updated>2021-01-05T13:24:01.499Z</updated>
    
    <content type="html"><![CDATA[<div class="note note-warning">            <p>若您的网页显示mathjax公式出现问题，导致无法阅读，请在任意Mathjax公式上点击右键，<code>Math settings</code>-&gt;<code>Math renderer</code>-&gt;更改为<code>HTML-CSS</code>即可解决。</p>          </div><div class="note note-primary">            <p>因博主水平有限，不足之处请直接在评论区指出，博主将感激不尽！</p>          </div><p>欢迎您光顾本博客！</p><p>本博客 包括<strong>LeetCode</strong>刷题记录和经验，机器学习，以及硕士专业研究课题方面。</p><p>本人硕士研究课题是 <u><strong>纠删码</strong>及<strong>去重</strong>在Ceph分布式存储集群上的应用</u>。对此研究方向感兴趣的道友欢迎一起交流。</p><hr><h2 id="👌LeetCode刷题记录"><a href="#👌LeetCode刷题记录" class="headerlink" title="👌LeetCode刷题记录"></a>👌LeetCode刷题记录</h2><h3 id="A-动态规划"><a href="#A-动态规划" class="headerlink" title="A. 动态规划"></a>A. 动态规划</h3><ul><li>动态规划 <a href="https://durantthorvalds.top/2020/07/21/2020-07-21-dynamic-planning/">&gt;&gt;传送门&lt;&lt;</a></li></ul><blockquote><p>动态规划是将问题分解为更易解决的子问题的一类方法，本blog由浅入深介绍动态规划的常见题型，dp是笔试最常考的问题之一。</p></blockquote><ul><li>做一只可爱的小🐖背包 Cover「背包九讲」 <a href="https://durantthorvalds.top/2020/07/25/2020-07-27-backpack-problem/">&gt;&gt;传送门&lt;&lt;</a></li></ul><blockquote><p>背包问题是很经典的一类dp问题。本blog介绍9种类型的背包问题。面试者若能举一反三，必将事半功倍。</p></blockquote><h3 id="B-树类问题"><a href="#B-树类问题" class="headerlink" title="B. 树类问题"></a>B. 树类问题</h3><ul><li>必须掌握的数据结构-树 <a href="https://durantthorvalds.top/2020/09/23/2020-9-23-binary_tree/">&gt;&gt;传送门&lt;&lt;</a></li></ul><blockquote><p>如果你还不知道什么是树，没有关系！我们将从零开始介绍，1. 二叉树的表示 2.二叉树的遍历 3. 二叉树序列化以及反序列化 4. 树形dp  5.线索二叉树 等。使得你对树类问题有基本认识。</p></blockquote><ul><li>树状数组和线段树 <a href="https://durantthorvalds.top/2020/07/17/2020-07-17-fenwicktree-segmentTree/">&gt;&gt;传送门&lt;&lt;</a></li></ul><blockquote><p>树状数组和线段树都是用于维护数列信息的数据结构，支持单点/区间修改，单点/区间询问信息。以增加权值与询问区间权值和为例，其余的信息需要维护也都类似。时间复杂度均为$O(logn)$。</p></blockquote><ul><li>Google划词搜索的秘密——前缀树 <a href="https://durantthorvalds.top/2020/07/21/2020-8-20-what-is-Trie/">&gt;&gt;传送门&lt;&lt;</a></li></ul><blockquote><p>前缀树又名<strong>Tries树</strong>、<strong>字典</strong>树、单词查找树等，常用于快速检索，大量字符串的排序和统计等。Trie相比于哈希表存储多个具有相同前缀的键时所用空间较少。因此前缀树只需要$O(m)$的空间，m为键长度。在字符串问题中很常见。</p></blockquote><ul><li>花拳绣腿学红黑树 <a href="https://durantthorvalds.top/2020/05/10/2020-05-10-red-black-tree/">&gt;&gt;传送门&lt;&lt;</a></li></ul><blockquote><p>红黑树的结点增删改查效率非常优良，都为$log(n) $, 应用方面：1. Linux内核进程调度由红黑树管理进程控制块。 2. Epoll用红黑树管理事件块。 3. nginx服务器用红黑树管理定时器。 4. C++ STL中的map和set的底层实现为红黑树。 5. Java中的TreeMap和TreeSet由红黑树实现。 6. Java8开始，HashMap中，当一个桶的链表长度超过8，则会改用红黑树。红黑树在面试中会出现，笔试中可不会出现哦。</p></blockquote><ul><li>全手写实现AVL树 <a href="https://durantthorvalds.top/2020/05/10/2020-04-30-avl-tree/">&gt;&gt;传送门&lt;&lt;</a></li></ul><blockquote><p>AVL树是严格平衡的二叉树，红黑树是弱平衡的二叉树。和红黑树相比，AVL树是严格的平衡二叉树，平衡条件必须满足（所有节点的左右子树高度差不超过1）。通过对任何一条从根到叶子的路径上各个节点着色的方式的限制，红黑树确保没有一条路径会比其它路径长出两倍，因此相同节点数的前提下，AVL树的高度往往低于红黑树。同样，在笔试中不会出现，面试时可能会问概念。有兴趣的读者可以自己实现。</p></blockquote><h3 id="C-模拟问题"><a href="#C-模拟问题" class="headerlink" title="C. 模拟问题"></a>C. 模拟问题</h3><ul><li>神奇的多指针 <a href="https://durantthorvalds.top/2020/07/21/2020-7-30-pointers/">&gt;&gt;传送门&lt;&lt;</a></li></ul><blockquote><p>很多时候多指针（双指针，三指针，快慢指针）能极大的帮助我们降低时间复杂度和空间复杂度，比如从$O(n^2)$降低到$O(n)$。例如求链表到数第N个节点，以及判断链表中是否有环，神奇的Floyd判圈法。</p></blockquote><ul><li>「面试向」排序算法 <a href="https://durantthorvalds.top/2020/07/21/2020-7-30-sorting-magic/">&gt;&gt;传送门&lt;&lt;</a></li></ul><blockquote><p>包括冒泡排序，选择排序，插入排序，希尔排序，归并排序，快速排序，堆排序，计数排序，桶排序，基数排序。研究它们算法，以及最好最坏平均时间复杂度和空间复杂度，是否为就地替换以及稳定性。</p></blockquote><ul><li>解空间极大问题通用策略 <a href="https://durantthorvalds.top/2020/08/01/2020-8-1-common-way-solve-big-problem/">&gt;&gt;传送门&lt;&lt;</a></li></ul><blockquote><p>如果输出有非常的项（比如求子集，全排列），要确保结果<strong>完整</strong>且不<strong>重复</strong>，有多种策略：1. 递归，2. 回溯，3. 字典， 4. 数学， 5. 状态压缩， 6. 剪枝技巧</p></blockquote><ul><li>滑动窗口，滑动的艺术 <a href="https://durantthorvalds.top/2020/07/21/2020-8-1-sliding-windows/">&gt;&gt;传送门&lt;&lt;</a></li></ul><blockquote><p>在滑动窗口中有两个指针，一个指针静止，而另一个指针保持移动。我们在s上滑动窗口，如果能够包含整个T（<strong>注意</strong>，T可能有重复字符），如果能收缩，我们就收缩窗口直到得到最小窗口。</p></blockquote><ul><li>红尘客栈之单调栈 <a href="https://durantthorvalds.top/2020/07/21/2020-8-17-recognition-monostack/">&gt;&gt;传送门&lt;&lt;</a></li></ul><blockquote><p>单调栈内元素保持<strong>非递减顺序</strong>，在特定的应用背景下，比如一维参量在一个<strong>连续</strong>范围变化，温度在一段时间变化，股价的增减，柱状图。请考虑单调栈。</p></blockquote><ul><li>数组和字符串的🆒skr操作——前缀和 <a href="https://durantthorvalds.top/2020/10/16/2020-10-16-prefix_sum/">&gt;&gt;传送门&lt;&lt;</a></li></ul><blockquote><p>前缀和的本质是一维或二维差分数组，对区间的查询和修改，比树状数组和线段树相比，不需要特定的数据结构，更加容易使用。</p></blockquote><ul><li>一句话能打败 99.99999%的程序员的位操作代码 <a href="https://durantthorvalds.top/2020/08/23/2020-8-23-bitoperation/">&gt;&gt;传送门&lt;&lt;</a></li></ul><blockquote><p>位操作是计算机最基本的操作之一，它可以与很多问题进行结合，从而优化解法空间复杂度，比如在状态压缩中利用左移，数组low_bit以及<strong>Brian kernighan算法</strong>.</p></blockquote><h3 id="D-贪心算法"><a href="#D-贪心算法" class="headerlink" title="D. 贪心算法"></a>D. 贪心算法</h3><ul><li>贪婪而巧妙的贪心算法 <a href="https://durantthorvalds.top/2020/09/02/2020-8-25-greedy/">&gt;&gt;传送门&lt;&lt;</a></li></ul><blockquote><p>贪心算法（又称贪婪算法）是指，在对问题求解时，总是做出在当前看来是最好的选择。也就是说，不从整体最优上加以考虑，他所做出的是在某种意义上的局部最优解。</p></blockquote><h3 id="E-数学"><a href="#E-数学" class="headerlink" title="E. 数学"></a>E. 数学</h3><ul><li>数学和线性代数用于解题 <a href="https://durantthorvalds.top/2020/09/02/2020-8-25-math/">&gt;&gt;传送门&lt;&lt;</a></li></ul><blockquote><p>数论，求质因数,最大公因数（GCD）,最小公倍数（LCM）, 快速幂算法, Gauss消元法, 几何，排列组会问题等</p></blockquote><h3 id="F-字符串"><a href="#F-字符串" class="headerlink" title="F. 字符串"></a>F. 字符串</h3><ul><li>臭名昭著的KMP算法<a href="https://durantthorvalds.top/2020/07/21/2020-8-29-KMP/">&gt;&gt;传送门&lt;&lt;</a></li></ul><blockquote><p><strong>KMP</strong>算法是用于字符串匹配，十分巧妙，也极难理解。</p></blockquote><ul><li>东方不败——回文问题<a href="https://durantthorvalds.top/2020/07/21/2020-08-19-manacher/">&gt;&gt;传送门&lt;&lt;</a></li></ul><blockquote><p>回文指正反读都一样的字符串，<strong>Manacher</strong>算法专门用于解决回文问题。当然，<strong>Rabin-Karp</strong>编码在一定条件下也是不错的解决问题的方法。</p></blockquote><h3 id="G-图论"><a href="#G-图论" class="headerlink" title="G. 图论"></a>G. 图论</h3><ul><li>再也不想做图类题目了 <a href="https://durantthorvalds.top/2020/07/21/2020-08-1-representation_of_graph/">&gt;&gt;传送门&lt;&lt;</a></li></ul><blockquote><p>本博客介绍了四种图的表示方法，包括 邻接矩阵表示法，关联矩阵表示法，邻接表表示法，弧表示法，星形表示法。以及图的典型算法，包括Dijkstra, Floyd, Bellman-Ford以及SPFA算法。</p></blockquote><ul><li>双胞胎DFS与BFS <a href="https://durantthorvalds.top/2020/09/27/2020-8-12-dfs-bfs/">&gt;&gt;传送门&lt;&lt;</a></li></ul><blockquote><p>二叉树里面的三种遍历既可以用DFS（递归写法），也可以用BFS（迭代+栈），而层序遍历对应的就是BFS。</p><p>在图和树类型题目，以及部分数组题目，都可以时不时看到两种算法同时出现。递归注意1. 触发条件 2. 终止条件 3. 剪枝问题。</p></blockquote><ul><li>浅析最小生成树  <a href="https://durantthorvalds.top/2020/09/18/2020-9-12-minimal-Spanning-tree/">&gt;&gt;传送门&lt;&lt;</a></li></ul><blockquote><p>最小生成树（$Minimal  Spanning  Tree,MST$）：有 n 个结点的<a href="https://baike.baidu.com/item/连通图/6460995">连通图</a>的生成树是原图的极小连通子图，且包含原图中的所有 n 个结点，并且有保持图连通的最少的边。比较常用的有两种算法：$Kruskal$算法和$Prim$算法。</p></blockquote><ul><li>优雅而巧妙的并查集 <a href="https://durantthorvalds.top/2020/08/14/2020-8-14-what-is-UnionSet/">&gt;&gt;传送门&lt;&lt;</a></li></ul><blockquote><p>并查集被许多$OIers$认为是简洁而高雅的数据结构之一，主要用于解决一些<strong>元素分组</strong>的问题，它管理一系列<strong>不相交</strong>的集合，并支持两种操作。即<strong>查询</strong>和<strong>合并</strong>。在求联通/合并问题能大显身手。</p></blockquote><ul><li>求解欧拉通路<a href="https://durantthorvalds.top/2020/08/27/2020-8-27-some_tricks/">&gt;&gt;传送门&lt;&lt;</a></li></ul><blockquote><p>给定一个 <em>n</em> 个点 <em>m</em> 条边的图，要求从指定的顶点出发，经过所有的边恰好一次（可以理解为给定起点的「一笔画」问题），使得路径的字典序最小。常见算法有$Hierholzer$算法。</p></blockquote><ul><li>抛砖引玉析回溯 <a href="https://durantthorvalds.top/2020/08/02/2020-8-2-trackback-demo/">&gt;&gt;传送门&lt;&lt;</a></li></ul><blockquote><p>回溯法，通常以dfs或bfs为载体，在特定的问题下，通过试错，得到所有可能状态，当然有些状态是多余的，因此<strong>剪枝</strong>显得极为重要。</p></blockquote><ul><li>距离向量路由选择算法——DV <a href="https://durantthorvalds.top/2020/12/15/%E8%B7%9D%E7%A6%BB%E5%90%91%E9%87%8F%E8%B7%AF%E7%94%B1%E9%80%89%E6%8B%A9%E7%AE%97%E6%B3%95DV/">&gt;&gt;传送门&lt;&lt;</a></li></ul><blockquote><p>目前的路由器都在运行的算法，你一定不想不知道！</p></blockquote><h3 id="H-综合应用"><a href="#H-综合应用" class="headerlink" title="H. 综合应用"></a>H. 综合应用</h3><ul><li>买股票问题 <a href="https://durantthorvalds.top/2020/08/26/2020-8-26-lotus/">&gt;&gt;传送门&lt;&lt;</a></li></ul><blockquote><p>这类问题涉及很多知识，可能包括dp，单调栈，有限状态自动机等。</p></blockquote><ul><li>权力的游戏之零和博弈  <a href="https://durantthorvalds.top/2020/07/21/2020-9-1-what-is-zero-sum/">&gt;&gt;传送门&lt;&lt;</a></li></ul><blockquote><p>在零和博弈中，让自己最优和让对手最差其实是相同的目标！原因还是那句话，两人的总得分不会变化，自己多了，对手必然减少。没有人是傻子，但是赢者通常会利用游戏规则!</p></blockquote><h3 id="G-其它"><a href="#G-其它" class="headerlink" title="G. 其它"></a>G. 其它</h3><ul><li>有限状态自动机（DSA）<a href="https://durantthorvalds.top/2020/09/02/2020-9-2-fsm/">&gt;&gt;传送门&lt;&lt;</a></li></ul><blockquote><p>确定有限状态自动机（以下简称「自动机」）是一类计算模型。它包含一系列状态，这些状态中：有一个特殊的状态，被称作「初始状态」。还有一系列状态被称为「接受状态」，它们组成了一个特殊的集合。其中，一个状态可能既是「初始状态」，也是「接受状态」。</p></blockquote><ul><li>代码优化系列<a href="https://durantthorvalds.top/2020/09/18/%E4%BB%A3%E7%A0%81%E4%BC%98%E5%8C%96%E7%B3%BB%E5%88%97/">&gt;&gt;传送门&lt;&lt;</a></li></ul><blockquote><p>包括C++,Java, Python代码中很容易遇见的坑，类似于错别字和陷阱，一定要注意。</p></blockquote><ul><li>设计类问题 <a href="">&gt;&gt;传送门&lt;&lt;</a></li></ul><blockquote><p>Leetcode 上典型设计问题的赏析。举一反三，触类旁通。</p></blockquote><ul><li>「研究向」Redis中跳表实现原理<a href="https://durantthorvalds.top/2020/12/01/%E3%80%90%E7%A0%94%E7%A9%B6%E5%90%91%E3%80%91Redis%E4%B8%AD%E8%B7%B3%E8%A1%A8%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/">&gt;&gt;传送门&lt;&lt;</a></li></ul><blockquote><p>在分布式存储系统Redis中一个非常优秀的算法，跳表，LC上也有对应的题目。</p></blockquote><h2 id="🤖机器学习系列"><a href="#🤖机器学习系列" class="headerlink" title="🤖机器学习系列"></a>🤖机器学习系列</h2><blockquote><p>以周志华老师《机器学习》为基准，力争从算法角度解释机器学习的所有问题。</p></blockquote><ul><li>机器学习I 基本概念 <a href="https://durantthorvalds.top/2020/08/04/2020-05-10-ML1/">&gt;&gt;传送门&lt;&lt;</a></li><li>机器学习II 模型评估与选择<a href="https://durantthorvalds.top/2020/08/04/2020-08-4-ML2/">&gt;&gt;传送门&lt;&lt;</a></li><li>机器学习III 线性模型 <a href="https://durantthorvalds.top/2020/08/23/2020-08-23-ML3/">&gt;&gt;传送门&lt;&lt;</a></li><li>机器学习算法I 决策树 <a href="https://durantthorvalds.top/2020/09/27/ML4_DecisionTree/">&gt;&gt;传送门&lt;&lt;</a></li><li>机器学习算法II 神经网络 <a href="[https://durantthorvalds.top/2020/10/02/20201002-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95II-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/](https://durantthorvalds.top/2020/10/02/20201002-机器学习算法II-神经网络/">&gt;&gt;传送门&lt;&lt;</a>)</li></ul><h2 id="🚀大数据与分布式存储系列"><a href="#🚀大数据与分布式存储系列" class="headerlink" title="🚀大数据与分布式存储系列"></a>🚀大数据与分布式存储系列</h2><blockquote><p>以云存储和大数据为研究背景。辐射包括Ceph，Hadoop，纠删码研究等方方面面。</p><p>其中「入门部署」表示初始部署集群，「高级部署」表示有一定挑战性的部署，比如手动部署，「参考」表示一些非核心的帮助理解的内容，「核心」表示深入理解原理，并且熟练操作。「综述」表示一些学术性总结内容。「研究向」表示需要很多时间和兴趣来研究的内容，面试一般不会考到，但很经典。</p></blockquote><h2 id="入门"><a href="#入门" class="headerlink" title="入门"></a>入门</h2><ul><li>「高级部署」ceph手动部署集群<a href="https://durantthorvalds.top/2020/10/21/Ceph%E6%89%8B%E5%8A%A8%E9%85%8D%E7%BD%AE%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/">&gt;&gt;传送门&lt;&lt;</a></li></ul><blockquote><p>包括手动部署，存储集群配置以及<strong>源码编译</strong>等。</p></blockquote><ul><li>「入门理论」ceph基础理论——<a href="https://durantthorvalds.top/2020/10/28/CEPH%E8%B8%A9%E5%9D%91%E5%AD%A6%E4%B9%A0/">&gt;&gt;传送门&lt;&lt;</a></li></ul><blockquote><p>ceph的基本原理，供读者有一个大致了解。</p></blockquote><ul><li>「入门部署」ceph-ansible部署踩坑日记——<a href="https://durantthorvalds.top/2020/11/25/Ceph-ansible%E9%83%A8%E7%BD%B2%E8%B8%A9%E5%9D%91%E6%97%A5%E8%AE%B0/">&gt;&gt;传送门&lt;&lt;</a></li></ul><blockquote><p>ceph-ansible是目前用的最广的ceph部署工具，功能远超ceph-deploy。建议采用此方式。</p></blockquote><ul><li>「入门部署」Ceph-deploy流程 <a href="https://durantthorvalds.top/2020/11/19/Ceph-deploy%E6%B5%81%E7%A8%8B/">&gt;&gt;传送门&lt;&lt;</a></li></ul><blockquote><p>完整的ceph-deploy部署流程。</p></blockquote><ul><li>「综述」纠删码在存储系统的应用——<a href="https://durantthorvalds.top/2020/11/15/%E7%BA%A0%E5%88%A0%E7%A0%81%E5%9C%A8%E5%AD%98%E5%82%A8%E7%B3%BB%E7%BB%9F%E7%9A%84%E5%BA%94%E7%94%A8/">&gt;&gt;传送门&lt;&lt;</a></li></ul><blockquote><p>最最最最最最新的纠删码综述，包括RS码，包括RS码，MSR，LRC等。不断更新，引领前沿。</p></blockquote><h2 id="进阶"><a href="#进阶" class="headerlink" title="进阶"></a>进阶</h2><ul><li>「核心」Ceph三部曲之一：浅析CRUSH算法———<a href="https://durantthorvalds.top/2020/11/27/A%20first%20glance%20at%20CRUSH/">&gt;&gt;传送门&lt;&lt;</a></li></ul><blockquote><p>介绍Ceph核心算法，CRUSH算法。以及相关实践。</p></blockquote><ul><li>「核心」Ceph三部曲之二：ceph纠删码部署——<a href="https://durantthorvalds.top/2020/11/26/ceph%E7%BA%A0%E5%88%A0%E7%A0%81%E9%83%A8%E7%BD%B2/">&gt;&gt;传送门&lt;&lt;</a></li></ul><blockquote><p>ceph中纠删码部署。从原理到五种库源码，笔者研究重点，因为目前Ceph纠删码还有很多待完善的地方。</p></blockquote><ul><li>「核心」Ceph三部曲之三：迁移之美——PG读写流程与状态迁移详解 ———<a href="https://durantthorvalds.top/2020/12/15/%E8%BF%81%E7%A7%BB%E4%B9%8B%E7%BE%8EPG%E8%AF%BB%E5%86%99%E6%B5%81%E7%A8%8B%E4%B8%8E%E7%8A%B6%E6%80%81%E8%BF%81%E7%A7%BB%E8%AF%A6%E8%A7%A3/">&gt;&gt;传送门&lt;&lt;</a></li></ul><blockquote><p>ceph最难理解也最有趣的概念，PG。深入学习必看!</p></blockquote><ul><li>「核心」Ceph三部曲之四:下一代对象存储引擎BlueStore ———<a href="https://durantthorvalds.top/2020/12/27/%E4%B8%8B%E4%B8%80%E4%BB%A3%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8EBlueStore/">&gt;&gt;传送门&lt;&lt;</a></li></ul><blockquote><p>目前的存储介质已经由传统的机械硬盘hdd升级到SSD和NVME，这为下一代对象存储BlueStore提供了基础，相比于目前FileStore，BlueStore拥有无与伦比的优势。</p></blockquote><ul><li>「核心」Ceph学习三部曲之五:控制先行——Ceph的QoS策略<a href="https://durantthorvalds.top/2020/12/28/%E6%8E%A7%E5%88%B6%E5%85%88%E8%A1%8C-Ceph%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0QoS/">&gt;&gt;传送门&lt;&lt;</a></li></ul><blockquote><p>详细介绍Ceph QoS策略，讲解dmClock等算法实现和操作。</p></blockquote><ul><li>「核心」Ceph学习三部曲之六：分布式块存储RBD<a href="https://durantthorvalds.top/2020/12/29/20201220-%E3%80%8C%E6%A0%B8%E5%BF%83%E3%80%8DCeph%E5%AD%A6%E4%B9%A0%E4%B8%89%E9%83%A8%E6%9B%B2%E4%B9%8B%E5%85%AD%EF%BC%9A%E5%88%86%E5%B8%83%E5%BC%8F%E5%9D%97%E5%AD%98%E5%82%A8RBD/">&gt;&gt;传送门&lt;&lt;</a></li></ul><blockquote><p>介绍Ceph分布式块存储，包括快照克隆等功能的理解。</p></blockquote><ul><li>「核心」Ceph学习三部曲之七：对象存储网关RGW<a href="https://durantthorvalds.top/2021/01/03/%E3%80%8C%E6%A0%B8%E5%BF%83%E3%80%8DCeph%E5%AD%A6%E4%B9%A0%E4%B8%89%E9%83%A8%E6%9B%B2%E4%B9%8B%E4%B8%83%EF%BC%9A%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E7%BD%91%E5%85%B3RGW/">&gt;&gt;传送门&lt;&lt;</a></li></ul><blockquote><p>Ceph对象存储使用Ceph对象网关守护进程（<code>radosgw</code>），它是一个用于与Ceph存储群集进行交互的HTTP服务器。由于它提供与OpenStack Swift和Amazon S3兼容的接口，因此Ceph对象网关具有自己的用户管理。</p></blockquote><ul><li>「参考」Ceph pool  ———<a href="https://durantthorvalds.top/2020/12/14/ceph%20pool/">&gt;&gt;传送门&lt;&lt;</a></li></ul><blockquote><p>Ceph的池是理解PG，OSD的桥梁，可以直接操作的对象！</p></blockquote><ul><li>「参考」Ceph配置文件conf ———<a href="https://durantthorvalds.top/2020/12/15/Ceph%20%E9%85%8D%E7%BD%AE%E5%8F%82%E6%95%B0/">&gt;&gt;传送门&lt;&lt;</a></li></ul><blockquote><p>ceph文件配置参考。</p></blockquote><ul><li>「参考」Ceph-Mon详解 ———<a href="https://durantthorvalds.top/2020/11/03/2020113-Ceph-Mon-%E8%AF%A6%E8%A7%A3/">&gt;&gt;传送门&lt;&lt;</a></li></ul><blockquote><p>对Ceph的管理者Monitor进行理解。监视器们维护着集群运行图的“主副本”，就是说客户端连到一个监视器并获取当前运行图就能确定所有监视器、 OSD 和元数据服务器的位置。</p></blockquote><h2 id="大师"><a href="#大师" class="headerlink" title="大师"></a>大师</h2><h2 id="🍜文化"><a href="#🍜文化" class="headerlink" title="🍜文化"></a>🍜文化</h2><blockquote><p>看看就好，笔者的一些随笔和感想。涵盖方面及其广泛。</p></blockquote><ul><li>小白投资入门（煎炸卤炖）———<a href="https://durantthorvalds.top/2020/12/14/%E5%B0%8F%E7%99%BD%E6%8A%95%E8%B5%84%E5%85%A5%E9%97%A8/">&gt;&gt;传送门&lt;&lt;</a></li></ul><blockquote><p>看知乎，学炒股。</p></blockquote><ul><li>日语名谚语系列 ———<a href="https://durantthorvalds.top/2020/11/19/%E6%AF%8E%E6%97%A5%E6%94%BE%E9%80%81%EF%BC%8D%E6%97%A5%E6%9C%AC%E3%81%AE%E8%AB%BA/">&gt;&gt;传送门&lt;&lt;</a></li></ul><blockquote><p>笔者对日语文化有浓厚兴趣，立志考过N1，いしょうに頑張ってなあ！</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;div class=&quot;note note-warning&quot;&gt;
            &lt;p&gt;若您的网页显示mathjax公式出现问题，导致无法阅读，请在任意Mathjax公式上点击右键，&lt;code&gt;Math settings&lt;/code&gt;-&amp;gt;&lt;code&gt;Math rend</summary>
      
    
    
    
    <category term="导航" scheme="http://durantthorvalds.top/categories/%E5%AF%BC%E8%88%AA/"/>
    
    
  </entry>
  
  <entry>
    <title>PAXOS-Made-Simple</title>
    <link href="http://durantthorvalds.top/2021/01/23/PAXOS-Made-Simple%20%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E5%AD%A6%E4%B9%A0Paxos/"/>
    <id>http://durantthorvalds.top/2021/01/23/PAXOS-Made-Simple%20%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E5%AD%A6%E4%B9%A0Paxos/</id>
    <published>2021-01-22T16:00:00.000Z</published>
    <updated>2021-01-23T14:39:09.060Z</updated>
    
    <content type="html"><![CDATA[<h1 id="PAXOS-Made-Simple-深入浅出学习Paxos"><a href="#PAXOS-Made-Simple-深入浅出学习Paxos" class="headerlink" title="PAXOS-Made-Simple 深入浅出学习Paxos"></a>PAXOS-Made-Simple 深入浅出学习Paxos</h1>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;PAXOS-Made-Simple-深入浅出学习Paxos&quot;&gt;&lt;a href=&quot;#PAXOS-Made-Simple-深入浅出学习Paxos&quot; class=&quot;headerlink&quot; title=&quot;PAXOS-Made-Simple 深入浅出学习Paxos&quot;&gt;&lt;/a</summary>
      
    
    
    
    <category term="分布式" scheme="http://durantthorvalds.top/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
    <category term="经典论文" scheme="http://durantthorvalds.top/categories/%E5%88%86%E5%B8%83%E5%BC%8F/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87/"/>
    
    
    <category term="分布式" scheme="http://durantthorvalds.top/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
    <category term="一致性" scheme="http://durantthorvalds.top/tags/%E4%B8%80%E8%87%B4%E6%80%A7/"/>
    
  </entry>
  
  <entry>
    <title>「核心」Ceph学习三部曲之八：分布式文件系统CephFS</title>
    <link href="http://durantthorvalds.top/2021/01/17/2021117-%E3%80%8C%E6%A0%B8%E5%BF%83%E3%80%8DCeph%E5%AD%A6%E4%B9%A0%E4%B8%89%E9%83%A8%E6%9B%B2%E4%B9%8B%E5%85%AB%EF%BC%9A%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9FCephFS/"/>
    <id>http://durantthorvalds.top/2021/01/17/2021117-%E3%80%8C%E6%A0%B8%E5%BF%83%E3%80%8DCeph%E5%AD%A6%E4%B9%A0%E4%B8%89%E9%83%A8%E6%9B%B2%E4%B9%8B%E5%85%AB%EF%BC%9A%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9FCephFS/</id>
    <published>2021-01-16T16:00:00.000Z</published>
    <updated>2021-01-23T14:23:29.782Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Ceph分布式文件系统：CephFS"><a href="#Ceph分布式文件系统：CephFS" class="headerlink" title="Ceph分布式文件系统：CephFS"></a>Ceph分布式文件系统：<em>CephFS</em></h1><p>​    Ceph文件系统或<strong>CephFS</strong>是在Ceph的分布式对象存储<strong>RADOS</strong>之上构建的POSIX兼容文件系统。CephFS致力于为各种应用程序提供最新，多用途，高可用性和高性能的文件存储，包括传统用例（如共享主目录，HPC暂存空间和分布式工作流共享存储）。</p><p>​    CephFS通过使用一些新颖的架构选择来实现这些目标。值得注意的是，文件元数据与文件数据存储在单独的RADOS池中，并通过可调整大小的<em>元数据服务器</em>或<strong>MDS</strong>集群提供服务，该集群可扩展以支持更高吞吐量的元数据工作负载。文件系统的客户端可以直接访问RADOS来读写文件数据块。</p><p>通过MDS集群协调对数据的访问，该集群充当客户端和MDS共同维护的分布式元数据缓存状态的授权机构。每个MDS都会将对元数据的改变写入到日志中。MDS不会在本地存储任何元数据状态。此模型允许在POSIX文件系统的上下文中客户端之间进行连贯且快速的协作。</p><p>CephFS基于MDS对元数据进行管理。它有如下特点:</p><ul><li><p>采用多实例消除性能瓶颈和提高可靠性；</p></li><li><p>采用大型日志文件和延迟删除日志机制来提升元数据的读写性能；</p></li><li><p>将Inode内嵌至Dentry中来提升文件索引效率；</p></li><li><p>采用目录分片来重新定义命名空间的层次结构，并且目录分片可以在MDS实例之间动态迁移，从而实现细粒度的流控和负载均衡机制</p></li></ul><p>  <img src="../img/cephfs-architecture.svg" alt="../_images/cephfs-architecture.svg"></p><p>在正式介绍MDS前，我们先了解以下文件系统基础知识。</p><h2 id="0-了解文件系统"><a href="#0-了解文件系统" class="headerlink" title="0 了解文件系统"></a>0 了解文件系统</h2><p>常见的本地文件系统有：Ext2/3/4，XFS，BTRFS，FAT，NTFS；常见的网络文件系统有NFS，CIFS。随着互联网快速发展，数据规模越来越大，传统的文件系统已经不能满足要求。分布式文件系统应运而生，比如CephFS、Lustre、HDFS、GFS、GlusterFS。</p><p>为了适配不同的文件系统，Linux采用VFS（虚拟文件系统）。</p><p>例如用户写入一个文件，使用POSIX标准的write接口，会被操作系统接管，转调sys_write这个系统调用（属于SCI层）。然后VFS层接受到这个调用，通过自身抽象的模型，转换为对给定文件系统、给定设备的操作，这一关键性的步骤是VFS的核心，需要有统一的模型，使得对任意支持的文件系统都能实现系统的功能。这就是VFS提供的统一的文件模型（common file model），底层具体的文件系统负责具体实现这种文件模型，负责完成POSIX API的功能，并最终实现对物理存储设备的操作。</p><p><img src="../img/70.png" alt="img"></p><p>VFS在系统中的位置如下图。</p><p><img src="../img/image-20210119182434745.png" alt="image-20210119182434745"></p><h2 id="1-文件系统中的元数据"><a href="#1-文件系统中的元数据" class="headerlink" title="1 文件系统中的元数据"></a>1 文件系统中的元数据</h2><p>VFS为了适配不同类型的元数据，定义了4种基本类型：</p><ul><li>$Superblock$：用于管理某一类文件的系统信息；</li><li>$Inode$：索引节点。类Unix文件系统中的一种数据结构，每个Inode保存文件系统中的一个文件系统对象的概要信息，<strong>但不包括文件名和文件内容本身</strong>。<u>Inode和文件名是一对多的关系</u>。</li><li>$Dentry$：目录项。是一个内存结构，由文件系统在内存中直接创立，它是类Unix系统的某个Inode的链接，包含了文件名、文件的Inode号等信息；</li><li>$File$：文件操作句柄，和进程相关，表示一个打开的文件，<u>File和Inode之间是一对多的关系</u>，因为多个进程可以打开一个文件。</li></ul><p>由于Inode和Dentry需要后端文件系统提供服务功能，需要重点介绍：</p><p>具体而言：</p><p>$a$. Inode只记录数据块在存储介质上的位置和分布，以及文件对象属性（包括权限、数据块信息、时间戳等），不包括文件名和内容等变长数据。Inode结构大小固定，但我们发现要找到具体的存储位置，还缺少文件在目录树中的位置信息，因此需要引入Dentry。</p><p>$b$. Dentry在文件系统中起到链接不同Inode的作用。Dentry包含文件名、文件名Inode等信息。它们最终组成文件目录树的结构。它的原理是：本级的Dentry记录了本级目录或者文件名以及下一级目录或者文件的Dentry位置，此外，Dentry本身也需要在具体的Inode对象中，所以它也有自己的Inode号。<strong>Dentry本身像一张表，记录了Inode和文件名的对应关系</strong>。下面这张图很好的表示了inode和dentry的关系。</p><p><img src="../img/1055408-20161104001642736-837882075.png" alt="img"></p><blockquote><p>图片来源：<a href="https://www.cnblogs.com/mister-lv/p/6028836.html">https://www.cnblogs.com/mister-lv/p/6028836.html</a></p></blockquote><h2 id="2-硬链接和软链接"><a href="#2-硬链接和软链接" class="headerlink" title="2 硬链接和软链接"></a>2 硬链接和软链接</h2><blockquote><p>推荐阅读：<a href="https://www.cnblogs.com/fqnb001/p/8778790.html">什么是软链接，硬链接</a></p></blockquote><p>为了实现文件共享，隐藏文件路径、文件安全和节省存储空间等功能或特性，需要引入链接这个概念。</p><h3 id="硬链接"><a href="#硬链接" class="headerlink" title="硬链接"></a>硬链接</h3><p>硬链接指多个文件名指向同一个Inode号(在MDS就是多个Dentry指向一个Inode)，可以用不同的文件名访问同样的内容；对文件内容进行修改，会影响到所有文件名；但是，删除一个文件名，不影响另一个文件名的访问（只有一个Inode的所有文件都被删除时，文件才会真正被删除），注意目录不能用于创建硬链接（防止形成环）。目录中的隐藏的.和..就是典型的硬链接。</p><pre><code class="hljs awk">ln 源文件 目标文件 <span class="hljs-regexp">//</span>创建硬链接ls -li <span class="hljs-regexp">//</span>查看链接</code></pre><h3 id="软链接"><a href="#软链接" class="headerlink" title="软链接"></a>软链接</h3><p>软链接是创建一个新的Inode，Inode的存储内容是另外一个文件路径名的指向，可以理解为<strong>windows的快捷方式</strong>。它的特点是可以灵活地实现诸多不做限制的要求：既可以对存在或者不存在的文件和目录创建软链接，也可以链接到不同的文件系统，还可以在删除链接时不影响指向的文件等。</p><pre><code class="hljs awk">ln -s 源文件 目标文件 <span class="hljs-regexp">//</span>查看软链接</code></pre><pre><code class="hljs awk">ll &#123;文件名&#125; <span class="hljs-regexp">//</span> 查看所有链接</code></pre><blockquote><p>题外话：讲个笑话，各位小朋友千万不要模仿（除非你想删库跑路）！</p><p><img src="../img/image-20210119192857642.png" alt="image-20210119192857642"></p></blockquote><p>扩展：</p><p><a href="https://blog.csdn.net/qq_37806908/article/details/97686753">linux .so .o .a文件区别</a></p><h2 id="3-日志"><a href="#3-日志" class="headerlink" title="3 日志"></a>3 日志</h2><p>日志是一种特殊的文件，用于循环记录文件系统的修改，并定期提交到文件系统进行保存。一旦系统发生崩溃，日志可以起到<strong>存档点</strong>的作用。</p><p>常见的日志设计模式无外乎以下三种：</p><p>（1）writeback</p><p>writeback模式只有元数据被写入到数据中，这样虽然可以保证日志一致性，但是可能引起日志崩溃。</p><p>（2）ordered</p><p>ordered模式也是只将元数据写入到日志，但是前提是数据已经写入数据盘。但是在数据写入硬盘后而元数据写入前系统崩溃的情景，就会发生数据丢失。</p><p>（3）data</p><p>data模式将元数据和数据都写入日志，这样可以最大限度的防止文件系统崩溃所导致的数据丢失，但是因为数据写入两次，性能会下降。</p><p>日志提交有两种方式：超时提交和满时提交。超时提交，顾名思义指在规定时间之后，日志会自动同步；满时提交指日志存储空间到达上限才会触发同步。一般文件系统会同时采用这两种模式。</p><hr><h2 id="正式部分"><a href="#正式部分" class="headerlink" title="正式部分"></a>正式部分</h2><h2 id="S1-CephFS"><a href="#S1-CephFS" class="headerlink" title="$\S1$ CephFS"></a>$\S1$ CephFS</h2><p>分布式系统通过多服务器实现负载均衡，为了快速索引元数据，必须将元数据和用户数据进行分离，这是Ceph最杰出的贡献之一。</p><p>为了实现负载均衡，有如下几种方式：</p><p>（1）静态子树分区</p><p>即通过手工的方式进行数据分配，典型的有Sprite、StorageTnak和PanFS。</p><p>（2）Hash计算分区法</p><p>​    通过hash计算来分配数据存储位置。这种方式适合数据分布均衡且需要应对各种异常的场景，但不太适合数据分布固定的场景。</p><p>（3）动态子树分区</p><p>​    通过实时监控集群节点的负载，动态调整子树分布于不同节点，这是Ceph默认的方式。对于有大量数据迁移的场景不合适。</p><p>虽然元数据不适合通过Hash计算的方式，但是为什么CephFS的元数据仍然基于Hash方式？</p><p>答案是Ceph元数据访问并不是之间从RADOS获取，而是存在一个元数据缓存区，其中元数据基于动态子树分区的方式进行分配，元数据的存在哪里就不是那么重要。</p><p>在此基础，我们得到CephFS的架构图。</p><p><img src="../img/image-20210120231307670.png" alt="image-20210120231307670"></p><p>为了加快数据访问效率，MDS将热点数据缓存在内存中。</p><p>CephFS存在以下三种形式的客户端接口：</p><p>（1）CephFS Kernel Object</p><p>为内核态接口，它使用mount -t ceph命令将CephFS挂载在操作系统指定目录下。</p><p>（2）Ceph FUSE</p><p>FUSE的全称是Filesystem in Userspace，即用户空间文件系统。它完全是在内核态实现的。</p><p>（3）User Space Client</p><p>为直接通过客户端应用程序调用CephFS提供的文件系统接口，比如Hadoop调用CephFS提供的Java文件系统接口实现文件转储。</p><h2 id="S2-Ceph-MDS服务器特质"><a href="#S2-Ceph-MDS服务器特质" class="headerlink" title="$\S2 $  Ceph MDS服务器特质"></a>$\S2 $  Ceph MDS服务器特质</h2><p>​    当进程打开文件时，客户端将请求发送到MDS群集。  MDS遍历文件系统层次结构以将文件名转换为文件inode，其中包括唯一的inode编号，文件所有者，模式，大小以及其他每个文件元数据。 如果文件存在并且已授予访问权限，则MDS返回索引节点的数目，文件大小以及有关用于将文件数据映射到对象的分条策略的信息。  </p><p>​    MDS还可以向客户端发出一项功能，以指定允许哪些操作。 功能包括控制客户端读取，缓存读取，写入和缓冲区写入的能力和安全密钥。 随后的MDS参与文件I / O的工作仅限于管理功能，以保持文件一致性并获得适当的语义。</p><p>​    Ceph概括了一系列条带化策略，可将文件数据映射到一系列对象上。 为了避免文件分配元数据的任何需要，对象名称只需组合文件索引节点号和条带号即可。 然后使用CRUSH（见三部曲其一blog）将对象副本分配给OSD。 例如，如果一个或多个客户端打开文件进行读取访问，则MDS会授予他们读取和缓存文件内容的功能。 有了索引节点的数目，布局和文件大小，客户端可以命名和定位包含文件数据的所有对象，并直接从OSD集群读取。</p><h3 id="2-1客户端同步"><a href="#2-1客户端同步" class="headerlink" title="2.1客户端同步"></a>2.1客户端同步</h3><p>​    POSIX语义明智地要求读取反映先前写入的任何数据，并且写入是原子的（即，重叠的并发写入的结果将反映特定的发生顺序）。 当多个客户端同时使用多个写入器或混合使用读取器和写入器打开文件时，MDS将撤消以前发布的任何读取缓存和写入缓冲功能，从而强制该文件的客户端I / O同步。 也就是说，每个应用程序的读取或写入操作将一直阻塞，直到OSD确认为止，从而有效地将更新序列化和同步的负担与存储每个对象的OSD放在了一起。 当写入跨越对象边界时，客户端会在受影响的对象上获得排他锁（由其各自的OSD授予），并立即提交写入和解锁操作以实现所需的序列化。 对象锁类似地用于通过获取锁并异步刷新数据来掩盖大写操作的延迟。</p><h3 id="2-2-元数据动态性"><a href="#2-2-元数据动态性" class="headerlink" title="2.2 元数据动态性"></a>2.2 元数据动态性</h3><p>​    Ceph中的文件和目录元数据很小，几乎完全由目录条目（文件名）和索引节点（80字节）组成。 与常规文件系统不同，不需要文件分配元数据-使用索引节点号构造对象名称，并使用CRUSH将其分配给OSD。 这简化了元数据的工作量，并使我们的MDS可以有效地管理非常大的文件集，而与文件大小无关。<br>   我们的设计进一步寻求通过使用两层存储策略来最小化与元数据相关的磁盘I / O，并通过动态子树分区[30]来最大化本地性和缓存效率。</p><h3 id="2-3-元数据存储"><a href="#2-3-元数据存储" class="headerlink" title="2.3 元数据存储"></a><strong>2.3 元数据存储</strong></h3><p>   日志的设计使得在MDS发生故障的情况下，另一个节点可以快速重新扫描日志以恢复故障节点的内存高速缓存中的关键内容（用于快速启动） 并以此重新覆盖文件系统状态。</p><p>​    该策略提供了两全其美的优势：<strong>以高效（顺序）的方式将更新流式传输到磁盘，并大大减少了重写工作量，从而可以针对未来的读取优化磁盘上的长期存储布局访问</strong>。</p><p><img src="../img/image-20210120232711333.png" alt="image-20210120232711333"></p><blockquote><p>图1：Ceph根据当前工作负载将目录层次结构的子树动态映射到元数据服务器。 仅当多个目录成为热点时，它们才会在多个节点间以散列形式出现。</p></blockquote><h3 id="2-4-动态子树分区"><a href="#2-4-动态子树分区" class="headerlink" title="2.4 动态子树分区"></a>2.4 动态子树分区</h3><p>​    我们的主副本缓存策略使单个权威MDS负责管理缓存一致性并为任何给定的元数据片段序列化更新。 </p><p>​     目前有动态子树分区和静态子树分区两种方式。静态子树分区无法应对动态的工作负载和数据集，而散列则破坏了元数据的局部性和有效的元数据预取和存储的关键机会。</p><p>   Ceph的MDS集群基于动态子树分区策略，该策略可在一组节点上自适应地分层分配缓存的元数据，如图1所示。每个MDS使用计数器来度量目录层次结构中元数据的统计分布， 它随指数时间衰减。 <strong>任何操作都会将受影响的索引节点及其所有祖先上的计数器递增到根目录，为每个MDS提供一个加权树，描述最近的负载分布。 定期比较MDS负载值，并迁移目录层次结构的适当大小的子树以保持工作负载均匀分布。</strong></p><p>总结 MDS优势在于</p><ol><li>以较大块对象形式存储减少了元数据的数量；</li><li>每个MDS独立更新自己的日志；</li><li>动态子树分区实现了文件系统的动态负载均衡；</li><li>元数据复制保证了MDS节点之间缓存一致性和MDS失败和超载下，相应元数据被迁移到正常MDS上；</li><li>锁机制保证了元数据复制；</li><li>流量控制解决了大量不可预测的用户的请求。</li></ol><h2 id="S3-MDS实现原理"><a href="#S3-MDS实现原理" class="headerlink" title="$\S3$  MDS实现原理"></a>$\S3$  MDS实现原理</h2><h3 id="3-1-元数据与Object的关系"><a href="#3-1-元数据与Object的关系" class="headerlink" title="3.1 元数据与Object的关系"></a>3.1 元数据与Object的关系</h3><p>CephFS中将Inode编号设置为Object名称。而实际Object通常被设置为固定大小，如果Dentry大小大于这个大小，就需要多个object来保存。一个标准的Object集合是以相同Inode开头加上所有Stripe的Objects，它包括一个完整Dentry信息。</p><p>根目录默认Inode编号1，它存储目录下所有文件和目录的Dentry。查询文件的过程也是根据Inode编号从根目录搜索到叶子节点的过程。</p><p>需要注意的是，元数据不是直接写入到Object，而是先顺序写入到条带化、固定大小的日志中，再根据落盘政策写入到后端object。</p><pre><code class="hljs stata">rados -p <span class="hljs-keyword">meta</span> <span class="hljs-keyword">ls</span><span class="hljs-comment">//标准元数据集合</span></code></pre><h3 id="3-2-嵌入式Inode和Primary-Dentry"><a href="#3-2-嵌入式Inode和Primary-Dentry" class="headerlink" title="3.2 嵌入式Inode和Primary Dentry"></a>3.2 嵌入式Inode和Primary Dentry</h3><p>为了提升性能，文件系统通常将Inode放置再Dentry附近，这样读取Dentry时可以同时将Inode获取，这便是嵌入式Inode。</p><p>在没有硬链接时，Dentry就是Primary Dentry。一般情况下存储Dentry对应Inode只占用一个Object；超过一个Object大小的，其扩展的Object也是存储在一起，依次读就可以获得Dentry和Inode信息，将此信息缓存并使用LRU算法进行淘汰。</p><h3 id="3-3-Remote-Dentry和Anchor"><a href="#3-3-Remote-Dentry和Anchor" class="headerlink" title="3.3 Remote Dentry和Anchor"></a>3.3 Remote Dentry和Anchor</h3><p>当存在硬链接时，第一个指向Inode的Dentry被称为Primary Dentry，后续的被称为Remote Dentry。</p><p>为了访问Remote Dentry， 人们提出了Anchor，它包括Path，Inode，Parent，Ref.前三者显而易见，Ref是被Inode引用的次数（即2.4节提到的计数器）。当进行目录重命名时，可能会影响整个链上的Inode，此时就需要一个事务来保证整个链上相关的Inode同时进行更新，将旧的Ref计数减少，新的Ref计数删除。如果Ref为0，说明Inode已经没有硬链接，可以从Anchor表删除，对Dentry的增加修改同理。</p><h3 id="3-4-日志"><a href="#3-4-日志" class="headerlink" title="3.4 日志"></a>3.4 日志</h3><p>MDS日志使用的是混合模式：</p><p>1）更新首先会写入到MDS的日志中；</p><p>2）将有改变的元数据标记为“dirty”，并在MDS缓存中置为“pinned”。</p><p>3）最终修改会更新到具体元数据对象中，但同时也会做延时处理直到从日志中剪掉，这使得日志可以变得非常大（数百兆）。</p><p>引入日志是CephFS的巨大创新，它不但能获取延迟回写和分组提交等提升性能的方法。</p><p>而且还能减少Dentry的更新：1. 大多数负载，比如多次修改同一个地方，或者临时文件创建和删除，其实在日志并未创建；2. 在日志的生命周期内，对给定Dentry的所有更新都被有效提交，所有这些更新被一次性提交。</p><p>日志除了故障恢复还支持在恢复MDS时启用其缓存保存大量热元数据，热数据来自日志，避免从冷缓存（即从后端RADOS中随机读取到缓存中）开始的效率加载而导致的大量等待获取元数据的I/O，从而加快MDS恢复。</p><p>每一个MDS会维护一个日志系统，其日志保存了最近创建和修改，但还未提交到Object文件中的内容，日志被切分为固定大小且有序的Object。</p><p>日志条目，MDS用它来跟踪元数据的变化信息。日志条目使用Metablob来描述单个元数据更新，每一个Metablob包含一个或多个目录的Fragment ID、Dentry和Inode。</p><h3 id="3-5-MDS负载均衡的实现"><a href="#3-5-MDS负载均衡的实现" class="headerlink" title="3.5 MDS负载均衡的实现"></a>3.5 MDS负载均衡的实现</h3><p>许多分布式文件系统使用静态子树分区(Static Subtree Partition) 的方式来实现，即固定地将不同层目录树分配到不同的服务器上，这种方法需要人工调整，十分不灵活。</p><p>Ceph使用动态子树分区来实现横向扩展，动态子树能够根据负载状况来自动迁移目录树，同时为了实现多个MDS管理目录的负载而支持细粒度分区。Sage Weil重点研究了在MDS缓存中存储元数，并且具备伸缩性和容忍任意MDS节点异常。</p><h3 id="3-5-1-目录分区"><a href="#3-5-1-目录分区" class="headerlink" title="3.5.1 目录分区"></a>3.5.1 目录分区</h3><p>Ceph扩展了目录层次结构以允许单目录内容被分解为多个片段，称为Fragments。因为Inode和Fragments是一对多的关系，它存储在目录Inode的FragTree结构中，它基于一个内部顶点开始进行2的N次幂分割。如下图，目录树被分割为一个或者多个Fragments，其中树叶是单独的Fragments。</p><p><img src="../img/image-20210123211806521.png" alt="image-20210123211806521" style="zoom:67%;" /></p><p>​    每一个Fragments都通过bitmask值来进行描述，类似ip和掩码。通过哈希文件和在FragTree中查找结果值，来实现目录Inode到Fragments 的映射。由于子树元数据被定义在一组目录Fragment中，所以可以通过在MDS之间迁移Fragment来实现负载均衡，任何Fragments在变大或者繁忙时都能分裂为2^n个子fragment。</p><h3 id="3-5-2-子树分区"><a href="#3-5-2-子树分区" class="headerlink" title="3.5.2 子树分区"></a>3.5.2 子树分区</h3><p>​    Ceph支持在MDS集群中，任意和自适应的对子树进行分区。在某些情况下，诸如在同一个目录下有多个文件是热点对象，目录定义子树的方式无法进行负载分离，但Fragment的分割功能允许将这些热点文件进行动态分离。</p><p>​    根目录所在的MDS被标记为MDS-0，其它的MDS则根据当前整个集群元数据的负载进行子树分区。下图展现了MDS的缓存结构：</p><h3 id="3-5-3-元数据复制"><a href="#3-5-3-元数据复制" class="headerlink" title="3.5.3 元数据复制"></a>3.5.3 元数据复制</h3><p>元数据复制的目的在于1. 保证各MDS节点缓存之间的一致性；MDS之间进行元数据复制操作，从而当MDS失败或者负载过重时能将元数据复制到其它正常的MDS上。</p><h3 id="3-5-4-锁机制"><a href="#3-5-4-锁机制" class="headerlink" title="3.5.4 锁机制"></a>3.5.4 锁机制</h3><p>每个Inode有5种锁状态，每一个控制不同的相关字段，例如：link计数和anchor状态字段，文件所有者模式字段、文件size、文件mtime和fragment字段。</p><p>总结一下，MDS负载均衡的实现原理。每个MDS会监视统计Inode和Fragment在缓存中的热度。每个Inode会从读和写来统计热度，而fragment会另外统计readdir操作情况、元数据获取频率、写入到对象存储的频率来统计热度。</p><p>在客户端请求时，受影响的元数据计数器会增加，元数据祖先也会受到影响，反过来又影响到复制和迁移的策略。MDS节点之间会定时分析它们的负载水平。如果某个节点负载过高，它将根据热度统计计数器来选择合适的子树进行迁移。</p><p>关于<strong>迁移</strong>，首先，目的MDS将所有需要迁移的子树元数据副本导入进来，然后源MDS通过日志导出事件提交迁移。在源MDS迁移之前，任何其他MDS会接收到要复制子树的元数据的信息，以此来获取被迁移子树的元数据信息。</p><h3 id="3-5-5-流量控制"><a href="#3-5-5-流量控制" class="headerlink" title="3.5.5 流量控制"></a>3.5.5 流量控制</h3><p>设想有成千上万的客户端想访问任何集群中存储的元数据，如果它们集中在一个MDS，那么很难有效的处理请求。理想的情形是 <strong>对非频繁访问的元数据之间到权威MDS获取，而频繁访问的条目分发到多个MDS中。</strong></p><p>这样做的关键在于客户端缓存了访问MDS的记录。首先利用客户端开始查询位置元数据的分布情况，然后再客户端访问后会收到MDS回复，即未来获取元数据的MDS编号。对非频繁访问的元数据之间到权威MDS获取，而频繁访问的条目则MDS集群会告知客户端随机到所有MDS中去获取，这样就实现了客户端访问MDS的流量控制。</p><h2 id="S4-MDS故障恢复"><a href="#S4-MDS故障恢复" class="headerlink" title="$\S4$ MDS故障恢复"></a>$\S4$ MDS故障恢复</h2><hr><h1 id="部署和操作CephFS"><a href="#部署和操作CephFS" class="headerlink" title="部署和操作CephFS"></a>部署和操作CephFS</h1><pre><code class="hljs dockerfile">ceph fs <span class="hljs-keyword">volume</span><span class="bash"> create &lt;fs name&gt;</span></code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Ceph分布式文件系统：CephFS&quot;&gt;&lt;a href=&quot;#Ceph分布式文件系统：CephFS&quot; class=&quot;headerlink&quot; title=&quot;Ceph分布式文件系统：CephFS&quot;&gt;&lt;/a&gt;Ceph分布式文件系统：&lt;em&gt;CephFS&lt;/em&gt;&lt;/h1&gt;</summary>
      
    
    
    
    <category term="ceph" scheme="http://durantthorvalds.top/categories/ceph/"/>
    
    <category term="分布式存储" scheme="http://durantthorvalds.top/categories/ceph/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/"/>
    
    
    <category term="系统架构" scheme="http://durantthorvalds.top/tags/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84/"/>
    
    <category term="ceph" scheme="http://durantthorvalds.top/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title>逆向思维</title>
    <link href="http://durantthorvalds.top/2021/01/16/%E9%80%86%E5%90%91%E6%80%9D%E7%BB%B4/"/>
    <id>http://durantthorvalds.top/2021/01/16/%E9%80%86%E5%90%91%E6%80%9D%E7%BB%B4/</id>
    <published>2021-01-15T16:00:00.000Z</published>
    <updated>2021-01-16T09:27:25.150Z</updated>
    
    <content type="html"><![CDATA[<h1 id="逆向思维"><a href="#逆向思维" class="headerlink" title="逆向思维"></a>逆向思维</h1><h4 id="174-地下城游戏"><a href="#174-地下城游戏" class="headerlink" title="174. 地下城游戏"></a><a href="https://leetcode-cn.com/problems/dungeon-game/">174. 地下城游戏</a></h4><h4 id="803-打砖块-【打砖块——补砖块】"><a href="#803-打砖块-【打砖块——补砖块】" class="headerlink" title="803. 打砖块 【打砖块——补砖块】"></a><a href="https://leetcode-cn.com/problems/bricks-falling-when-hit/">803. 打砖块</a> 【打砖块——补砖块】</h4><h4 id="312-戳气球【戳气球——放气球】"><a href="#312-戳气球【戳气球——放气球】" class="headerlink" title="312. 戳气球【戳气球——放气球】"></a><a href="https://leetcode-cn.com/problems/burst-balloons/">312. 戳气球</a>【戳气球——放气球】</h4><p>注意：一般而言，用到逆向思维的都是困难题。</p><p>所谓逆向思维就是我们要从题目的反面去考虑。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;逆向思维&quot;&gt;&lt;a href=&quot;#逆向思维&quot; class=&quot;headerlink&quot; title=&quot;逆向思维&quot;&gt;&lt;/a&gt;逆向思维&lt;/h1&gt;&lt;h4 id=&quot;174-地下城游戏&quot;&gt;&lt;a href=&quot;#174-地下城游戏&quot; class=&quot;headerlink&quot; title=</summary>
      
    
    
    
    <category term="算法" scheme="http://durantthorvalds.top/categories/%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="困难" scheme="http://durantthorvalds.top/tags/%E5%9B%B0%E9%9A%BE/"/>
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://durantthorvalds.top/2021/01/13/%E3%80%8C%E9%98%85%E8%AF%BB%E3%80%8D%E7%A8%8B%E5%BA%8F%E5%91%98%E7%9A%84%E6%9C%89%E8%B6%A3%E9%A2%98%E7%9B%AE%F0%9F%8E%A8/"/>
    <id>http://durantthorvalds.top/2021/01/13/%E3%80%8C%E9%98%85%E8%AF%BB%E3%80%8D%E7%A8%8B%E5%BA%8F%E5%91%98%E7%9A%84%E6%9C%89%E8%B6%A3%E9%A2%98%E7%9B%AE%F0%9F%8E%A8/</id>
    <published>2021-01-13T07:35:45.972Z</published>
    <updated>2021-01-16T09:29:42.139Z</updated>
    
    <content type="html"><![CDATA[<h1 id="「阅读」程序员的有趣题目🎨"><a href="#「阅读」程序员的有趣题目🎨" class="headerlink" title="「阅读」程序员的有趣题目🎨"></a>「阅读」程序员的有趣题目🎨</h1>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;「阅读」程序员的有趣题目🎨&quot;&gt;&lt;a href=&quot;#「阅读」程序员的有趣题目🎨&quot; class=&quot;headerlink&quot; title=&quot;「阅读」程序员的有趣题目🎨&quot;&gt;&lt;/a&gt;「阅读」程序员的有趣题目🎨&lt;/h1&gt;</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://durantthorvalds.top/2021/01/03/2021-1-3%E6%8E%A2%E7%A7%98FUSE%EF%BC%8C%E8%AE%B2%E8%AE%B2%E6%95%85%E4%BA%8B/"/>
    <id>http://durantthorvalds.top/2021/01/03/2021-1-3%E6%8E%A2%E7%A7%98FUSE%EF%BC%8C%E8%AE%B2%E8%AE%B2%E6%95%85%E4%BA%8B/</id>
    <published>2021-01-03T10:23:35.544Z</published>
    <updated>2021-01-03T10:25:11.147Z</updated>
    
    <content type="html"><![CDATA[<h1 id="探秘FUSE，讲讲故事"><a href="#探秘FUSE，讲讲故事" class="headerlink" title="探秘FUSE，讲讲故事"></a>探秘FUSE，讲讲故事</h1><blockquote><p>Github Release<a href="https://github.com/libfuse/libfuse/releases">下载</a></p><p>SourceForge </p></blockquote><p>FUSE,($Filesystem In Userspace$)，用户空间文件系统（Filesystem in Userspace），是Linux 中用于挂载某些<a href="https://baike.baidu.com/item/网络空间/7650101">网络空间</a>，如<a href="https://baike.baidu.com/item/SSH/10407">SSH</a>，到本地文件系统的模块。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;探秘FUSE，讲讲故事&quot;&gt;&lt;a href=&quot;#探秘FUSE，讲讲故事&quot; class=&quot;headerlink&quot; title=&quot;探秘FUSE，讲讲故事&quot;&gt;&lt;/a&gt;探秘FUSE，讲讲故事&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;Github Release&lt;a href</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>「核心」Ceph学习三部曲之七：对象存储网关RGW</title>
    <link href="http://durantthorvalds.top/2021/01/03/%E3%80%8C%E6%A0%B8%E5%BF%83%E3%80%8DCeph%E5%AD%A6%E4%B9%A0%E4%B8%89%E9%83%A8%E6%9B%B2%E4%B9%8B%E4%B8%83%EF%BC%9A%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E7%BD%91%E5%85%B3RGW/"/>
    <id>http://durantthorvalds.top/2021/01/03/%E3%80%8C%E6%A0%B8%E5%BF%83%E3%80%8DCeph%E5%AD%A6%E4%B9%A0%E4%B8%89%E9%83%A8%E6%9B%B2%E4%B9%8B%E4%B8%83%EF%BC%9A%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E7%BD%91%E5%85%B3RGW/</id>
    <published>2021-01-02T16:00:00.000Z</published>
    <updated>2021-01-17T12:28:12.946Z</updated>
    
    <content type="html"><![CDATA[<div class="note note-primary">            <p>本blog包括理论和实践两个部分，实践部分需要您事先部署成功Ceph集群！</p><p>参考《Ceph设计与实现》谢型果等，第六章。以及<a href="https://docs.ceph.com/en/latest/man/8/rados/">官方RADOS指南</a>，以及<a href="https://docs.ceph.com/en/latest/radosgw/">官方RGW教程</a></p>          </div><h1 id="1-概述"><a href="#1-概述" class="headerlink" title="1.概述"></a>1.概述</h1><p>前面我们讲过，Ceph集成了BlueStore分布式对象存储，针对非结构化的数据，比如静态数据，备份存储以及流媒体等场景。上一节，我们介绍了RADOS 中的RBD模块，并且rados提供了API接口librados供用户使用。这一节我们将介绍RADOS Gateway , 即RADOS网关，它主要支持两种类型的接口：</p><ul><li><strong>与S3兼容</strong>：为对象存储功能提供与Amazon S3 RESTful API的大部分子集兼容的接口。</li><li><strong>兼容Swift</strong>：为对象存储功能提供与OpenStack Swift API的大部分子集兼容的接口。</li></ul><p>因为Ceph核心模块RADOS提供的访问接口是私有接口，不支持通用的HTTP协议访问，因而为了支持HTTP协议访问，涉及了支持RESTful接口访问而设计的RADOS gateway.</p><blockquote><p><strong>官方定义</strong>：Ceph对象存储使用Ceph对象网关守护进程（<code>radosgw</code>），它是一个用于与Ceph存储群集进行交互的HTTP服务器。由于它提供与OpenStack Swift和Amazon S3兼容的接口，因此Ceph对象网关具有自己的用户管理。Ceph对象网关可以将数据存储在用于存储来自Ceph文件系统客户端或Ceph块设备客户端的数据的同一Ceph存储群集中。S3和Swift API共享一个公共的名称空间，因此可以使用一个API编写数据，而使用另一个API检索数据。</p><p>另外，<strong>Ceph对象存储也未使用MDS服务器！</strong></p></blockquote><p><img src="/img/1ae399f8fa9af1042d3e1cbf31828f14eb3fe01a6eb3352f88c3d2a04ac4dc50.png" alt=""></p><p>RGW作为对象存储网关系统，一方面扮演RADOS集群客户端角色，为对象存储应用提供RESTful接口；另一方面，扮演HTTP角色，接收并解析互联网传输数据。RGW目前支持主流的WEB服务器：Civetweb，Apache，Nginx等，其中Civetweb是一个C++库，可以内嵌到RGW框架中，是RGW默认的WEB服务器；Apache与Nginx需要以独立进程存在，收到应用请求后，通过RGW注册的监听端口号将请求转发到RGW上进行处理。</p><h2 id="1-1-数据组织"><a href="#1-1-数据组织" class="headerlink" title="1.1 数据组织"></a>1.1 数据组织</h2><p>一个对象存储系统包括三个部分：用户、存储桶和对象。</p><ul><li><strong>用户</strong>：指对象存储应用的使用者。一个用户拥有一个或多个存储桶。</li><li><p><strong>存储桶</strong>：是对象的容器，设置这一层级的目的是方便关联和操作具有同一属性的一类对象而引入的一层关联单元。</p></li><li><p><strong>对象</strong>：对象是存储的基本单位，包括数据和元数据两个部分。其中元数据在类型和数目上不受限制。与文件系统不同，对象存储系统中所有对象以扁平的方式存储，对象之间没有之间的关联。并且，对象存储不提供部分编辑功能，这意味着，即使更新一个字符，也必须将整个对象从云端下载下来，更新后上传。</p></li></ul><p>以Amazon S3为例，它数据实体包括user、bucket、object，如下图所示；而OpenStack将用户的概念细分为account和user，其中account对应一个项目或者租户，每个account可以被多个user共享，其他的集成实体比如container和object与以上的存储桶、对象概念相符。</p><p>RGW为了兼容Amazon S3和OpenStack接口，所以将用户分为user和subuser，分别对应S3用户和Swift用户。</p><p><img src="/img/image-20210103132412544.png" alt="image-20210103132412544" style="zoom:67%;" /></p><p><img src="/img/image-20210103132643135.png" alt="image-20210103132643135" style="zoom:67%;" /></p><p><img src="/img/image-20210103132828794.png" alt="image-20210103132828794" style="zoom:67%;" /></p><p>我们将详细讨论，这些数据实体所包含的信息和数据组织形式，由上一期我们知道，数据存在RADOS有三种方式，第一种是二进制；第二种是以键值对存在扩展属性xattr中；第三种是存在扩展属性omap中。</p><h2 id="1-2-用户"><a href="#1-2-用户" class="headerlink" title="1.2 用户"></a>1.2 用户</h2><p>对用户的设计管理主要包含以下几个方面：首先是为了对RESTful API进行请求认为，其次是为了控制用户对存储资源的访问权限，最后是为了控制用户的可用存储空间，因此一个用户包含的信息包括用户认证信息、访问认证控制权限信息和配额信息。</p><p>我们首先介绍RGW的认证机制，RGW针对S3 API和Swift API采用不同的认证机制。</p><p>S3用户认证兼容AWS2和AWS4两种认证机制，它们都是基于密匙认证。</p><p>认证过程如下：</p><p>1）应用在发送请求之前，使用用户私有密匙（secret_key）、请求内容等，采用与RGW网关约定好的算法计算出数字签名后，将数字签名以及用户访问密匙（access_key）封装在请求中发送给RGW网关。</p><p>2）RGW网关收到请求后，使用用户访问密匙作为索引从RADOS集群中读取用户信息，并从用户信息中获取用户私有密匙。</p><p>3）使用用户私有密匙、请求内容等，采用与约定好的算法计算数字签名。</p><p>4）判断RGW生成的数字签名与请求的签名是否匹配，如果是匹配的，则认为请求是真实的，用户认证通过。</p><p>对于Swift，采用的是令牌认证(token)。</p><p>1）应用在发出真正的操作请求前，向RGW网关请求一个有时限的令牌。</p><p>2）RGW收到令牌后，使用子用户ID作为索引从RADOS集群中读取出子用户信息，并使用子用户信息中获取到的Swift私有密匙生成一个令牌返回给应用。</p><p>3）应用在后续的操作中携带该令牌，RGW收到操作请求后，采用与（2）相同的方式生成一个令牌，并判断生成的令牌与请求中的令牌是否一致，如果一致，身份验证通过。</p><p>值得注意的是，对于每种资源所要求的权限是不同的，用户必须具备相应的权限。</p><p>此外，为了防止某些用户占用太多的存储空间，以及方便根据付费分配空间，RGW允许对用户进行配额限制。</p><p>RGW使用<code>RGWUserInfo</code>管理元数据。</p><div class="table-container"><table><thead><tr><th>字段</th><th>意义</th></tr></thead><tbody><tr><td>users.uid</td><td>在“ <user>”对象中包含每个用户信息（RGWUserInfo），并在“ <user> .buckets”对象的omaps中包含存储桶的每个用户列表。如果非空，则“ &lt;用户&gt;”可以包含租户，例如：<code>prodtx$prodt test2.buckets prodtx$prodt.buckets test2</code></td></tr><tr><td>users.email</td><td></td></tr><tr><td>access_keys</td><td>用户认证。包括用户访问密匙Id，和私有密匙key</td></tr><tr><td>swift_keys</td><td>Swift用户认证。包括子用户ID：Subuser，以及子用户私有密匙Key。</td></tr><tr><td>subusers</td><td>子用户。包括Name和perm_mask（子用户访问权限）。</td></tr><tr><td>op_mask</td><td>用户访问权限。包括read、write、delete。</td></tr><tr><td>Caps</td><td>授权用户权限。由<caps-type,perm>组成</td></tr></tbody></table></div><p>RGW将用户信息保存在RADOS对象的数据部分，一个用户对应一个RADOS对象。由于大部分情况下，使用“pool名+对象名”来查询一个对象。</p><p>由于认证过程中需要使用用户访问密匙、子用户作为索引读取用户信息，并且在设置存储桶和对象的访问权限时，允许在存储桶和对象的访问权限授予email为xxx的用户，在操作进行鉴权检查时需要使用email作为索引获取用户信息。RGW采用了二级索引方式，即分别创建以用户访问密匙、子用户、email命名的三个RADOS对象，并将用户ID保存在对象的数据部分。当需要使用某个索引查询用户信息时，首先从索引对象读出用户ID，然后使用用户ID作为索引读取用户信息。</p><h2 id="1-3-存储桶"><a href="#1-3-存储桶" class="headerlink" title="1.3 存储桶"></a>1.3 存储桶</h2><p>一个存储桶对应一个RADOS对象。包含两类信息，一种是用户自定义的元数据信息，通常以键值对形式存储 。</p><p>另一类信息是对象存储策略、存储桶中索引对象的数目以及应用对象与索引对象的映射关系、存储桶的配额等，由RGWBucketInfo管理。</p><p>在创建存储桶的同时，RGW网关会同步创建一个或多个索引（index）对象，用于保存该存储桶下的对象列表，以支持查询存储桶对象列表（List Bucket）功能，因此在存储桶中有新的对象上传或者删除必须更新索引对象。</p><p>​    为了避免索引对象的更新成为对象上传删除的瓶颈，RGW采用了Ceph惯用的伎俩——shard，即分片，它的确会带来性能上的提升，但这也会影响查询存储桶对象列表操作的性能。</p><h3 id="存储桶的创建"><a href="#存储桶的创建" class="headerlink" title="存储桶的创建"></a>存储桶的创建</h3><p>流程如下：</p><ol><li>从HTTP请求解析出相关参数；</li><li>判断存储桶是否存在；若存在则依次判断已存在的bucket的拥有者是否为前用户，以及已存在bucket与带创建的bucket的存储策略是否相同，若为否，则返回bucket已存在；否则转到3；                                                                                                                                                                                                                                                                  </li><li>创建bucket实体；</li><li>更新user_id.buckets对象；</li><li>返回创建成功。</li></ol><blockquote><p>注意：同一租户下不同用户不能创建同名的存储桶。</p><p>我们知道OMAP由一个头部和多个KV条目组成，针对user_id.buckets对象，OMAP头部保存用户使用空间统计信息<code>cls_user_header</code>; OMAP的KV条目保存一个存储桶使用的空间统计信息<code>cls_user_bucket_entry</code>。</p></blockquote><h2 id="1-4-对象"><a href="#1-4-对象" class="headerlink" title="1.4 对象"></a>1.4 对象</h2><p>RGW对单个对象提供了两种上传接口：<strong>整体上传</strong>与<strong>分段上传</strong>。RGW限制了整体上传一个对象不能大于5GB（与Amazon S3）相同。用户上传的对象不能大于该限制，否则会上传失败。</p><p>我们介绍两个宏值：</p><ul><li><code>rgw_max_chunk_size</code>：该宏值用来表示RGW下发到RADOS集群单个I/O的大小，同时决定对象分成多个RADOS对象时首对象的大小，以下简称分块大小。</li><li><code>rgw_obj_stripe_size</code>：该宏值用来指定当一个对象被分为多个RADOS对象时中间对象的大小，以下简称条带大小。</li><li><code>Class RGWObjManifest</code>：用来管理用户上传的对象和RADOS对象的对应关系，以下简称manifest。</li></ul><p>用户上传一个大小小于分块的对象，那么很容易理解，该RADOS对象以应用对象名称命名，应用对象元数据也保存在该RADOS对象的扩展属性中；若用户上传的对象大于分块，那么将被分解为一个大小等于分块大小的首对象，多个大小等于条带大小的中间对象，和一个小于条带大小的尾对象。</p><p>当所有分段上传完成之后，RGW会生成一个RADOS对象，用于保存应用对象元数据和所有分段的manifest。</p><p>值得注意的是，用户上传的元数据的大小最好能被条带大小整除，否则会造成RADOS对象比每个分段条带数多且对象大小分布不均匀。从而数据管理复杂度增加。</p><h2 id="1-5-数据存储位置"><a href="#1-5-数据存储位置" class="headerlink" title="1.5 数据存储位置"></a>1.5 数据存储位置</h2><p>不同的用户数据最终以RADOS对象为单位存储到RADOS集群，RGW使用zone来管理用户数据的存储位置，zone由一组存储池（pool）组成，不同的存储池用来保存不同的数据，RGW使用RGWZoneParams来管理不同的存储池。</p><h2 id="1-6-I-O流程"><a href="#1-6-I-O流程" class="headerlink" title="1.6 I/O流程"></a>1.6 I/O流程</h2><p>如图所示，OP线程从HTTP前端接收到I/O请求后，首先在REST API通用处理层，从HTTP语义中解析出S3或Swift数据并进行一系列检查，之后再根据不同API请求执行不同处理流程，如需从RADOS集群获取数据或者往RADOS集群写入数据，则通过RGW与RADOS接口适配层调用librados接口来进行交互。</p><p><img src="/img/image-20210110211049069.png" alt="image-20210110211049069"></p><h3 id="用户认证"><a href="#用户认证" class="headerlink" title="用户认证"></a>用户认证</h3><p>对于S3 API，RGW支持认证用户和匿名用户访问。RGW V2支持本地认证、LDAP和keystone三种认证方式。</p><p>对于Swift，支持临URL认证、RGW本地认证、Keystone认证、匿名认证和匿名认证等五种认证引擎。</p><h3 id="用户、存储桶、对象访问ACL"><a href="#用户、存储桶、对象访问ACL" class="headerlink" title="用户、存储桶、对象访问ACL"></a>用户、存储桶、对象访问ACL</h3><p>对于S3 API：权限如下：</p><ul><li>READ：查询对象</li><li><p>WRITE：上传、删除对象</p></li><li><p>READ_ACP：允许读取存储桶访问控制表</p></li><li><p>WRITE_ACP：允许修改存储桶访问控制表</p></li><li><p>FULL_CONTROL: 完全控制权限</p></li></ul><p>对于Swift，则分为用户访问控制和存储桶访问控制：</p><ul><li>read-only: 读取本用户下所有内容，比如存储桶列表，对象列表等；</li><li>read-write：授权指定用户读或写任意一个存储桶；</li><li>admin：可以创建删除更新用户头部信息，并且授予其它用户权限。</li><li></li></ul><h3 id="配额"><a href="#配额" class="headerlink" title="配额"></a>配额</h3><p>用户和存储桶的配额分别用user_quota和bucket_quota表示。RGW实例的配额通过如下方式：</p><ul><li>全局配置，适用于所有用户；</li><li>创建用户或更新用户配置时设置；</li><li>对于Swift接口，可在更新用户数据时配置或修改user_quota，在更新存储桶元数据时，配置或修改bucket_quota。</li></ul><p>关于配额，开发者们也进行了富有工程价值的思考，为了避免I/O重复读写，RGW网关设计了一个LRU缓存，将用户和存储桶配额以一个map的形式保存在缓存中，优先从缓存中进行读写，然后再定时将缓存数据刷到对象中。</p><p>这样的话对于数据在缓存保存时间，刷新间隔都有要求。设计者提出了用三个参数进行控制：</p><ul><li><code>rgw_bucket_quota_ttl</code>: 存储桶配额缓存可信任时间段。在检查配额是否达到限制时，如果缓存中记录的使用量达到配额上限（默认95%）或者距离上次从RADOS集群中更新配额到缓存中的事件超过该时间时，则需要更新缓存，并重置该参数。</li><li><code>rgw_user_quota_bucket_sync_interval</code>：控制存储桶已用空间从缓存刷到RADOS集群的间隔时间。</li><li><code>rgw_user_quota_sync_interval</code>：控制用户已用空间从缓存刷到RADOS集群的间隔时间。</li></ul><h2 id="1-7-对象上传"><a href="#1-7-对象上传" class="headerlink" title="1.7 对象上传"></a>1.7 对象上传</h2><p>RGW针对对象上传设计了两个接口：<strong>整体上传接口</strong>和<strong>分段上传接口</strong>。</p><p>整体上传有三个步骤：</p><ol><li>prepare：初始化manifest数据结构；</li><li>handle_data：RGW每次从HTTP server中取出<code>rgw_max_chunk_size</code>字节数据，存放在一个bufferlist中，然后分成一个或多个I/O异步下发送到RADOS层，每个I/O的大小等于<code>min(rgw_max_chunk_size,next_part_ofs - data_ofs)</code> ，<code>next_part_ofs</code>表示下一个RADOS对象保存的用户数据偏移位置，<code>data_ofs</code>表示当前数据的偏移位置。</li></ol><p>以一个块大小为2MB，条带大小为5MB为例，用户上传一个9MB对象，那么应用对象先按照2MB进行分割，最后会余下1MB。其中0-2MB为对象1， 2MB-4MB和4MB-6MB，6MB-7MB共同拼接为对象2，最后的7MB-8MB以及8MB-9MB组成对象3.</p><ol><li>complete：该阶段主要是将对象元数据更新到head_obj，同时将对象条目更新到索引对象中。</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;div class=&quot;note note-primary&quot;&gt;
            &lt;p&gt;本blog包括理论和实践两个部分，实践部分需要您事先部署成功Ceph集群！&lt;/p&gt;&lt;p&gt;参考《Ceph设计与实现》谢型果等，第六章。以及&lt;a href=&quot;https://docs.cep</summary>
      
    
    
    
    <category term="ceph" scheme="http://durantthorvalds.top/categories/ceph/"/>
    
    <category term="分布式存储" scheme="http://durantthorvalds.top/categories/ceph/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/"/>
    
    
    <category term="系统架构" scheme="http://durantthorvalds.top/tags/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84/"/>
    
    <category term="ceph" scheme="http://durantthorvalds.top/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title>「核心」Ceph学习三部曲之六：分布式块存储RBD</title>
    <link href="http://durantthorvalds.top/2020/12/29/%E3%80%8C%E6%A0%B8%E5%BF%83%E3%80%8DCeph%E5%AD%A6%E4%B9%A0%E4%B8%89%E9%83%A8%E6%9B%B2%E4%B9%8B%E5%85%AD%EF%BC%9A%E5%88%86%E5%B8%83%E5%BC%8F%E5%9D%97%E5%AD%98%E5%82%A8RBD/"/>
    <id>http://durantthorvalds.top/2020/12/29/%E3%80%8C%E6%A0%B8%E5%BF%83%E3%80%8DCeph%E5%AD%A6%E4%B9%A0%E4%B8%89%E9%83%A8%E6%9B%B2%E4%B9%8B%E5%85%AD%EF%BC%9A%E5%88%86%E5%B8%83%E5%BC%8F%E5%9D%97%E5%AD%98%E5%82%A8RBD/</id>
    <published>2020-12-28T16:00:00.000Z</published>
    <updated>2021-01-03T05:16:36.728Z</updated>
    
    <content type="html"><![CDATA[<div class="note note-primary">            <p>本blog包括理论和实践两个部分，实践部分需要您事先部署成功Ceph集群！</p><p>参考《Ceph设计与实现》谢型果等，第六章。以及<a href="https://docs.ceph.com/en/latest/man/8/rados/">官方RADOS指南</a>，以及<a href="https://docs.ceph.com/en/latest/man/8/rbd/">官方RBD教程</a></p>          </div><h1 id="无心插柳——分布式块存储RBD"><a href="#无心插柳——分布式块存储RBD" class="headerlink" title="无心插柳——分布式块存储RBD"></a>无心插柳——分布式块存储RBD</h1><blockquote><p>RADOS(Reliable, Autonomic Distributed Object Store) </p><p><strong>rbd</strong>是用于处理rados块设备（RBD）映像的实用程序，由Linux rbd驱动程序和QEMU / KVM的rbd存储驱动程序使用。RBD映像是简单的块设备，在对象上划分条带并存储在RADOS对象存储中。分割image的对象的大小必须是2的幂。——来自官方</p></blockquote><p>RBD($RADOS Block  Device$)指分布式块存储服务组件，是Ceph对外的三大存储服务组件之一。另外两个分别是CephFS以及Radosgw                                                                                          。我们将在后面介绍。上层应用访问RBD有两种途径：librbd以及krbd。其中librbd是基于librados的用户态接口库，而krbd是继承在GNU/Linux内核中的一个内核模块。通过librbd命令行工具，将RBD块设备映射为本地的一个块设备文件。</p><p><img src="/img/rados.png" alt=""></p><p>一个块通常是512字节，块设备接口无处不在，非常适合与包括Ceph在内的海量数据存储进行交互。Ceph块设备是精简配置的，可调整大小的，并且可以在多个OSD上条带化存储数据。Ceph块设备利用了 RADOS功能，包括快照，复制和强大的一致性。Ceph块存储客户端通过内核模块或<code>librbd</code>库与Ceph集群通信。</p><p>Ceph的的块设备提供与广阔的可扩展性，高性能的 <a href="https://docs.ceph.com/en/latest/rbd/rbd-ko/">内核模块</a>，或者KVM系列如<a href="https://docs.ceph.com/en/latest/rbd/qemu-rbd/">QEMU</a>和基于云计算系统，如<a href="https://docs.ceph.com/en/latest/rbd/rbd-openstack">OpenStack的</a>和<a href="https://docs.ceph.com/en/latest/rbd/rbd-cloudstack">的CloudStack</a>依赖的libvirt和QEMU与Ceph的块设备集成。可以同一群集同时运行<a href="https://docs.ceph.com/en/latest/radosgw/#object-gateway">Ceph RADOS网关</a>， <a href="https://docs.ceph.com/en/latest/cephfs/#ceph-file-system">Ceph文件系统</a>和Ceph块设备。</p><p><img src="/img/rbd1.png" alt="image-20201220224241218" style="zoom:67%;" /></p><p>RBD架构如上图所示，由于元数据信息非常少，且访问不频繁，因此RBD在Ceph集群中不需要daemon守护进程直接将元数据加载到内存进行元数据访问加速，所有数据操作直接与MON和OSD进行交互。</p><h2 id="S1-元数据"><a href="#S1-元数据" class="headerlink" title="$\S1$ 元数据"></a>$\S1$ 元数据</h2><p>RBD块设备在Ceph中被称为image，由元数据和数据组成。其中元数据有三种存储方式，第一种将元数据编码后以二进制文件的形式存储在RADOS对象的数据部分，后面将该类型标识为data。第二种将元数据以键值对的形式存储在RADOS对象的扩展属性中，称为xattr；第三种将元数据以键值对形式存储在RADOS对象omap中，称为omap。更多关于元数据的讨论见<a href="https://durantthorvalds.top/2020/12/27/%E4%B8%8B%E4%B8%80%E4%BB%A3%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8EBlueStore/#6-ObjectStore-OS">BlueStore</a></p><h3 id="1-1-image元数据对象"><a href="#1-1-image元数据对象" class="headerlink" title="1.1 image元数据对象"></a>1.1 image元数据对象</h3><ul><li><code>rbd_id.&lt;name&gt;</code>，data类型，记录image名称到image id 的单向映射关系。</li><li><code>rbd_header.&lt;id&gt;</code>，omap\xattr类型，记录image所支持的功能特性、容量大小等基本信息以及配置参数、自定义元数据，锁信息等。</li><li><code>rbd_object_map.&lt;id&gt;</code>，data类型，记录组成image的所有数据对象的存在状态。</li></ul><p><img src="/img/rbd3.png" alt="image-20201221173651999" style="zoom:67%;" /></p><p>通常情况下，image的数据和元数据存储在同一个存储池下。但是当前纠删码池不支持omap，必须将数据对象和元数据分开存储，需要一个独立的元数据<code>data_pool_id</code>用于记录数据对象所在的存储池。</p><p>$\S 1$ rbd_id</p><p>image内部的元数据和数据的名称以id为基础，这样即使image重命名，内部结构也基本不发生改变。</p><p>$\S 2$ rbd_header</p><p>这是image最主要的元数据对象，其对象名由rbd_header.<id>,<id>表示rbd_id所记录的内部id。</p><p>$\S3$ rbd_object_map</p><p>为了解决克隆image数据I/O对象执行时间过长的问题，Ceph引入object-map，它将所有数据对象的存在记录在一个独立的元数据对象，总共有四种状态，b00对象不存在、b01对象存在、b10对象待删除、b11对象存在且从第一次快照创建后没有进行写。</p><h3 id="1-2-RBD管理元数据对象"><a href="#1-2-RBD管理元数据对象" class="headerlink" title="1.2 RBD管理元数据对象"></a>1.2 RBD管理元数据对象</h3><ul><li><p>rbd_directory</p><p>omap类型。记录存储池中所有image列表。其中 <code>name_&lt;name&gt;</code> 记录image名称所对应的image id；<code>id_&lt;id&gt;</code>记录image名称。</p></li><li><p>rbd_children</p><p>omap类型。记录父image快照到克隆image之间的单向映射关系（parent-&gt;children）。其中<code>&lt;parent&gt;</code>记录当前存储池下基于父image快照创建的一个或多个克隆image id列表。由<pool_id,image_id,snap_id>三个字段组成，用于表示克隆image所关联的父image快照，而元数据内容为克隆image的id集合。</p></li></ul><h2 id="S2-数据"><a href="#S2-数据" class="headerlink" title="$\S2$  数据"></a>$\S2$  数据</h2><p>如rbd_header的定义，在创建image时可以通过<code>object-size</code>控制数据对象的容量大小，默认为4MB，image数据以该大小为单元进行等量划分。每个数据对象的名称由rbd_header元数据中对象前缀和对象序号组成。</p><p><strong>数据条带化</strong></p><p>RBD image在许多对象上分条，然后由Ceph分布式对象存储（RADOS）存储。结果，对图像的读取和写入请求分布在群集中的许多节点上，通常可以防止在单个image变大或繁忙时任何单个节点成为瓶颈。默认为类似于RAID-0的方式进行条带化（Stripping V2）.</p><p>条带化由三个参数控制：</p><ul><li><p><code>object-size</code></p><p>我们分割的对象的大小是2的幂。将四舍五入到最接近的2的幂。默认对象大小为4 MB，最小为4K，最大为32M。</p></li><li><p><code>stripe_unit</code></p><p>在继续下一个对象之前，每个[ <em>stripe_unit</em> ]连续字节都存储在同一对象中。</p></li><li><p><code>stripe_count</code></p><p>之后，我们写[ <em>stripe_unit</em> ]字节为[ <em>stripe_count</em> ]对象，我们循环回到初始对象和写入另一个条纹，直到对象达到其最大尺寸。在这一点上，我们继续下一个[ <em>stripe_count</em> ]对象。</p></li></ul><p>默认情况下，[ <em>stripe_unit</em> ]与对象大小相同，[ <em>stripe_count</em> ]为1。指定不同的[ <em>stripe_unit</em> ]和/或[ <em>stripe_count</em> ]通常被称为使用“花式”条带，并且需要v2。</p><h2 id="S3-功能特性"><a href="#S3-功能特性" class="headerlink" title="$\S3 $ 功能特性"></a>$\S3 $ 功能特性</h2><h3 id="RADOS快照"><a href="#RADOS快照" class="headerlink" title="RADOS快照"></a>RADOS快照</h3><p>由于快照的存在，一个RADOS对象可能由一个head对象和多个克隆组成。在OSD端使用SnapSet结构体来保存对象的快照信息，其中clone_overlap字段记录clone对象与head对象的数据内容重叠的区间，在数据恢复时，可以减少OSD直接的信息传输。</p><p>RADOS对象创建快照后数据读取流程非常简单，RADOS客户端在读操作中携带需要读取的RADOS对象的snapid，通过snapid定位到clone对象或head对象即可读到相应的数据。</p><p>假设初始时的head对象是一个完整的使用默认4MB大小的对象，且之前未有过COW操作。对该对象制作快照snap1，然后写数据至[512K~1M]区间，此时会触发COW，通过底层的克隆操作生成一个clone对象clone1，然后将新数据写入head对象。写操作会同步更新这个head对象上记录的clone_overlap[snap1]，对于一个新的快照对象一开始这个重叠区间是整个对象的[0 ~ 4M]， 然后每个新的写入操作会在这个区间减去新写的区间。 </p><h3 id="RBD快照"><a href="#RBD快照" class="headerlink" title="RBD快照"></a>RBD快照</h3><p>RBD快照只需要保存少量的快照元数据信息，其底层数据I/O的实现完全依赖于RADOS快照实现，</p><h3 id="克隆"><a href="#克隆" class="headerlink" title="克隆"></a>克隆</h3><p>RBD克隆是在RBD快照基础上实现的可写快照，与RBD快照功能相似，RBD克隆的实现也依赖COW。与RBD快照不同的是 快照功能依赖于RADOS层的对象快照实现，但是功能完全在RBD客户端实现。</p><p>创建克隆image的过程基本上就是创建一个新的image，但是在image的元数据中会记录一个parent键值对，也就是记录克隆image与快照相连的父子关系。所以才会有parent和rbd_children等属性。由于克隆关系可能存在多层，因此RBD客户端会尝试访问最顶层的parent。</p><p><strong>如果访问克隆对象遇到不一致如何处理</strong>？</p><p>克隆image读流程</p><ul><li>RBD客户端读取指定区间的数据，假设该区间最终落到第一个数据对象；</li><li>由于第一个对象不存在，故会返回对象不存在错误；</li><li>RBD会访问parent并得到关联的快照信息，需要注意的是，创建克隆image时可以指定与快照image不同的条带化参数，因此从快照image读取数据的区间可能落到与第一步不同的数据对象；</li><li>读操作返回</li></ul><p>克隆image写流程</p><ul><li>RBD客户端读取指定区间的数据，假设该区间最终落到第一个数据对象；</li><li>由于第一个对象不存在，故会返回对象不存在错误；</li><li>与读操作不同，写入操作需要从快照image读取克隆image第一个数据对象整个对象区间的数据；</li><li>对快照image的读操作返回；</li><li>将从快照读取的数据写入克隆image的第一个数据对象，然后重新执行原始的针对克隆image的第一个数据对象的写操作，最终写操作完成。</li></ul><hr><h1 id="实践部分"><a href="#实践部分" class="headerlink" title="实践部分"></a>实践部分</h1><h2 id="创建和删除"><a href="#创建和删除" class="headerlink" title="创建和删除"></a>创建和删除</h2><ol><li><p>我们可以创建一个rbd_image并且指定它的大小：</p><pre><code class="hljs livecodeserver">rbd <span class="hljs-built_in">create</span> mypool/myimage <span class="hljs-comment">--size 102400</span></code></pre><p>或者指定对象大小（8M）</p><pre><code class="hljs angelscript">rbd create mypool/myimage --size <span class="hljs-number">102400</span> --object-size <span class="hljs-number">8</span>M</code></pre></li><li><p>删除（小心！）</p></li></ol><pre><code class="hljs nginx"><span class="hljs-attribute">rbd</span> rm mypool/myimage</code></pre><ol><li>创建快照</li></ol><pre><code class="hljs nginx"><span class="hljs-attribute">rbd</span> snap create mypool/myimage<span class="hljs-variable">@mysnap</span></code></pre><h2 id="获取rbd-id"><a href="#获取rbd-id" class="headerlink" title="获取rbd_id"></a>获取rbd_id</h2><pre><code class="hljs vim">rados <span class="hljs-built_in">get</span> -<span class="hljs-keyword">p</span> rbd rbd_id.<span class="hljs-symbol">&lt;image_name&gt;</span> file_rbd_id<span class="hljs-keyword">cat</span> file_rbd_id</code></pre><p>可以获得类似<code>ac62a15cbf99</code>这样的id标识。</p><ul><li>features 已启用的功能特性</li></ul><div class="table-container"><table><thead><tr><th>特性</th><th>bit位</th><th>注解</th></tr></thead><tbody><tr><td>layering</td><td>0(LSB)</td><td>是否支持image克隆操作，克隆image与关联的父image快照之间通过COW实现数据共享</td></tr><tr><td>striping</td><td>1</td><td>是否进行数据对象间数据条带化，类似于RAID 0，在创建image时如果指定了条带化参数，数据会在多个image数据对象之间进行条带化</td></tr><tr><td>exclusive-lock</td><td>2</td><td>是否支持分布式锁，即image自带互斥访问锁机制以限制以限制同时只能有一个客户端访问image，主要应用于虚拟机热迁移</td></tr><tr><td>object-map</td><td>3</td><td>是否记录组成image的数据对象存在的状态位图，通过查表加速类似于导入、导出、克隆分离、已使用容量计算等操作，同时有助于减少COW机制带来的克隆image的I/O时延，依赖于exclusive-lock特性</td></tr><tr><td>fast-diff</td><td>4</td><td>用于计算快照间增量数据等操作加速，依赖于object-map特性</td></tr><tr><td>deep-flatten</td><td>5</td><td>克隆分离时十分同时解除克隆image创建的快照与父image之间的关联关系。该特性只是为了阻止老的RBD客户端访问image而设置</td></tr><tr><td>journaing</td><td>6</td><td>是否记录image修改操作到日志对象，用于远程异步镜像功能，依赖于exclusive-lock特性</td></tr><tr><td>data-pool</td><td>7(MSB)</td><td>是否将数据对象存储于与元数据不同的存储池，用于支持将image的数据对象存储于EC纠删码存储池</td></tr></tbody></table></div><hr>  <pre><code class="hljs angelscript">value (<span class="hljs-number">8</span> bytes) :<span class="hljs-number">00000000</span>  <span class="hljs-number">3</span>d <span class="hljs-number">00</span> <span class="hljs-number">00</span> <span class="hljs-number">00</span> <span class="hljs-number">00</span> <span class="hljs-number">00</span> <span class="hljs-number">00</span> <span class="hljs-number">00</span>                           |=.......|<span class="hljs-number">00000008</span></code></pre><p>  0x3d对应二进制<code>0&#39;b00111101</code> 表示启用了layering、exclusive-lock、object-map，fast-diff，deep-flatten等特性。</p><ul><li><p>object_prefix 数据对象名称前缀</p><pre><code class="hljs angelscript">value (<span class="hljs-number">25</span> bytes) :<span class="hljs-number">00000000</span>  <span class="hljs-number">15</span> <span class="hljs-number">00</span> <span class="hljs-number">00</span> <span class="hljs-number">00</span> <span class="hljs-number">72</span> <span class="hljs-number">62</span> <span class="hljs-number">64</span> <span class="hljs-number">5f</span>  <span class="hljs-number">64</span> <span class="hljs-number">61</span> <span class="hljs-number">74</span> <span class="hljs-number">61</span> <span class="hljs-number">2</span>e <span class="hljs-number">61</span> <span class="hljs-number">63</span> <span class="hljs-number">36</span>  |....rbd_data.ac6|<span class="hljs-number">00000010</span>  <span class="hljs-number">32</span> <span class="hljs-number">61</span> <span class="hljs-number">31</span> <span class="hljs-number">35</span> <span class="hljs-number">63</span> <span class="hljs-number">62</span> <span class="hljs-number">66</span> <span class="hljs-number">39</span>  <span class="hljs-number">39</span>                       |<span class="hljs-number">2</span>a15cbf99|<span class="hljs-number">00000019</span></code></pre></li><li><p>order 组成image的数据对象容量大小，以2为底的指数</p><pre><code class="hljs angelscript">value (<span class="hljs-number">1</span> bytes) :<span class="hljs-number">00000000</span>  <span class="hljs-number">16</span>                                                |.|<span class="hljs-number">00000001</span></code></pre></li><li><p>parent 当存在克隆关系时，克隆image记录的关联的父image快照信息</p></li><li><p>size 容量大小</p><pre><code class="hljs angelscript">value (<span class="hljs-number">8</span> bytes) :<span class="hljs-number">00000000</span>  <span class="hljs-number">00</span> <span class="hljs-number">00</span> <span class="hljs-number">00</span> <span class="hljs-number">40</span> <span class="hljs-number">00</span> <span class="hljs-number">00</span> <span class="hljs-number">00</span> <span class="hljs-number">00</span>                           |...@....|<span class="hljs-number">00000008</span></code></pre><p>64位整型，上述表示1GB</p></li><li><p>snap_seq 用于记录image最后一次创建的快照的id</p><pre><code class="hljs angelscript">value (<span class="hljs-number">8</span> bytes) :<span class="hljs-number">00000000</span>  <span class="hljs-number">04</span> <span class="hljs-number">00</span> <span class="hljs-number">00</span> <span class="hljs-number">00</span> <span class="hljs-number">00</span> <span class="hljs-number">00</span> <span class="hljs-number">00</span> <span class="hljs-number">00</span>                           |........|<span class="hljs-number">00000008</span></code></pre><p>上图表示最后一次创建快照id为0x04</p></li><li><p>snap_<snap_id>所记录的是一个cls_rbd_snap结构体实例，记录快照名称、id等基本信息。</p><pre><code class="hljs routeros">rados getomapval -p cephfs_data rbd_header.ac62a15cbf99 snapshot_0000000000000004 f_snapceph-decoder<span class="hljs-built_in"> type </span>cls_rbd_snap import \&gt; f_snap decode dump_jsonceph-dencoder<span class="hljs-built_in"> type </span>cls_rbd_snap import f_snap decode dump_json</code></pre><pre><code class="hljs json">&#123;    <span class="hljs-attr">&quot;id&quot;</span>: <span class="hljs-number">4</span>,    <span class="hljs-attr">&quot;name&quot;</span>: <span class="hljs-string">&quot;snap1&quot;</span>,    <span class="hljs-attr">&quot;image_size&quot;</span>: <span class="hljs-number">1073741824</span>,    <span class="hljs-attr">&quot;protection_status&quot;</span>: <span class="hljs-string">&quot;unprotected&quot;</span>,    <span class="hljs-attr">&quot;child_count&quot;</span>: <span class="hljs-number">0</span>&#125;</code></pre><p>与普通的image不同，克隆image在创建时不能指定容量大小，而是由image_size决定克隆image的初始容量大小。features是父image在创建快照时的features元数据记录，创建克隆image时如果不显示指定需要启用的功能特性<code>--image-feature &lt;feature_name&gt;</code>，则默认会使用features所有记录的值。protection_status 用于标示快照被保护状态，处于被保护的快照不能被删除，克隆image必须基于被保护的快照进行创建，主要是为了防止克隆image所引用的父image被误删除。</p></li><li><p>stripe_count\stripe_unit所记录的元素据是64位整数，用于记录image条带化信息。详细见理论部分。</p></li></ul><h2 id="RBD管理元数据对象"><a href="#RBD管理元数据对象" class="headerlink" title="RBD管理元数据对象"></a>RBD管理元数据对象</h2><ul><li><h3 id="rbd-directory"><a href="#rbd-directory" class="headerlink" title="rbd_directory"></a>rbd_directory</h3></li></ul><p>用于记录当前存储池中image列表。</p><pre><code class="hljs apache"><span class="hljs-attribute">rados</span> listomapvals -P cephfs_data rbd_directory id_ac<span class="hljs-number">62</span>a<span class="hljs-number">15</span>cbf<span class="hljs-number">99</span></code></pre><pre><code class="hljs angelscript">value (<span class="hljs-number">12</span> bytes) :<span class="hljs-number">00000000</span>  <span class="hljs-number">08</span> <span class="hljs-number">00</span> <span class="hljs-number">00</span> <span class="hljs-number">00</span> <span class="hljs-number">72</span> <span class="hljs-number">62</span> <span class="hljs-number">64</span> <span class="hljs-number">69</span>  <span class="hljs-number">6</span>d <span class="hljs-number">61</span> <span class="hljs-number">67</span> <span class="hljs-number">65</span>              |....rbdimage|<span class="hljs-number">0000000</span>cid_aca0d915627fvalue (<span class="hljs-number">13</span> bytes) :<span class="hljs-number">00000000</span>  <span class="hljs-number">09</span> <span class="hljs-number">00</span> <span class="hljs-number">00</span> <span class="hljs-number">00</span> <span class="hljs-number">72</span> <span class="hljs-number">62</span> <span class="hljs-number">64</span> <span class="hljs-number">69</span>  <span class="hljs-number">6</span>d <span class="hljs-number">61</span> <span class="hljs-number">67</span> <span class="hljs-number">65</span> <span class="hljs-number">32</span>           |....rbdimage2|<span class="hljs-number">0000000</span>dname_rbdimagevalue (<span class="hljs-number">16</span> bytes) :<span class="hljs-number">00000000</span>  <span class="hljs-number">0</span>c <span class="hljs-number">00</span> <span class="hljs-number">00</span> <span class="hljs-number">00</span> <span class="hljs-number">61</span> <span class="hljs-number">63</span> <span class="hljs-number">36</span> <span class="hljs-number">32</span>  <span class="hljs-number">61</span> <span class="hljs-number">31</span> <span class="hljs-number">35</span> <span class="hljs-number">63</span> <span class="hljs-number">62</span> <span class="hljs-number">66</span> <span class="hljs-number">39</span> <span class="hljs-number">39</span>  |....ac62a15cbf99|<span class="hljs-number">00000010</span>name_rbdimage2value (<span class="hljs-number">16</span> bytes) :<span class="hljs-number">00000000</span>  <span class="hljs-number">0</span>c <span class="hljs-number">00</span> <span class="hljs-number">00</span> <span class="hljs-number">00</span> <span class="hljs-number">61</span> <span class="hljs-number">63</span> <span class="hljs-number">61</span> <span class="hljs-number">30</span>  <span class="hljs-number">64</span> <span class="hljs-number">39</span> <span class="hljs-number">31</span> <span class="hljs-number">35</span> <span class="hljs-number">36</span> <span class="hljs-number">32</span> <span class="hljs-number">37</span> <span class="hljs-number">66</span>  |....aca0d915627f|<span class="hljs-number">00000010</span></code></pre><h3 id="rbd-children"><a href="#rbd-children" class="headerlink" title="rbd_children"></a>rbd_children</h3><p>当image之间具有克隆关系时，rbd_children 元数据对象用于记录父image快照到克隆image之间的单向映射关系。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;div class=&quot;note note-primary&quot;&gt;
            &lt;p&gt;本blog包括理论和实践两个部分，实践部分需要您事先部署成功Ceph集群！&lt;/p&gt;&lt;p&gt;参考《Ceph设计与实现》谢型果等，第六章。以及&lt;a href=&quot;https://docs.cep</summary>
      
    
    
    
    <category term="ceph" scheme="http://durantthorvalds.top/categories/ceph/"/>
    
    <category term="分布式存储" scheme="http://durantthorvalds.top/categories/ceph/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/"/>
    
    
    <category term="系统架构" scheme="http://durantthorvalds.top/tags/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84/"/>
    
    <category term="ceph" scheme="http://durantthorvalds.top/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title>「核心」Ceph学习三部曲之五:控制先行——Ceph的QoS策略</title>
    <link href="http://durantthorvalds.top/2020/12/28/%E6%8E%A7%E5%88%B6%E5%85%88%E8%A1%8C-Ceph%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0QoS/"/>
    <id>http://durantthorvalds.top/2020/12/28/%E6%8E%A7%E5%88%B6%E5%85%88%E8%A1%8C-Ceph%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0QoS/</id>
    <published>2020-12-27T16:00:00.000Z</published>
    <updated>2021-01-03T05:04:54.066Z</updated>
    
    <content type="html"><![CDATA[<h1 id="控制先行——Ceph的QoS策略"><a href="#控制先行——Ceph的QoS策略" class="headerlink" title="控制先行——Ceph的QoS策略"></a>控制先行——Ceph的QoS策略</h1><div class="note note-primary">            <p>本blog包括理论和实践两个部分，实践部分需要您事先部署成功Ceph集群！</p><p>参考《Ceph设计与实现》谢型果等，第五章。9/7/2017 韩国SK团队进展 .PPT见<a href="https://www.slideshare.net/ssusercee823/implementing-distributed-mclock-in-ceph">链接</a>,Code见<a href="https://github.com/ceph/ceph/pull/16369">链接</a>.</p>          </div><blockquote></blockquote><p>Ceph作为开源社区的明星，因为其高可扩展性、高可靠性，受到各大厂商的热烈追捧，并成为OpenStack事实上的默认存储后端。但是人非圣贤，孰能无过?它也同样面临I/O资源分配的问题。为了保证客户能够体验更好的服务，Ceph引入了QoS($Quality of Service$).</p><p>$dmClock$是一种分布式系统的I/O调度算法，它起源于mClock（<a href="https://www.usenix.org/legacy/events/osdi10/tech/full_papers/Gulati.pdf">论文地址</a>）,目前被应用于Ceph中。</p><p>Ceph作为分布式存储系统集大成者，不能像传统QoS实现位置首选的中心节点，必须在每个OSD中实现QoS。下图展示了社区目前（2017版本）的Ceph QoS pool单元：</p><p><img src="/img/image-20201219164136113.png" alt="image-20201219164136113" style="zoom:80%;" /></p><h2 id="1-dmClock算法原理"><a href="#1-dmClock算法原理" class="headerlink" title="1 dmClock算法原理"></a>1 dmClock算法原理</h2><p>首先我们先了解mClock，这是一种基于时间标签的I/O调度算法，最先被VMware提出了的用于集中式管理的存储系统。它使用了reservation（预留，表示客户端获得的最低I/O资源）、weight（权重，表示客户端占用共享I/O资源的权重，共享I/O指满足预留之后剩余的I/O资源）以及limit（上限，表示用户所能使用的最大I/O）作为一套模板（QoS spec），作用于不同的客户端。下图是其典型应用模型：</p><p><img src="/img/dmClock.png" alt="image-20201216151614635"></p><p>mClock是典型的C/S架构，Client可以驻留在实际的客户端或者服务器端，主要负责下发QoS模板的参数值、收集请求的完成信息等；server为mClock的服务端，实现I/O调度的核心功能。</p><p>其算法流程如下：</p><ol><li>Server为每个客户端设置一套QoS模板参数，包括预留（r），权重（w）和上限（l）三个部分，并依次计算出I/O请求的时间标签，其中预留和上限为绝对时间，权重标签为相对时间。</li><li>服务器分为两个阶段来处理I/O请求：一是Constarint-based，只处理满足预留时间标签的请求；二是Weight-based阶段，处理满足上限的时间标签的权重标签请求。</li><li>服务器先工作也Constraint-based，再转入Weight-based阶段。</li></ol><p>如果用$q_i$表示QoS的模板参数$q_i\in\{r_i,w_i,l_i\}$，$Q^r_i$表示来自第i个客户的第r个请求的时间标签。有如下公式：</p><script type="math/tex; mode=display">Q_i^r=max\{Q^{r-1}_i+1/q_i,current\_time\}</script><p>以第一个请求到达时间作为初始基准标签，后续标签依据预设的模板参数值，对单位时间进行均匀切分计算而来。在Constraint-based阶段 ，各客户端请求被均匀的处理，而在weight-based阶段，则将出现竞争，由于权重标签是相对值，它和真实的时间之差通常会很大，从而出现饥饿现象。因此需要调整旧client的权重标签，以新client的权重时间标签为基准，添加一个补偿值。</p><p>dmClock是mClock 的分布式版本，两者的基本原理相同。每个请求的时间标签计算公式如下：</p><script type="math/tex; mode=display">R_i^r=max\{R_i^{r-1}+\rho_i/r_i,current\_time\}\\W_i^r=max\{W_i^{r-1}+\delta_i/w_i,current\_time\}\\L_i^r=max\{L_i^{r-1}+\delta_i/l_i,current\_time\}\\</script><p>dmClock和mClock的主要区别在于：</p><ul><li>分布式系统具有多个服务器，服务器回应每个I/O请求时，返回其在哪个阶段被处理完成。</li><li>客户端记录每个服务器完成的请求个数，在向服务器下发请求时，携带距上次下发请求以来，收到完成的请求个数的增量，并且是除目标服务器之外，其它服务器完成的请求数之和，分别用$\rho$和$\delta$表示两个阶段的增量处理个数。</li><li>服务器计算请求的时间标签，使用$\rho$和$\delta$作为调整因子，不再以$1/q$均匀递增。从而减小了每个服务器处理的请求的个数。</li></ul><p>通过对$\rho$和$\delta$的调整，使得集群整体对外提供预期的I/O处理效果。</p><h2 id="2-QoS的设计与实现"><a href="#2-QoS的设计与实现" class="headerlink" title="2 QoS的设计与实现"></a>2 QoS的设计与实现</h2><p>在OSD中，存在<code>op_shardedwq</code>队列处理各种来自上级的I/O，并且这是一个复合队列，通常包含若干子队列。I/O请求从队列出列后，通过ObjectStore接口与磁盘交互。</p><p>OSD支持多种不同的子队列，目前主要包括优先级队列（prio）和基于权重的优先级队列（wpq）两种，</p><p>I/O操作类型主要包括以下几种：</p><ol><li>ClientOp：来自客户端的读写I/O请求；</li><li>SubOp：OSD之间的I/O请求。主要包括客户端I/O产生的副本间数据读写请求，以及由数据同步、数据扫描、负载均衡等引起的I/O请求。</li><li>SnapTrim：快照数据删除。</li><li>Scrub：用于发现对象的静默数据错误。其中Scrub只扫描元数据，而Deep Scrub对对象整体进行扫描。</li><li>Recovery：数据恢复和迁移。集群扩容、OSD添加与移除、手动进行数据重平衡都有可能触发recovery过程。</li></ol><p><img src="/img/dmClock2.png" alt="image-20201218175910498" style="zoom:67%;" /></p><p>上图表示OSD内部结构，我们对原有的prior队列，wpq队列以及新增的dmClock队列加以分析。</p><h2 id="2-1-优先级队列prior"><a href="#2-1-优先级队列prior" class="headerlink" title="2.1 优先级队列prior"></a>2.1 优先级队列prior</h2><p>prior是一个基于令牌桶的优先队列，由三个级别组成：1.I/O类型的优先级prior； 2. 客户端级别的client队列；3.真实的list请求；每个元素包括请求r以及数据大小cost。可以把prior看成一个三维的队列。</p><p>每个prior队列，在其第一个请求入队时，被创建，并分配一个大小为<code>max_tokens</code>的令牌桶。</p><p>关于出队的规则，有以下几点：</p><ol><li>选择合理的prior：从小到大轮询所有prior，只要满足条件则被选中。即，该prior队列的令牌桶中剩余 的令牌数量足够多，可以容纳将被选中的请求（每个请求出队时，必须拿到与其大小cost相当的令牌的个数）。</li><li>选择合理的client：对同优先级下的client进行轮询，即第一个client出队一个请求后，将请求的出队权交给第二个client，该优先级再次被选中时，从第二个client出队请求。</li><li>选择合理的请求：从被选中的client的请求list表中出队一个请求（FIFO策略）。</li></ol><p>当出队一个请求时，从令牌桶中拿掉与请求大小cost相当的令牌个数，随后将拿到的令牌数分发、交还至所有prior队列，使得令牌总数维持不变。令牌分发的规则是，按照各自prior的占用比重，每个prior队列可回收的令牌总数token：</p><script type="math/tex; mode=display">token = \frac{prior}{total\_prior}\times cost</script><p>对于prio较大的队列将优先被考虑，I/O类型到达优先级可以用过配置参数修改，如下表所示：</p><div class="table-container"><table><thead><tr><th>优先级配置参数</th><th>默认值</th></tr></thead><tbody><tr><td><code>osd_client_op_priority</code></td><td>63</td></tr><tr><td><code>osd_snap_trim_priority</code></td><td>5</td></tr><tr><td><code>osd_scrub_prority</code></td><td>5</td></tr><tr><td><code>osd_recovery_op_priority</code></td><td>3</td></tr></tbody></table></div><p>优先队列同样存在一些局限性，如果集群中某个OSD分布了比其它OSD更多的PG或者Object对象时，该OSD由于需要处理更多的副本请求，导致客户端长时间得不到处理出现饥饿现象。</p><p>为此社区引入了基于权重的优先级队列wpq.</p><h2 id="2-2-权重优先级队列wpq"><a href="#2-2-权重优先级队列wpq" class="headerlink" title="2.2 权重优先级队列wpq"></a>2.2 权重优先级队列wpq</h2><p>基于权重的wpq不需要创建令牌桶，与prior仅在出队方式上有区别：</p><ul><li><p>采用权重概率的方式确定prior级别，每个队列的优先级prior作为其权重，该prior队列被选中的概率即为其权重占总权重的比例。通过随机数对total_prior取余的方式得到在$[0,total_prior-1]$的范围内完全随机分布。(rand()%total_prior)</p></li><li><p>被选中的prior队列并不一定能出队请求，还需要根据将要出队的请求大小来确定，即是否满足$rand()\%max_cost\le (max_cost - (request_size*9/10))$</p></li></ul><p>max_cost指该prior队列最大的请求的大小。较小请求对应右边值更大，因而出队概率更高。</p><ul><li>client级别和真实请求的选择和prior相同。</li></ul><h2 id="2-3-dmClock队列"><a href="#2-3-dmClock队列" class="headerlink" title="2.3 dmClock队列"></a>2.3 dmClock队列</h2><p>dmClock是一个两级映射的队列，第一级为客户端的client队列，第二级是真实的请求队列，每个请求包含三个时间标签$<R_i,W_i,L_i>$, 其中i表示所属的client编号，没有使用优先级prior。</p><p>dmClock采用<strong>完全二叉树</strong>这种数据结构来处理大量的请求。分别构建预留时间标签、权重时间标签、上限时间标签二叉树，树节点为每个client对应的请求队列，节点在二叉树的位置，则根据其队首元素的三个标签决定，总体原则是父节点时间标签小于子节点。</p><h3 id="入队"><a href="#入队" class="headerlink" title="入队"></a>入队</h3><p>对于已存在的client，将请求直接挂入请求队列的尾部；对于新增client，除了新创建一个对应的请求队列，还要将队列作为一个新节点加入标签二叉树。根据完全二叉树的特点，采用顺序的变长数组结构存储，新节点先加入二叉树的尾部，再调整至合适的位置，二叉树的调整规则如下：</p><ul><li>R预留标签二叉树：以节点的队首元素的预留标签为基础，值小的节点调整至树的上层，反之调整至下层，最终根节点的预留标签值最小；</li><li>W权重标签二叉树：该树的节点中有两种状态，一种满足出队条件，其上限小于或等于当前时间，ready标记被置为true；另一种不满足出队条件，ready被置为false。对节点位置调整时，根据请求队列队首元素的ready状态，满足出队条件的节点调至上层，不满足出队条件的调至下层。相同状态的节点再由权重标签的大小决定，标签值较小的往上调整，反之往下调整。</li><li>L上限标签二叉树：用于判决权重二叉树的节点中的请求是否满足出队条件，也使用ready进行标记区分。但与权重二叉树不同，ready为true的节点向下调整。ready相同的节点根据上限标签值的大小决定。</li></ul><h3 id="出队"><a href="#出队" class="headerlink" title="出队"></a>出队</h3><p>首先进入constraint-based阶段取预留标签二叉树的根节点的请求队列，判断其队首元素的预留标签是否小于当前时间，作为是否满足出队条件的依据。如果条件满足，则选取该节点对应的client，从其请求队列的队首出队一个元素；否则进入weight-based阶段，从上限标签二叉树的根节点开始，逐个判断队首元素的上限标签是否小于等于当前时间，并，设置满足条件的请求的ready为true，以决定其是否可以参加随后的<strong>权重竞争</strong>。所谓的权重竞争，指对所有满足上限条件的clients，依据其队首元素的权重标签值，调整自身在权重二叉树的位置的过程，最终位于根节点的client胜出。</p><h2 id="2-4-Client的设计"><a href="#2-4-Client的设计" class="headerlink" title="2.4  Client的设计"></a>2.4  Client的设计</h2><p>目前Client 的设计有三种初步方案：</p><p>1）使用mClock作为一种分配调度策略，控制客户端的I/O请求和Ceph内部产生的I/O调度。这将所有不同真实客户端作为同一个抽象的client考虑；</p><p>2）使用dmClock以存储池或者卷为粒度，为其设置QoS模板参数，客户端请求以消息的形式发送至OSD。这是将每个存储池或存储池中的卷作为一个client；</p><p>3）使用dmClock为每个真实客户端设置一套QoS模板，这是将每个真实client作为一个client；</p><p>对于每一个 QoS 对象来说，首先需要在 OSD 中实现以下前置条件:</p><ol><li>对于每个客户端来说，每个请求具有唯一的标识符，客户端和请求形成全局唯一</li><li>必须将每个请求对应的 QoS 控制信息持久化</li><li>OSD 能够通过标识符从来访的请求中找到 QoS 控制信息</li></ol><h2 id="2-5-总结与展望"><a href="#2-5-总结与展望" class="headerlink" title="2.5 总结与展望"></a>2.5 总结与展望</h2><p>目前对QoS的优化有以下几种方向：</p><ol><li><p>合理模板参数的设置</p><p>只有集群运行于超负荷时（入队速率大于出队速率），权重的效果才能体现出来。</p></li><li><p>I/O带宽的限制</p><p>QoS限速体系的设计，比如通过OIO throttling来进行限速【参考SK团队PPT】。</p></li><li><p>突发I/O的处理</p><p>dmClock的做法是为每个client预先设置一个可调整参数$\sigma$，当出现突发I/O状况，减小该client的权重标签至$t-\sigma_i/w_i$，从而使其在权重竞争中更有优势。这里有一个问题，就是服务端如何判断客户端产生突发I/O访问？通过记录客户端的状态是一种可行的方式。</p></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;控制先行——Ceph的QoS策略&quot;&gt;&lt;a href=&quot;#控制先行——Ceph的QoS策略&quot; class=&quot;headerlink&quot; title=&quot;控制先行——Ceph的QoS策略&quot;&gt;&lt;/a&gt;控制先行——Ceph的QoS策略&lt;/h1&gt;&lt;div class=&quot;note </summary>
      
    
    
    
    <category term="Ceph" scheme="http://durantthorvalds.top/categories/Ceph/"/>
    
    <category term="理论" scheme="http://durantthorvalds.top/categories/Ceph/%E7%90%86%E8%AE%BA/"/>
    
    
    <category term="QoS" scheme="http://durantthorvalds.top/tags/QoS/"/>
    
  </entry>
  
  <entry>
    <title>「核心」Ceph学习三部曲之四:下一代对象存储引擎BlueStore</title>
    <link href="http://durantthorvalds.top/2020/12/27/%E4%B8%8B%E4%B8%80%E4%BB%A3%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8EBlueStore/"/>
    <id>http://durantthorvalds.top/2020/12/27/%E4%B8%8B%E4%B8%80%E4%BB%A3%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8EBlueStore/</id>
    <published>2020-12-26T16:00:00.000Z</published>
    <updated>2020-12-20T14:51:35.249Z</updated>
    
    <content type="html"><![CDATA[<div class="note note-primary">            <p>本blog包括理论和实践两个部分，实践部分需要您事先部署成功Ceph集群！</p><p>参考《Ceph设计与实现》谢型果等，第二章。以及<a href="https://docs.ceph.com/en/latest/rados/operations/bluestore-migration/">官方BlueStore教程</a>。</p><p>推荐博客<a href="http://www.itworld123.com/2019/06/04/storage/ceph/Ceph%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8EBlueStore%E7%AE%80%E6%9E%90/">Ceph存储引擎BlueStore简析</a></p>          </div><h1 id="下一代对象存储引擎BlueStore"><a href="#下一代对象存储引擎BlueStore" class="headerlink" title="下一代对象存储引擎BlueStore"></a>下一代对象存储引擎BlueStore</h1><p>相比于目前FileStore，BlueStore拥有无与伦比的优势：</p><ul><li>充分考虑下一代全SSD以及NVMe SSD闪存阵列的适配。例如将高效索引元数据的引擎由LevelDB替换为RocksDB。</li><li>传统的基于POSIX接口的FileStore需要通过操作系统自带的文件系统间接管理磁盘。BlueStore选择绕开文件系统，从而使得I/O路径大大减小。</li><li>在设计中将元素据和用户数据严格分离，因此元素据可以单独采用高速固态存储设备，诸如NVMe SSD，以实现性能加速。</li><li>与传统机械硬盘相比，SSD普遍采用4k 或者更大的块大小，因此采用位图进行管理可以获得更高的空间收益。</li></ul><h2 id="1-设计理念"><a href="#1-设计理念" class="headerlink" title="1 设计理念"></a>1 设计理念</h2><p>在存储系统中，所有读操作都是同步的，即除非在缓存命中，否则必须从磁盘中读到指定内容才向客户端返回。而写操作则不一样，一般处于效率考虑，所有写操作都会在内存中进行缓存，由文件系统进行组织后再批量写入磁盘。</p><p>数据可靠性：我们考虑写的期间发生断电的情况，因为内存是易失性的，所有数据会丢失。针对这个问题，有人提出用一个掉电不丢失的中间设备作为过渡设备，等数据写入普通磁盘后再释放中间设备上的空间，这个写中间设备的过程被称为<strong>写日志</strong>。中间设备被称为日志设备。但这样会消耗额外硬件资源。</p><p> 数据一致性：数据修改要么全部完成，要么没有变化（All or nothing）. 具体而言，我们用ACID（A: Atomicity, C: Consistency, I:Isolation, D:Durability）来描述这种系统，即<strong>事务型系统</strong>。</p><p><strong>术语</strong></p><p>块大小： 指对磁盘进行操作的最小粒度。 对普通机械硬盘为512字节，而SSD为4KB。</p><p>RMW：覆盖写。 如果本次改写的内容不足一个块，那么需要将对应的块读进来，将待修改的内容与原先内容进行合并。它的问题在于：额外的读惩罚，以及潜在的数据丢失风险。</p><p>COW：写时重定向。在磁盘分配新的空间，再写入，写完成后再释放旧数据。</p><h2 id="2-BlueStore写策略"><a href="#2-BlueStore写策略" class="headerlink" title="2 BlueStore写策略"></a>2 BlueStore写策略</h2><p>BlueStore综合运用了RMW和COW，任何一个写请求，根据磁盘块大小，分为三个部分，即首尾非块大小对齐部分和中间块大小对齐部分，针对两边RMW，针对中间采用COW。</p><p>BlueStore提供的读写访问接口都是基于PG粒度的。</p><h2 id="3-缓存替换机制"><a href="#3-缓存替换机制" class="headerlink" title="3 缓存替换机制"></a>3 缓存替换机制</h2><p>LRU算法：最近最少使用，时间局部性原理。</p><p>LFU算法：最近不经常使用，SDD访问模型。</p><p>ARC算法，同时考虑了LRU和LFU的长处，同时使用两个队列对缓存中页面进行管理：</p><ul><li>MRU (Most Recently Used) 队列保存最近访问过的页面</li><li><p>MFU（Most Frequently Used）队列保存最近一段时间<strong>至少被访问过两次</strong>的界面。</p></li><li><p>两个队列的长度是可变的，会根据请求队列的特征自动进行调整，取LRU和LFU共同之所长。</p><ul><li>当系统中请求序列呈现明显的时间局部性，MFU队列长度变为0，从而退化为LRU。</li><li>当系统中请求序列呈现明显的空间局部性，MRU队列长度变为0，从而退化为LFU。</li></ul></li></ul><p>2Q算法：双队列热点算法，一种针对数据库特别是关系数据库系统优化的缓存淘汰算法：</p><p>数据库系统由于需要保证每个操作的原子性，所以经常存在多个事务操作同一块热点数据的场景，因此针对数据库系统的缓存淘汰算法主要关注如何识别多个并发事务之间的数据相关性。</p><p>与ARC类似，2Q也使用了多个队列来管理整个缓存空间，分布称为$A1in,A1out,Am$。这些队列都是LRU队列，其中$A1in$与$Am$是真正的缓存队列，$A1out$是影子队列，i.e.只保存相关页面的管理结构。</p><ul><li>新的页面一开始总是被加入A1in，当某个页面被频繁访问，2Q认为这些访问是相关的，不会针对该页面执行任何热度提升的操作，直到其被正常淘汰至Aout。这个时间间隔被称为“相关时间间隔”。</li><li>当A1out中某个页面被再次访问时，2Q认为这些访问不再相关，此时执行页面热度提升，将其加入Am头部。Am队列中的页面再次被命中时，同样将其加入Am队列头部进行页面热度提升。从Am中淘汰的页面也进入A1out。这个时间间隔被称为“热度保留间隔”。</li></ul><h2 id="4-缓存管理"><a href="#4-缓存管理" class="headerlink" title="4 缓存管理"></a>4 缓存管理</h2><p>BlueStore 目前采用了LRU和2Q两种算法。</p><p>参考Theodore和Dennis的测试结论，推荐A1in和Am队列的容量配比1:1.</p><p>BlueStore的cache既可以用于缓存用户数据，也可以用于缓存元数据。bluestore中默认元数据的比重位90%。</p><p>BlueStore中元素据分为两类：Collection和Onode. Collection是PG在BlueStore中内存管理结构。每个OSD最多承载100个PG而且Collection管理结构本身比较小，故被设计成常驻内存。而Onode的数量和其管理的磁盘空间成正比，因而不可能常驻内存，需要引入淘汰机制。Onode采用LRU。</p><h2 id="5-BlueFS"><a href="#5-BlueFS" class="headerlink" title="5 BlueFS"></a>5 BlueFS</h2><p>诞生于2011年的LevelDB是基于Google的BigTable数据库系统发展而来。然而随着SSD普及，LevelDB无法发挥SSD全部性能，因而诞生了RocksDB。</p><ul><li><p>RocksDB适合存储小型或者中型键值对；性能随着键值对长度上升下降很快。</p></li><li><p>性能随CPU核数以及后端存储设备的I/O能力呈线性扩展。</p></li></ul><p>传统的本地文件系统（XFS，ext4，ZFS）等不能与RocksDB完全兼容，因而专门为其量身打造一款本地文件系统——BlueFS。在逻辑空间上分为三个层次 </p><p>（1）慢速空间 </p><p> 主要用于存储对象数据，可由大容量机械硬盘担任存储。</p><p>（2）高速空间（DB）</p><p>主要存储BlueStore内部的元素据，比如Onode。 可以由SSD提供。</p><p>（3）超高速（WAL）</p><p>WAL(Write Ahead Log)指日志。 可以由NVMe SSD或NVRAM等高速设备充当。</p><p>BlueFS上的磁盘数据包括文件、目录、日志三种类型。其定位文件分为两步：1. 通过<code>dir_map</code>找到文件的最底层文件夹 2.通过<code>file_map</code>找到对应的文件。其磁盘数据结构如下：</p><div class="table-container"><table><thead><tr><th style="text-align:center">成员</th><th style="text-align:center">含义</th></tr></thead><tbody><tr><td style="text-align:center">ino</td><td style="text-align:center">唯一标识一个fnode</td></tr><tr><td style="text-align:center">size</td><td style="text-align:center">文件大小</td></tr><tr><td style="text-align:center">mtime</td><td style="text-align:center">文件上一次被修改时间</td></tr><tr><td style="text-align:center">prefer_bdev</td><td style="text-align:center">存储该文件优先使用的设备</td></tr><tr><td style="text-align:center">extents</td><td style="text-align:center">磁盘上物理段集合包括{bdev，offset，length}</td></tr></tbody></table></div><p><img src="/img/image-20201202001057524.png" alt="image-20201202001057524"></p><h2 id="6-ObjectStore-OS"><a href="#6-ObjectStore-OS" class="headerlink" title="6 ObjectStore(OS)"></a>6 ObjectStore(OS)</h2><p>Ceph是一个指导原则是所有存储的不管是块设备、对象存储、文件存储最后都转化成了底层的对象object，这个object包含3个元素data，xattr，omap。data是保存对象的数据；xattr是保存对象的扩展属性，每个对象文件都可以设置文件的属性，这个属性是一个key/value值对，这类操作的特征是kv对并且与某一个Object关联，但是受到文件系统的限制，key/value对的个数和每个value的大小都进行了限制。如果要设置的对象的key/value不能存储在文件的扩展属性中；还存在另外一种方式保存omap(在Ceph中称为omap)，omap实际上是保存到了key/vaule  值对的RocksDB中，在这里value的值限制要比xattr中好的多。</p><p>对于FileStore实现，每个Object在FileStore层会被看成是一个文件，Object的属性(xattr)会利用文件的xattr属性存取，因为有些文件系统(如Ext4)对xattr的长度有限制，因此超出长度的Metadata会被存储在DBObjectMap里。而Object的omap则直接利用DBObjectMap实现。因此，可以看出xattr和omap操作是互通的，在用户角度来说，前者可以看作是受限的长度，后者更宽泛(API没有对这些做出硬性要求)。目前纠删码还不支持omap。</p><p>而在BlueStore则没有这种限制。</p><hr><h1 id="部署和操作BlueStore"><a href="#部署和操作BlueStore" class="headerlink" title="部署和操作BlueStore"></a>部署和操作BlueStore</h1><h1 id="BLUESTORE迁移"><a href="#BLUESTORE迁移" class="headerlink" title="BLUESTORE迁移"></a>BLUESTORE迁移</h1><p>每个OSD都可以运行BlueStore或FileStore，并且单个Ceph集群可以包含两者的混合。先前已部署FileStore的用户可能希望过渡到BlueStore，以利用改进的性能和健壮性。有几种策略可以实现这种过渡。</p><p>单个OSD不能单独进行原地转换，但是：BlueStore和FileStore根本不同，以致于无法实用。“转换”将依靠群集的正常复制和修复支持，或者依靠将OSD内容从旧的（FileStore）设备复制到新的（BlueStore）设备的工具和策略。</p><h2 id="部署新的OSD与BLUESTORE"><a href="#部署新的OSD与BLUESTORE" class="headerlink" title="部署新的OSD与BLUESTORE"></a>部署新的OSD与BLUESTORE</h2><p>可以使用BlueStore部署任何新的OSD（例如，在扩展群集时）。这是默认行为，因此不需要进行特定更改。</p><p>同样，更换故障驱动器后重新配置的任何OSD都可以使用BlueStore。</p><h2 id="将现有的OSD"><a href="#将现有的OSD" class="headerlink" title="将现有的OSD"></a>将现有的OSD</h2><h3 id="标记并替换"><a href="#标记并替换" class="headerlink" title="标记并替换"></a>标记并替换</h3><p>最简单的方法是依次标记每个设备，等待数据在群集中复制，重新配置OSD，然后再次将其标记回。它很容易实现自动化。但是，它需要的数据迁移量超出了必要，因此不是最佳选择。</p><ol><li><p>确定要替换的FileStore OSD：</p><pre><code class="hljs ini"><span class="hljs-attr">ID</span>=&lt;osd-id-number&gt;<span class="hljs-attr">DEVICE</span>=&lt;disk-device&gt;</code></pre><p>您可以使用以下命令判断给定的OSD是FileStore还是BlueStore：</p><pre><code class="hljs perl">ceph osd metadata $ID | <span class="hljs-keyword">grep</span> osd_objectstore</code></pre><p>您可以使用以下命令获取文件存储与bluestore的当前计数：</p><pre><code class="hljs applescript">ceph osd <span class="hljs-built_in">count</span>-metadata osd_objectstore</code></pre></li><li><p>将文件存储OSD标记为：</p><pre><code class="hljs nginx"><span class="hljs-attribute">ceph</span> osd out <span class="hljs-variable">$ID</span></code></pre></li><li><p>等待数据从有问题的OSD迁移：</p><pre><code class="hljs bash"><span class="hljs-keyword">while</span> ! ceph osd safe-to-destroy <span class="hljs-variable">$ID</span> ; <span class="hljs-keyword">do</span> sleep 60 ; <span class="hljs-keyword">done</span></code></pre></li><li><p>停止OSD：</p><pre><code class="hljs bash">systemctl <span class="hljs-built_in">kill</span> ceph-osd@<span class="hljs-variable">$ID</span></code></pre></li><li><p>记下此OSD使用的设备：</p><pre><code class="hljs crystal">mount | grep /var/<span class="hljs-class"><span class="hljs-keyword">lib</span>/<span class="hljs-title">ceph</span>/<span class="hljs-title">osd</span>/<span class="hljs-title">ceph</span>-$<span class="hljs-title">ID</span></span></code></pre></li><li><p>卸载OSD：</p><pre><code class="hljs crystal">umount /var/<span class="hljs-class"><span class="hljs-keyword">lib</span>/<span class="hljs-title">ceph</span>/<span class="hljs-title">osd</span>/<span class="hljs-title">ceph</span>-$<span class="hljs-title">ID</span></span></code></pre></li><li><p>销毁OSD数据。请<em>格外小心，</em>因为这会破坏设备的内容；在继续操作之前，请确保不需要设备上的数据（即，群集运行状况良好）。</p><pre><code class="hljs dockerfile">ceph-<span class="hljs-keyword">volume</span><span class="bash"> lvm zap <span class="hljs-variable">$DEVICE</span></span></code></pre></li><li><p>告诉集群OSD已被破坏（并且可以使用相同的ID重新配置新的OSD）：</p><pre><code class="hljs nginx"><span class="hljs-attribute">ceph</span> osd destroy <span class="hljs-variable">$ID</span> --<span class="hljs-literal">yes</span>-i-really-mean-it</code></pre></li><li><p>使用相同的OSD ID在其位置重新配置BlueStore OSD。这要求您确实根据上面看到的内容确定要擦除的设备。小心！</p><pre><code class="hljs dockerfile">ceph-<span class="hljs-keyword">volume</span><span class="bash"> lvm create --bluestore --data <span class="hljs-variable">$DEVICE</span> --osd-id <span class="hljs-variable">$ID</span></span></code></pre></li><li><p>重复。</p></li></ol><p>您可以允许替换OSD的重新填充与下一个OSD的排空同时进行，或者对多个OSD并行执行相同的步骤，只要确保在销毁群集之前群集是完全干净的（所有数据具有所有副本）即可。任何OSD。否则，将减少数据的冗余，并增加（甚至可能导致）数据丢失的风险。</p><p>优点：</p><ul><li>简单。</li><li>可以逐个设备完成。</li><li>不需要备用设备或主机。</li></ul><p>缺点：</p><ul><li>数据通过网络复制了两次：一次复制到集群中的其他OSD（以保持所需的副本数），然后再次返回到重新配置的BlueStore OSD。</li></ul><h3 id="整个主机更换"><a href="#整个主机更换" class="headerlink" title="整个主机更换"></a>整个主机更换</h3><p>如果集群中有一个备用主机，或者有足够的可用空间来疏散整个主机以用作备用主机，则可以在每个主机的基础上使用存储的每个数据副本进行转换仅迁移一次。</p><p>首先，您需要有一个没有数据的空主机。有两种方法可以执行此操作：从尚未包含在群集中的新的空主机开始，或者从群集中现有主机上卸载数据。</p><h4 id="使用新的，空的主机"><a href="#使用新的，空的主机" class="headerlink" title="使用新的，空的主机"></a>使用新的，空的主机</h4><p>理想情况下，主机应具有与将要转换的其他主机大致相同的容量（尽管并不严格）。</p><pre><code class="hljs ini"><span class="hljs-attr">NEWHOST</span>=&lt;empty-host-name&gt;</code></pre><p>将主机添加到CRUSH层次结构，但不要将其附加到根目录：</p><pre><code class="hljs smali">ceph osd crush<span class="hljs-built_in"> add-bucket </span>$NEWHOST host</code></pre><p>确保已安装ceph软件包。</p><h4 id="使用现有的主机"><a href="#使用现有的主机" class="headerlink" title="使用现有的主机"></a>使用现有的主机</h4><p>如果要使用已经是群集一部分的现有主机，并且该主机上有足够的可用空间，以便可以迁移其所有数据，则可以执行以下操作：</p><pre><code class="hljs autoit">OLDHOST=&lt;existing-cluster-host-<span class="hljs-keyword">to</span>-offload&gt;ceph osd crush unlink $OLDHOST <span class="hljs-keyword">default</span></code></pre><p>其中“默认”是CRUSH地图中的直接祖先。（对于具有未修改配置的较小群集，通常将是“默认”，但也可能是机架名称。）现在，您应该在OSD树输出的顶部看到没有父节点的主机：</p><pre><code class="hljs lsl">$ bin/ceph osd treeID CLASS WEIGHT  TYPE NAME     STATUS REWEIGHT PRI-AFF<span class="hljs-number">-5</span>             <span class="hljs-number">0</span> host oldhost<span class="hljs-number">10</span>   ssd <span class="hljs-number">1.00000</span>     osd<span class="hljs-number">.10</span>        up  <span class="hljs-number">1.00000</span> <span class="hljs-number">1.00000</span><span class="hljs-number">11</span>   ssd <span class="hljs-number">1.00000</span>     osd<span class="hljs-number">.11</span>        up  <span class="hljs-number">1.00000</span> <span class="hljs-number">1.00000</span><span class="hljs-number">12</span>   ssd <span class="hljs-number">1.00000</span>     osd<span class="hljs-number">.12</span>        up  <span class="hljs-number">1.00000</span> <span class="hljs-number">1.00000</span><span class="hljs-number">-1</span>       <span class="hljs-number">3.00000</span> root <span class="hljs-section">default</span><span class="hljs-number">-2</span>       <span class="hljs-number">3.00000</span>     host foo <span class="hljs-number">0</span>   ssd <span class="hljs-number">1.00000</span>         osd<span class="hljs-number">.0</span>     up  <span class="hljs-number">1.00000</span> <span class="hljs-number">1.00000</span> <span class="hljs-number">1</span>   ssd <span class="hljs-number">1.00000</span>         osd<span class="hljs-number">.1</span>     up  <span class="hljs-number">1.00000</span> <span class="hljs-number">1.00000</span> <span class="hljs-number">2</span>   ssd <span class="hljs-number">1.00000</span>         osd<span class="hljs-number">.2</span>     up  <span class="hljs-number">1.00000</span> <span class="hljs-number">1.00000</span>...</code></pre><p>如果一切正常，请直接跳到下面的“等待数据迁移完成”步骤，然后从那里继续进行操作以清理旧的OSD。</p><h4 id="迁移过程"><a href="#迁移过程" class="headerlink" title="迁移过程"></a>迁移过程</h4><p>如果您使用的是新主机，请从步骤1开始。对于现有主机，请跳至下面的步骤5。</p><ol><li><p>为所有设备配置新的BlueStore OSD：</p><pre><code class="hljs awk">ceph-volume lvm create --bluestore --data <span class="hljs-regexp">/dev/</span><span class="hljs-variable">$DEVICE</span></code></pre></li><li><p>验证OSD通过以下方式加入集群：</p><pre><code class="hljs dos">ceph osd <span class="hljs-built_in">tree</span></code></pre><p>您应该看到新主机<code>$NEWHOST</code>与它下面的所有的OSD的，但主机应该<em>不</em>被嵌套任何其他节点下的层次结构（像）。例如，如果是空主机，则可能会看到以下内容：<code>root default``newhost</code></p><pre><code class="hljs lsl">$ bin/ceph osd treeID CLASS WEIGHT  TYPE NAME     STATUS REWEIGHT PRI-AFF<span class="hljs-number">-5</span>             <span class="hljs-number">0</span> host newhost<span class="hljs-number">10</span>   ssd <span class="hljs-number">1.00000</span>     osd<span class="hljs-number">.10</span>        up  <span class="hljs-number">1.00000</span> <span class="hljs-number">1.00000</span><span class="hljs-number">11</span>   ssd <span class="hljs-number">1.00000</span>     osd<span class="hljs-number">.11</span>        up  <span class="hljs-number">1.00000</span> <span class="hljs-number">1.00000</span><span class="hljs-number">12</span>   ssd <span class="hljs-number">1.00000</span>     osd<span class="hljs-number">.12</span>        up  <span class="hljs-number">1.00000</span> <span class="hljs-number">1.00000</span><span class="hljs-number">-1</span>       <span class="hljs-number">3.00000</span> root <span class="hljs-section">default</span><span class="hljs-number">-2</span>       <span class="hljs-number">3.00000</span>     host oldhost1 <span class="hljs-number">0</span>   ssd <span class="hljs-number">1.00000</span>         osd<span class="hljs-number">.0</span>     up  <span class="hljs-number">1.00000</span> <span class="hljs-number">1.00000</span> <span class="hljs-number">1</span>   ssd <span class="hljs-number">1.00000</span>         osd<span class="hljs-number">.1</span>     up  <span class="hljs-number">1.00000</span> <span class="hljs-number">1.00000</span> <span class="hljs-number">2</span>   ssd <span class="hljs-number">1.00000</span>         osd<span class="hljs-number">.2</span>     up  <span class="hljs-number">1.00000</span> <span class="hljs-number">1.00000</span>...</code></pre></li><li><p>确定要转换的第一个目标主机</p><pre><code class="hljs ini"><span class="hljs-attr">OLDHOST</span>=&lt;existing-cluster-host-to-convert&gt;</code></pre></li><li><p>将新主机交换到群集中旧主机的位置：</p><pre><code class="hljs nginx"><span class="hljs-attribute">ceph</span> osd crush swap-bucket <span class="hljs-variable">$NEWHOST</span> <span class="hljs-variable">$OLDHOST</span></code></pre><p>此时，所有数据<code>$OLDHOST</code>将开始迁移到上的OSD <code>$NEWHOST</code>。如果新旧主机的总容量不同，您可能还会看到一些数据迁移到集群中的其他节点或从集群的其他节点迁移，但是只要这些主机的大小相同，这将是相对少量的数据。</p></li><li><p>等待数据迁移完成：</p><pre><code class="hljs reasonml"><span class="hljs-keyword">while</span> ! ceph osd safe-<span class="hljs-keyword">to</span>-destroy <span class="hljs-constructor">$(<span class="hljs-params">ceph</span> <span class="hljs-params">osd</span> <span class="hljs-params">ls</span>-<span class="hljs-params">tree</span> $OLDHOST)</span>; <span class="hljs-keyword">do</span> sleep <span class="hljs-number">60</span> ; <span class="hljs-keyword">done</span></code></pre></li><li><p>停止所有空的旧OSD <code>$OLDHOST</code>：</p><pre><code class="hljs crystal">ssh $OLDHOSTsystemctl kill ceph-osd.targetumount /var/<span class="hljs-class"><span class="hljs-keyword">lib</span>/<span class="hljs-title">ceph</span>/<span class="hljs-title">osd</span>/<span class="hljs-title">ceph</span>-*</span></code></pre></li><li><p>销毁并清除旧的OSD：</p><pre><code class="hljs nginx">for osd in `ceph osd ls-tree $OLDHOST`; do    <span class="hljs-attribute">ceph</span> osd purge <span class="hljs-variable">$osd</span> --<span class="hljs-literal">yes</span>-i-really-mean-itdone</code></pre></li><li><p>擦拭旧的OSD设备。这要求您确定要手动擦除哪些设备（请小心！）。对于每个设备：</p><pre><code class="hljs dockerfile">ceph-<span class="hljs-keyword">volume</span><span class="bash"> lvm zap <span class="hljs-variable">$DEVICE</span></span></code></pre></li><li><p>将现在为空的主机用作新主机，然后重复：</p><pre><code class="hljs ini"><span class="hljs-attr">NEWHOST</span>=<span class="hljs-variable">$OLDHOST</span></code></pre></li></ol><p>优点：</p><ul><li>数据只能通过网络复制一次。</li><li>一次转换整个主机的OSD。</li><li>可以并行转换为一次转换多个主机。</li><li>每个主机上都不需要备用设备。</li></ul><p>缺点：</p><ul><li>需要备用主机。</li><li>整个主机的OSD值将同时迁移数据。这很可能会影响整个群集的性能。</li><li>所有迁移的数据仍然在网络上进行了一整跳。</li></ul><h3 id="每OSD设备副本"><a href="#每OSD设备副本" class="headerlink" title="每OSD设备副本"></a>每OSD设备副本</h3><p>可以使用的<code>copy</code>功能转换单个逻辑OSD <code>ceph-objectstore-tool</code>。这要求主机具有一个或多个空闲设备来供应新的空BlueStore OSD。例如，如果群集中的每个主机都有12个OSD，则需要第13个可用设备，以便可以依次转换每个OSD，然后再收回旧设备以转换下一个OSD。</p><p>注意事项：</p><ul><li>此策略要求准备一个空白的BlueStore OSD，而无需分配该<code>ceph-volume</code> 工具不支持的新OSD ID 。更重要的是，<em>dmcrypt</em>的设置与OSD身份紧密相关，这意味着该方法不适用于加密的OSD。</li><li>设备必须手动分区。</li><li>工具未实现！</li><li>没有记录！</li></ul><p>优点：</p><ul><li>在转换期间，很少或没有数据在网络上迁移。</li></ul><p>缺点：</p><ul><li>工具尚未完全实现。</li><li>流程未记录。</li><li>每个主机必须具有备用或空设备。</li><li>OSD在转换过程中处于脱机状态，这意味着新的写入操作将仅写入OSD的一部分。这会增加由于后续故障而导致数据丢失的风险。（但是，如果在转换完成之前出现故障，则可以启动原始FileStore OSD来提供对其原始数据的访问。）</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;div class=&quot;note note-primary&quot;&gt;
            &lt;p&gt;本blog包括理论和实践两个部分，实践部分需要您事先部署成功Ceph集群！&lt;/p&gt;&lt;p&gt;参考《Ceph设计与实现》谢型果等，第二章。以及&lt;a href=&quot;https://docs.cep</summary>
      
    
    
    
    <category term="ceph" scheme="http://durantthorvalds.top/categories/ceph/"/>
    
    <category term="bluestore" scheme="http://durantthorvalds.top/categories/ceph/bluestore/"/>
    
    
    <category term="ceph" scheme="http://durantthorvalds.top/tags/ceph/"/>
    
    <category term="理论" scheme="http://durantthorvalds.top/tags/%E7%90%86%E8%AE%BA/"/>
    
  </entry>
  
  <entry>
    <title>「参考」Ceph配置参数conf</title>
    <link href="http://durantthorvalds.top/2020/12/15/Ceph%20%E9%85%8D%E7%BD%AE%E5%8F%82%E6%95%B0/"/>
    <id>http://durantthorvalds.top/2020/12/15/Ceph%20%E9%85%8D%E7%BD%AE%E5%8F%82%E6%95%B0/</id>
    <published>2020-12-15T08:00:00.000Z</published>
    <updated>2020-12-16T08:33:05.680Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Ceph-配置参数"><a href="#Ceph-配置参数" class="headerlink" title="Ceph 配置参数"></a>Ceph 配置参数</h1><blockquote><p>涉及pool, PG, CRUSH的配置参数。</p><p>参考<a href="https://docs.ceph.com/en/latest/rados/configuration/pool-pg-config-ref/">官方文档</a></p></blockquote><p>一个典型的Ceph配置文件如下：</p><pre><code class="hljs routeros">[global]# By default, Ceph makes 3 replicas of objects. <span class="hljs-keyword">If</span> you want <span class="hljs-keyword">to</span> make four# copies of an object the<span class="hljs-built_in"> default </span>value--a primary copy <span class="hljs-keyword">and</span> three replica# copies--reset the<span class="hljs-built_in"> default </span>values as shown <span class="hljs-keyword">in</span> <span class="hljs-string">&#x27;osd pool default size&#x27;</span>.# <span class="hljs-keyword">If</span> you want <span class="hljs-keyword">to</span> allow Ceph <span class="hljs-keyword">to</span> write a lesser number of copies <span class="hljs-keyword">in</span> a degraded# state, <span class="hljs-builtin-name">set</span> <span class="hljs-string">&#x27;osd pool default min size&#x27;</span> <span class="hljs-keyword">to</span> a number less than the# <span class="hljs-string">&#x27;osd pool default size&#x27;</span> value.osd<span class="hljs-built_in"> pool default </span>size = 3  # Write an object 3 times.osd<span class="hljs-built_in"> pool default </span>min size = 2 # Allow writing two copies <span class="hljs-keyword">in</span> a degraded state.# Ensure you have a realistic number of placement groups. We recommend# approximately 100 per OSD. E.g., total number of OSDs multiplied by 100# divided by the number of replicas (i.e., osd<span class="hljs-built_in"> pool default </span>size). So <span class="hljs-keyword">for</span># 10 OSDs <span class="hljs-keyword">and</span> osd<span class="hljs-built_in"> pool default </span>size = 4, we<span class="hljs-string">&#x27;d recommend approximately</span><span class="hljs-string"># (100 * 10) / 4 = 250.</span><span class="hljs-string">        # always use the nearest power of 2</span><span class="hljs-string"></span><span class="hljs-string">osd pool default pg num = 256</span><span class="hljs-string">osd pool default pgp num = 256</span></code></pre><div class="note note-primary">            <p>笔者注：</p><p>关于PG和PGP：</p><ul><li><p>PG =放置组( Placement Group)<br>PGP =用于放置的放置组(Placement Group for Placement purpose)</p><p>pg_num = 映射到OSD的PG的数量，它必须是2的幂次。</p><p>当对任何一个池增加pg_num时，该池的每个PG都会分裂成一半，但它们都将始终映射到其父OSD。</p><p>在此之前，Ceph不会开始重新平衡。 现在，当您为同一池增加pgp_num值时，PG开始从父级迁移到其他OSD，并且群集重新平衡开始。 这就是PGP扮演重要角色的方式。</p></li></ul>          </div><hr><blockquote><p>标记为※的是笔者认为比较重要的或者需要重点理解的配置项。供参考。</p></blockquote><h2 id="PG"><a href="#PG" class="headerlink" title="PG"></a>PG</h2><h3 id="※mon-max-pool-pg-num"><a href="#※mon-max-pool-pg-num" class="headerlink" title="※mon max pool pg num"></a>※mon max pool pg num</h3><ul><li><p>描述</p><p>每个池的最大放置组数。</p></li><li><p>类型</p><p>整数</p></li><li><p>默认</p><p><code>65536</code></p></li></ul><h3 id="※mon-pg-create-interval"><a href="#※mon-pg-create-interval" class="headerlink" title="※mon pg create interval"></a>※mon pg create interval</h3><ul><li><p>描述</p><p>在同一Ceph OSD守护进程中创建PG之间的秒数。[interval含义参考PG那篇博文]</p></li><li><p>类型</p><p>浮动</p></li><li><p>默认</p><p><code>30.0</code></p></li></ul><h3 id="mon-pg-stuck-threshold"><a href="#mon-pg-stuck-threshold" class="headerlink" title="mon pg stuck threshold"></a>mon pg stuck threshold</h3><ul><li><p>描述</p><p>PG被认为阻塞的秒数。</p></li><li><p>类型</p><p>32位整数</p></li><li><p>默认</p><p><code>300</code></p></li></ul><h3 id="mon-pg-min-inactive"><a href="#mon-pg-min-inactive" class="headerlink" title="mon pg min inactive"></a>mon pg min inactive</h3><ul><li><p>描述</p><p>如果PG保持不活动状态的时间<code>mon_pg_stuck_threshold</code>超过此设置的时间，将在群集日志中发出一个<code>HEALTH_ERR</code>信号。非正数表示禁用。</p></li><li><p>类型</p><p>整数</p></li><li><p>默认</p><p><code>1</code></p></li></ul><h3 id="mon-pg-warn-min-per-osd"><a href="#mon-pg-warn-min-per-osd" class="headerlink" title="mon pg warn min per osd"></a>mon pg warn min per osd</h3><ul><li><p>描述</p><p>如果每个OSD中的PG的平均数量低于此数量，则在群集日志中发出 <code>HEALTH_WARN</code>。（非正数禁用此功能）</p></li><li><p>类型</p><p>整数</p></li><li><p>默认</p><p><code>30</code></p></li></ul><h3 id="mon-pg-warn-min-objects"><a href="#mon-pg-warn-min-objects" class="headerlink" title="mon pg warn min objects"></a>mon pg warn min objects</h3><ul><li><p>描述</p><p>如果群集中的对象总数低于此数目，则不发出警告</p></li><li><p>类型</p><p>整数</p></li><li><p>默认</p><p><code>1000</code></p></li></ul><h3 id="mon-pg-warn-min-pool-objects"><a href="#mon-pg-warn-min-pool-objects" class="headerlink" title="mon pg warn min pool objects"></a>mon pg warn min pool objects</h3><ul><li><p>描述</p><p>对象号低于此数字的池不发出警告</p></li><li><p>类型</p><p>整数</p></li><li><p>默认</p><p><code>1000</code></p></li></ul><h3 id="mon-pg-check-down-all-threshold"><a href="#mon-pg-check-down-all-threshold" class="headerlink" title="mon pg check down all threshold"></a>mon pg check down all threshold</h3><ul><li><p>描述</p><p>降低OSD百分比的阈值之后，我们将检查所有PG的陈旧状态。</p></li><li><p>类型</p><p>浮动</p></li><li><p>默认</p><p><code>0.5</code></p></li></ul><h3 id="mon-pg-warn-max-object-skew"><a href="#mon-pg-warn-max-object-skew" class="headerlink" title="mon pg warn max object skew"></a>mon pg warn max object skew</h3><ul><li><p>描述</p><p>如果某个特定池的平均对象数大于整个池的平均对象数，在群集日志中发出<code>HEALTH_WARN</code> 。（零或非正数将禁用此功能）。请注意，此选项适用于管理者。<code>mon pg warn max object skew</code></p></li><li><p>类型</p><p>浮动</p></li><li><p>默认</p><p><code>10</code></p></li></ul><h3 id="mon-delta-reset-interval"><a href="#mon-delta-reset-interval" class="headerlink" title="mon delta reset interval"></a>mon delta reset interval</h3><ul><li><p>描述</p><p>在将pg delta重置为0之前，处于非活动状态的秒数。我们跟踪每个池的已用空间的delta，因此，例如，对于我们来说，更容易理解恢复的进度或缓存层的性能。但是，如果没有报告某个池的活动，我们只需重置该池的增量历史记录即可。</p></li><li><p>类型</p><p>整数</p></li><li><p>默认</p><p><code>10</code></p></li></ul><h3 id="mon-osd-max-op-age"><a href="#mon-osd-max-op-age" class="headerlink" title="mon osd max op age"></a>mon osd max op age</h3><ul><li><p>描述</p><p>关注之前的最大操作年龄（使其为2的幂）。如果请求被阻止的时间超过此限制，则将发出<code>HEALTH_WARN</code>。</p></li><li><p>类型</p><p>浮动</p></li><li><p>默认</p><p><code>32.0</code></p></li></ul><h2 id="OSD"><a href="#OSD" class="headerlink" title="OSD"></a>OSD</h2><h3 id="osd-pg-bits"><a href="#osd-pg-bits" class="headerlink" title="osd pg bits"></a>osd pg bits</h3><ul><li><p>描述</p><p>每个Ceph OSD守护程序的放置组位数。</p></li><li><p>类型</p><p>32位整数</p></li><li><p>默认</p><p><code>6</code></p></li></ul><h3 id="osd-pgp-bits"><a href="#osd-pgp-bits" class="headerlink" title="osd pgp bits"></a>osd pgp bits</h3><ul><li><p>描述</p><p>PGP的每个Ceph OSD守护程序的位数。</p></li><li><p>类型</p><p>32位整数</p></li><li><p>默认</p><p><code>6</code></p></li></ul><h3 id="※osd-crush-chooseleaf-type"><a href="#※osd-crush-chooseleaf-type" class="headerlink" title="※osd crush chooseleaf type"></a>※osd crush chooseleaf type</h3><ul><li><p>描述</p><p><code>chooseleaf</code>在CRUSH规则中使用的存储桶类型。使用顺序等级而不是名称。</p></li><li><p>类型</p><p>32位整数</p></li><li><p>默认</p><p><code>1</code>。通常，一台主机包含一个或多个Ceph OSD守护程序。</p></li></ul><h3 id="※osd-crush-initial-weight"><a href="#※osd-crush-initial-weight" class="headerlink" title="※osd crush initial weight"></a>※osd crush initial weight</h3><ul><li><p>描述</p><p>将新添加的osds的初始权重添加到crushmap中。【参考crush那篇文章】</p></li><li><p>类型</p><p>双</p></li><li><p>默认</p><p><code>the size of newly added osd in TB</code>。默认情况下，新添加的osd的初始压缩重量设置为以TB为单位的卷大小。有关详细信息，请参见对<a href="https://docs.ceph.com/en/latest/rados/operations/crush-map#weightingbucketitems">存储桶项目</a>进行<a href="https://docs.ceph.com/en/latest/rados/operations/crush-map#weightingbucketitems">加权</a>。</p></li></ul><h3 id="※osd-pool-default-crush-rule"><a href="#※osd-pool-default-crush-rule" class="headerlink" title="※osd pool default crush rule"></a>※osd pool default crush rule</h3><ul><li><p>描述</p><p>创建复制池时要使用的默认CRUSH规则。</p></li><li><p>类型</p><p>8位整数</p></li><li><p>默认</p><p><code>-1</code>，这意味着“<strong>选择数字ID最低的规则并使用它</strong>”。这是为了在没有规则0的情况下创建池。</p></li></ul><h3 id="※osd-pool-erasure-code-stripe-unit"><a href="#※osd-pool-erasure-code-stripe-unit" class="headerlink" title="※osd pool erasure code stripe unit"></a>※osd pool erasure code stripe unit</h3><ul><li><p>描述</p><p>设置用于纠删码池的对象条带块的默认大小（以字节为单位）。每个大小为S的对象将存储为N条，每个数据块接收字节。每个字节的条带<code>N*strip_unit</code>将分别进行编码/解码。可以通过纠删码配置文件中的设置<code>strip_unit</code>来覆盖此选项 。</p></li><li><p>类型</p><p>无符号32位整数</p></li><li><p>默认</p><p><code>4096</code></p></li></ul><h3 id="※osd-pool-default-size"><a href="#※osd-pool-default-size" class="headerlink" title="※osd pool default size"></a>※osd pool default size</h3><ul><li><p>描述</p><p>设置池中对象的副本数。预设值与相同 。<code>ceph osd pool set &#123;pool-name&#125; size &#123;size&#125;</code></p></li><li><p>类型</p><p>32位整数</p></li><li><p>默认</p><p><code>3</code></p></li></ul><h3 id="※osd-pool-default-min-size"><a href="#※osd-pool-default-min-size" class="headerlink" title="※osd pool default min size"></a>※osd pool default min size</h3><ul><li><p>描述</p><p>设置池中对象的最小写入副本数，以确认对客户端的写入操作。如果未达到最小值，则Ceph将不会确认对客户端的写入，<strong>这可能会导致数据丢失</strong>。在<code>degraded</code>模式下运行时，此设置可确保最少数量的副本。</p></li><li><p>类型</p><p>32位整数</p></li><li><p>默认</p><p><code>0</code>，表示没有特别的下限。如果<code>0</code>，最小值为。<code>size - (size / 2)</code></p></li></ul><h3 id="※osd-pool-default-pg-num"><a href="#※osd-pool-default-pg-num" class="headerlink" title="※osd pool default pg num"></a>※osd pool default pg num</h3><ul><li><p>描述</p><p>池的默认放置组数。默认值是一样<code>pg_num</code>用<code>mkpool</code>。</p></li><li><p>类型</p><p>32位整数</p></li><li><p>默认</p><p><code>32</code></p></li></ul><h3 id="osd-pool-default-pgp-num"><a href="#osd-pool-default-pgp-num" class="headerlink" title="osd pool default pgp num"></a>osd pool default pgp num</h3><ul><li><p>描述</p><p>池放置的默认放置组数。默认值是一样<code>pgp_num</code>用<code>mkpool</code>。PG和PGP应该相等（目前）。</p></li><li><p>类型</p><p>32位整数</p></li><li><p>默认</p><p><code>8</code></p></li></ul><h3 id="osd-pool-default-flags"><a href="#osd-pool-default-flags" class="headerlink" title="osd pool default flags"></a>osd pool default flags</h3><ul><li><p>描述</p><p>新池的默认标志。</p></li><li><p>类型</p><p>32位整数</p></li><li><p>默认</p><p><code>0</code></p></li></ul><h3 id="osd-max-pgls"><a href="#osd-max-pgls" class="headerlink" title="osd max pgls"></a>osd max pgls</h3><ul><li><p>描述</p><p>要列出的展示位置组的最大数量。请求大量请求的客户端可以占用Ceph OSD守护程序。</p></li><li><p>类型</p><p>无符号64位整数</p></li><li><p>默认</p><p><code>1024</code></p></li><li><p>注意</p><p>默认应该没问题。</p></li></ul><h3 id="osd-min-pg-log-entries"><a href="#osd-min-pg-log-entries" class="headerlink" title="osd min pg log entries"></a>osd min pg log entries</h3><ul><li><p>描述</p><p>修剪日志文件时要保留的最小放置组日志数。</p></li><li><p>类型</p><p>32位Int Unsigned</p></li><li><p>默认</p><p><code>250</code></p></li></ul><h3 id="osd-max-pg-log-entries"><a href="#osd-max-pg-log-entries" class="headerlink" title="osd max pg log entries"></a>osd max pg log entries</h3><ul><li><p>描述</p><p>修剪日志文件时要保留的放置组日志的最大数量。</p></li><li><p>类型</p><p>32位Int Unsigned</p></li><li><p>默认</p><p><code>10000</code></p></li></ul><h3 id="osd-default-data-pool-replay-window"><a href="#osd-default-data-pool-replay-window" class="headerlink" title="osd default data pool replay window"></a>osd default data pool replay window</h3><ul><li><p>描述</p><p>OSD等待客户端重播请求的时间（以秒为单位）。</p></li><li><p>类型</p><p>32位整数</p></li><li><p>默认</p><p><code>45</code></p></li></ul><h3 id="osd-max-pg-per-osd-hard-ratio"><a href="#osd-max-pg-per-osd-hard-ratio" class="headerlink" title="osd max pg per osd hard ratio"></a>osd max pg per osd hard ratio</h3><ul><li><p>描述</p><p>在OSD拒绝创建新PG之前，集群允许的每个OSD PG数量的比率。如果OSD服务的PG数量超过<code>osd max pg per osd hard ratio*mon max pg per osd</code>，则OSD停止创建新的PG 。</p></li><li><p>类型</p><p>浮动</p></li><li><p>默认</p><p><code>2</code></p></li></ul><h3 id="※osd-recovery-priority"><a href="#※osd-recovery-priority" class="headerlink" title="※osd recovery priority"></a>※osd recovery priority</h3><ul><li><p>描述</p><p>工作队列中恢复的优先级。</p></li><li><p>类型</p><p>整数</p></li><li><p>默认</p><p><code>5</code></p></li></ul><h3 id="※osd-recovery-op-priority"><a href="#※osd-recovery-op-priority" class="headerlink" title="※osd recovery op priority"></a>※osd recovery op priority</h3><ul><li><p>描述</p><p>如果不覆盖池，则用于恢复操作的默认优先级。</p></li><li><p>类型</p><p>整数</p></li><li><p>默认</p><p><code>3</code></p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Ceph-配置参数&quot;&gt;&lt;a href=&quot;#Ceph-配置参数&quot; class=&quot;headerlink&quot; title=&quot;Ceph 配置参数&quot;&gt;&lt;/a&gt;Ceph 配置参数&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;涉及pool, PG, CRUSH的配置参数。&lt;/p&gt;
&lt;</summary>
      
    
    
    
    <category term="ceph" scheme="http://durantthorvalds.top/categories/ceph/"/>
    
    <category term="PG" scheme="http://durantthorvalds.top/categories/ceph/PG/"/>
    
    
    <category term="Ceph字典" scheme="http://durantthorvalds.top/tags/Ceph%E5%AD%97%E5%85%B8/"/>
    
  </entry>
  
  <entry>
    <title>「核心」Ceph学习三部曲之三:迁移之美——PG读写流程与状态迁移详解</title>
    <link href="http://durantthorvalds.top/2020/12/15/%E8%BF%81%E7%A7%BB%E4%B9%8B%E7%BE%8EPG%E8%AF%BB%E5%86%99%E6%B5%81%E7%A8%8B%E4%B8%8E%E7%8A%B6%E6%80%81%E8%BF%81%E7%A7%BB%E8%AF%A6%E8%A7%A3/"/>
    <id>http://durantthorvalds.top/2020/12/15/%E8%BF%81%E7%A7%BB%E4%B9%8B%E7%BE%8EPG%E8%AF%BB%E5%86%99%E6%B5%81%E7%A8%8B%E4%B8%8E%E7%8A%B6%E6%80%81%E8%BF%81%E7%A7%BB%E8%AF%A6%E8%A7%A3/</id>
    <published>2020-12-14T16:00:00.000Z</published>
    <updated>2020-12-17T09:25:20.325Z</updated>
    
    <content type="html"><![CDATA[<h1 id="迁移之美——PG读写流程与状态迁移详解"><a href="#迁移之美——PG读写流程与状态迁移详解" class="headerlink" title="迁移之美——PG读写流程与状态迁移详解"></a>迁移之美——PG读写流程与状态迁移详解</h1><div class="note note-primary">            <p>本blog包括理论和实践两个部分，实践部分需要您事先部署成功Ceph集群！由于篇幅较大，建议先看看完理论部分的术语再看官方实践部分，最后到理论部分搜索关键字进行理解。</p><p>参考《Ceph设计与实现》谢型果等，第四章。以及<a href="https://docs.ceph.com/en/latest/rados/operations/placement-groups/">官方PG教程</a>。</p>          </div><h1 id="1-基本概念"><a href="#1-基本概念" class="headerlink" title="1 基本概念"></a>1 基本概念</h1><p>PG是Ceph最难理解的部分之一，但是它也是Ceph最精妙有意思的部分。</p><p>Placement Groups。即归置组（又称放置组）。Ceph对所有的存储资源都进行池化管理，对对象进行两级映射存储$Objects\rightarrow PGs \rightarrow OSDs $</p><ul><li><p>第一级映射是静态的，负责将任何前端类型的应用数据按照固定大小进行切割、编号后作为随机哈希函数输入，均匀映射至PG，以实现负载均衡。</p></li><li><p>第二级映射实现PG到OSD的映射。</p></li></ul><p>PG最引人关注的特性是它可以在OSD之间自由的迁移，这是Ceph赖以实现自动数据恢复、自动数据平衡等高级特性的基础。因为PG的数量远小于对象的数量，因此以PG为单位进行操作更具灵活性。</p><p><img src="/img/ceph-arch.png" alt=""></p><p>存储池中的对象到OSD的映射是通过PG来完成。一方面，存储池中PG数目决定了其并发处理多个对象的能力；另一方面，过多的PG会消耗大量CPU，同时容易使得磁盘长期处于过载状态。（经验表明：磁盘利用率保持在70%左右可以使得I/O并发能力和平均相应时延最佳）。</p><p>因此在创建存储池时，需要合理的指定PG的数目，一般将每个OSD中PG限制在100个左右最佳。</p><p>需要注意的是，创建存储池中指定的PG数目其实是指<strong>逻辑PG数目</strong>。为了数据的可靠性，Ceph会将每个逻辑PG转换为多个实例PG，由它们负责将对象的不同备份或者部分写入不同的OSD。</p><p>如果使用多副本，那么每个逻辑PG被转换为与副本数相等的PG实例。处于数据一致性考虑，我们可以选择Paxos作为数据分布一致性算法，但这样过于重量级。其实我们只需要在PG实例中选择一个主要的PG作为通用的入口进行操作分发或者集中（例如peering）。</p><p>如果使用纠删码，每个逻辑PG会被分为k+m个实例。与多副本不同，这些PG只保存每个对象的一个分片（shard），所以需要对其身份进行严格区分。同样，需要在PG实例中选择一个主要的PG。</p><p>按照约定，主要PG由CRUSH返回的第一个OSD充当。PGID由CRUSH计算，使得PG在所有OSD之间均匀分布。</p><h2 id="1-1-术语和约定"><a href="#1-1-术语和约定" class="headerlink" title="1.1 术语和约定"></a>1.1 术语和约定</h2><ul><li><p>PGID</p><p>PG有一个全局唯一的ID——$PGID$，所有的pool由Monitor统一管理，由pool-id+PG在pool内唯一ID+shard（仅适用于纠删码存储池）组成。</p></li><li><p>OS</p><p>指对象存储的种类（Object Store）例如FileStore和BlueStore。</p></li></ul><ul><li><p>Info</p><p>PG内基本元数据信息。</p></li><li><p>Log </p><p>基于Eversion顺序记录所有客户端发起的修改操作的历史信息，为后续提供历史操作回溯和数据同步的依据。</p></li><li><p>Authoritative History</p><p>指权威日志。它是Peering过程中进行数据同步的依据，通过交换Info并基于一定的规则从所有的PG实例中选举产生。通过重放权威日志，可以使得PG内部每个对象的版本号达成一致。</p></li><li><p>PGBackend</p><p>字面意思是PG后端。负责将对原始对象的操作转化为副本之间的分布式操作。对于多副本而言是<code>ReplicatedBackend</code>；对于纠删码而言是<code>ECBackend</code>。</p></li><li><p>Epoch</p><p>一般情况下指OSDMap（OSDMap 是 Ceph 集群中所有 OSD 的信息）的版本号，由Monitor生成，总是单调递增。Epoch变化意味着OSDMap发生变化，需要通过一定的策略扩散至所有客户端和位于服务端的OSD。</p></li><li><p>Version </p><p>version指本次修改生效之后的版本号。</p></li><li><p>Eversion</p><p>由Epoch和Version组成。Version总是当前的Primary产生，连续单调增，和Epoch一起标志一次PG内修改操作。如223’23。</p></li><li><p>Interval</p><p>指OSDMap一个连续的Epoch的持续时间，Interval和具体的PG绑定。</p></li><li><p>Acting Set</p><p>指一个有序的OSD集合。当前或者曾在某个Interval负责承载对应PG的PG实例。通常与Up Set相同，但有时候设置了PG Temp会导致两者不相同。</p></li><li><p>Primary</p><p>指Acting Set的第一个OSD，负责处理来自客户端的读写请求，同时也是peering的发起者和协调者。</p></li><li><p>Peering</p><p>指归属于同一个PG的所有PG实例就本PG所存储的全部对象以及对象相关的元数据操作进行协商并最终一致的过程。</p><p>Peering 基于Log和Info进行。这里的达成一致，并不表示每个PG实例都能获得最新的内容。事实上，为例尽快恢复对外业务，一旦Peering完成，在满足条件下就可以切换为Active状态，后续的数据恢复可以在后台进行。</p></li><li><p>Recovery</p><p>指针对PG某些实例进行数据同步的过程，其最终目标是将PG重新变为Active+Clean状态。它可以在后台进行。</p></li><li><p>Backfill</p><p>Backfill字面意思是回填，是Recovery的一种特殊场景，指Peering完成后，如果基于当前的权威日志无法对Up Set当中的某些PG实例实现增量同步，则通过完全拷贝当前的Primary所有对象的方式进行<strong>全量同步</strong>。</p></li><li><p>PG Temp</p><p>作为PG临时载体的OSD集合。Peering过程中，如果当前的Interval通过CRUSH计算的Up Set不合理（例如Up Set中的一些OSD新加入集群，根本没有PG的任何历史信息），那么可以通知OSDMonitor设置PG Temp的方式来显式的指定一些仍然具有相对完备PG信息的OSD加入Acting Set，使得Acting Set中的OSD再完成Peering之后能够临时处理客户端发起的读写请求，以尽可能减少业务中断的时间。上述过程会导致Up Set和Acting Set临时不一致。UpSet是CRUSH原始计算的映射结果；因为Peering过程中不能处理客户端读写请求，引入PG Temp可以缩短业务中断的时间，当Up Set中的副本在后台通过Recovery 或者Backfill 完成数据同步时，此时可以通知OSDMonitor取消PG Temp.</p><div class="note note-warning">            <ul><li>之所以需要PG Temp来修改OSDMap，是因为需要同步通知到所有客户端，让它们后续将读写请求发送到Acting Set而不是Up Set中的Primary。</li><li>PG Temp生效之后，PG将处于Remapped状态。</li><li>Peering完成之后，Up Set中与Acting Set不一致的OSD将在后台通过Recovery或者Backfill的方式与当前的Primary进行数据同步；数据同步完成后，PG需要重新修改PG Temp为空集合，完成Acting Set至Up Set的切换，此时取消Remapped标记。</li></ul>          </div></li><li><p>Stray</p><p>指PG所在的OSD不是PG当前的Acting Set中。</p></li></ul><p><img src="/img/ceph-d.png" alt="img"></p><blockquote><p>上图1：客户侧，Monitor和 Primary、Replica的关系</p></blockquote><p><img src="/img/ceph_io2.png" alt="img"></p><blockquote><p>上图2：正常的读写流程</p><p>客户侧先产生一个cluster handle（也就是后文所说的op）。之后连接monitor，再从Primary OSD进行读写。</p></blockquote><p><img src="/img/ceph-e.jpg" alt="img"></p><blockquote><p>上图3：Backfill的读写流程. 由于一些OSD离线太久，或者新的OSD加入到集群导致PG实例整体迁移，上图明显属于后者，需要通过Backfill指定临时主进行全增量同步并且选择新的Primary。</p></blockquote><p>客户端读写流程详细分析：</p><ol><li>OSD收到客户端发出的读写请求，将其封装为一个op（客户端发出的读写请求），并基于其携带的PGID发送至对应PG。</li><li>PG收到op之后，完成一系列检查，所有条件均满足后，开始真正执行op。<ul><li>如果op只包含读操作，那么直接执行同步读（对应多副本），或者异步读（对应纠删码），等待操作完成后向客户端应答。</li><li>如果op包含写操作，首先由primary基于op生成一个针对原始对象的事务及相关操作，然后将其提交给PGBackend安装备份策略转化为每个PG实例（包含所有primary和所有Replica）真正需要执行的本地事务并进行分发，当primary收到所有副本的写入完成应答之后，对应的op执行完成，此时由primary向客户端回应写完成。</li></ul></li></ol><h2 id="1-2-PG快速定位对象"><a href="#1-2-PG快速定位对象" class="headerlink" title="1.2 PG快速定位对象"></a>1.2 PG快速定位对象</h2><blockquote><p>对应参考书 2.2.1 PG</p></blockquote><p>首先由特定类型的Client根据其操作的对象名计算出一个32位的哈希值，然后根据其操作的对象名计算出一个32位的哈希值，然后根据归属的pool及此时的哈希值，通过简单的计算，比如取模，即可找到最终承载该对象的PG。</p><p>我们发现如果pool内的PG数目如果能写成$2^n$形式，那么其低n比特都是相同的。我们将$2^n-1$称为PG的<strong>掩码</strong>。否则，若PG不能写成$2^n$的形式，则不能保证针对不同的输入低n比特相同这一“稳定”的性质。（比如有12个PG，那么对于属于同一个pool的PGID只有低两位相同）</p><p>因此一种行之有效的方法是用掩码代替取模。取hash低n-1位，即$hash\&amp;(2^n-1)$ .但这种映射存在问题，如果PG数目不能被2整除，那么采用这种方式会导致空穴，也就是取模结果没有实际PG对应。</p><p>比如一个pool有12个PG，n=4，但是12-15这些值都浪费了：</p><blockquote><div class="table-container"><table><thead><tr><th>0</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>6</th><th>7</th><th>8</th><th>9</th><th>10</th><th>11</th><th>12</th><th>13</th><th>14</th><th>15</th></tr></thead><tbody><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>x</td><td>x</td><td>x</td><td>x</td></tr></tbody></table></div><p>我们可以想办法压缩空间 ，$hash\&amp;(2^{n-1}-1)$，使得不能被2整除的PGID仍能被合理映射。</p><div class="table-container"><table><thead><tr><th>0</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>6</th><th>7</th><th>8</th><th>9</th><th>10</th><th>11</th><th>12</th><th>13</th><th>14</th><th>15</th></tr></thead><tbody><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>0</td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td><td>6</td><td>7</td></tr></tbody></table></div><p>如果$hash\&amp;(2^{n}-1)&lt;pg_num$，那么可以直接返回$hash\&amp;(2^n-1)$.</p><p>否则，我们返回$hash\&amp;(2^{n-1}-1)$.</p><p>在参考书上被称为稳定哈希（stable hash）。</p></blockquote><p>Ceph主要设计理念之一是高扩展性。当集群中PG增加，新的PG会被随机均匀地映射至所有OSD上。作为stable hash的输入的PG数目已经发生变化，导致某些对象从旧PG重新映射至新PG，因此需要转移这部分对象，我们称为<strong>PG分裂</strong>（这也是为什么PG数目必须是2的次幂的原因！）。</p><h1 id="2-详细剖析PG读写流程"><a href="#2-详细剖析PG读写流程" class="headerlink" title="2 详细剖析PG读写流程"></a>2 详细剖析PG读写流程</h1><h2 id="2-1-消息接收与分发"><a href="#2-1-消息接收与分发" class="headerlink" title="2.1 消息接收与分发"></a>2.1 消息接收与分发</h2><p>OSD绑定的Public Messenger 收到客户端发送的读写请求后，通过OSD注册的回调函数——<code>ms_fast_dispatch</code>进行快速派发：</p><ul><li>基于消息(Messenger)创建一个op， 用于对消息进行跟踪，并记录消息携带的Epoch。</li><li>查找OSD关联的客户端会话上下文，，将op加入其内部的<code>waiting_on_map</code>队列，获取OSDMap，，并将其与<code>waiting_on_map</code>队列的所有op进行逐一比较——如果OSD当前的OSDMap的Epoch不小于op所携带的Epoch，则进一步将其派发至OSD的<code>op_shardedwq</code>队列（OSD内部的工作队列）；否则直接终止派发。</li><li>如果会话的上下文的<code>waiting_on_map</code>不为空，说明至少存在一个op，其携带的Epoch比OSDMap更新，此时将其加入OSD全局<code>session_waiting_for_map</code>集合，该集合汇集了当前所有需要等待OSD更新完OSDMap之后才能继续处理的会话上下文；否则将对应的会话从<code>session_waiting_for_map</code>中移除。</li></ul><p>上面有些概念我们先抛砖引玉，下面我们具体讲</p><h2 id="2-2-do-request"><a href="#2-2-do-request" class="headerlink" title="2.2 do_request"></a>2.2 do_request</h2><p><code>do_request</code>作为PG处理op的第一步，主要完成全局（PG级别的）检查：</p><ul><li>Epoch——如果op携带的Epoch更新，那么需要等待PG完成OSDMap同步之后才能进行处理。</li><li><p>op能否被直接丢弃——一些可能的场景有：</p><ul><li>op对应的客户端链接已经断开</li><li>收到op时，PG当前已经切换到一个更新的Interval（OSD生成一个连续Epoch的间隔），后续客户端会重发。</li><li>op在PG分裂之前发送，后续客户端会重发。</li></ul></li><li><p>PG自身的状态如果不为Active，op同样会被阻塞。</p><p>PG内部维护了许多不同类型的重试队列，保证请求按顺序被处理。当对应限制解除之后，op会重新进入<code>op_shardedwq</code>，等待被PG执行。</p></li></ul><h2 id="2-3-do-op"><a href="#2-3-do-op" class="headerlink" title="2.3 do_op"></a>2.3 do_op</h2><p>do_op进行对象级别的检查：</p><p>1）按照op携带的操作类型，初始化op中各种标志位。</p><p>2）完成对op的合法性校验，不合法的情况包括：1.PG包含op所携带的对象；2.op之间携带可以并发执行的标志 ；3. 客户端权限不足；4. op携带对象名称、key或者命名空间长度超过最大限制（只有在FileStore下存在此限制）5.op对应客户端被纳入黑名单 6. op在集群被标记为Full之前发送 7. PG所在的OSD存储空间不足 8. op包含写操作并且企图访问快照对象 9. op包含写操作并且一次写入量太大（超过<code>osd_max_write_size</code>）。</p><p>3）检查op携带的对象是否不可读或者处于降级状态或者正在被scrub（读取数据对象并重新计算校验和），加入相应队列。</p><p>4）检查op是否为重发（基于op的repid在当前的Log中查找，如果找到说明为重发）。</p><p>5）获取对象上下文，创建OpContext对op进行跟踪，并通过<code>execute_ctx</code>真正开始执行op。</p><blockquote><p>💬关于可用存储空间控制</p><p>Ceph使用四个配置项，<code>mon_osd_full_ratio</code>、 <code>mon_osd_nearfull_ratio</code>、<code>osd_backfill_full_ratio</code>（OSD空间利用率大于此值，PG被拒绝以backfill方式迁入）、<code>osd_failsafe_full_ratio</code>（防止OSD磁盘被100%写满的最后一道屏障）。 </p><p>每个OSD通过<strong>心跳</strong>机制周期性的检测自身空间利用率，并上报至Monitor。<code>osd_backfill_full_ratio</code>的存在意义是有些数据迁移是自动触发的，我们无法预料到自动数据平衡后数据会落到哪个磁盘，因此必须设置此项进行控制。</p><p>💬关于对象上下文 (原书134页图4-3)</p><p>对象上下文主要保存了对象OI(Object Info)和SS(Snap Set)属性。同时表明对象是否仍然存在。查找head对象上下文相对简单，如果没有在缓存中命中，直接在磁盘中读取即可。然而如果op直接操作快照或者对象克隆，这个过程将变得很复杂。其难点在于一个克隆对象可以对应多个快照，因此需要根据快照序列号定位到某个特定的克隆对象，然后通过分析其位于OI中的snap属性才能进一步判断快照序列号是否位于克隆对象之中。</p></blockquote><h2 id="2-4-execute-ctx"><a href="#2-4-execute-ctx" class="headerlink" title="2.4 execute_ctx"></a>2.4 execute_ctx</h2><p><code>execute</code>是真正执行op的步骤。它首先基于当前的快照模式，更新OpContext中的快照上下文(SnapContext)——如果是自定义快照模式，直接基于op携带的快照信息更新；否则基于PGPool更新。 </p><p>为了保证数据的一致性，所以包含修改操作的PG会预先由Primary通过<code>prepare_transcation</code>封装为一个PG事务，然后由不同类型的PGBackend负责转化为OS(objectStore,笔者注)能够识别的本地事务，最后在副本间分发和同步。</p><h2 id="2-5-事务准备"><a href="#2-5-事务准备" class="headerlink" title="2.5 事务准备"></a>2.5 事务准备</h2><p>针对多副本，因为每个副本保存的对象完全相同，所以由Primary生成的PG事务也可以直接作为每个副本的本地事务直接执行。引入纠删码之后，每个副本保存的都是独一无二的分片，所以需要对原始对象的整体操作（对应PG操作）和每个分片操作（对应OS事务）加以区分。</p><p>本节介绍如何基于op生成PG级别的事务，这个过程通过<code>prepare_transaction</code>完成。</p><ol><li>通过<code>do_osd_ops</code>生成原始op对应的PG事务。</li><li>如果op针对<code>head</code>对象操作，通过<code>make_writable</code>检查是否需要预先执行克隆操作。</li><li>通过<code>finish_ctx</code>检查是否需要创建或者删除<code>snapdir</code>对象，生成日志，并更新对象的OI及SS属性。</li></ol><p>下面我们具体介绍相关流程：</p><h3 id="1-do-osd-ops"><a href="#1-do-osd-ops" class="headerlink" title="1 do_osd_ops"></a>1 do_osd_ops</h3><ol><li>检查write操作携带的<code>trancate_seq</code>，并和对象上下文保存的<code>truncate_seq</code>比较从而判定客户端执行write操作和trimtrunc/truncate操作的真实顺序，并对write操作涉及的逻辑地址范围进行修正。</li><li>检查本地写入逻辑地址范围是否合法——例如我们当前限制对象大小不超过100GB。(对应<code>osd_max_object_size</code>)。</li><li>将write对象转化为PGTransaction中的事务。</li><li>如果是满对象写（包括新写或者覆盖写），或者为追加写并且之前存在数据校验和，则重新计算并更新OI中数据校验和，作为后续执行Deep Scrub的依据；否则清除校验和。在OpContext中积累本次write修改的逻辑地址范围以及其它统计（例如写操作次数、写入字节数），同时更新对象大小。</li></ol><h3 id="2-make-writable"><a href="#2-make-writable" class="headerlink" title="2 make_writable"></a>2 make_writable</h3><p>如果op针对head对象进行修改</p><ol><li>判断head对象是否需要执行克隆：取对象当前的SnapSet，和OpContext中SnapContext 中内容进行比较——如果SnapSet中最新的快照序列号比SnapContext中最新的快照序列号小，说明自上次快照之后，又产生新的快照。此时不能直接对head对象进行修改，而是需要先执行克隆（默认为全对象克隆）。如果SnapContext携带了多个新的快照序列号，那么所有比SnapSet中更新的快照序列号都将关联至同一个克隆对象。</li></ol><blockquote><p>这里有一个特殊情况——如果当前操作为删除head对象，并且该对象自创建之后没有经历任何修改（此时SnapSet为空），也需要该head对象正常执行克隆后再删除，后续将创建一个snapdir对象来转移这些快照及相关的克隆信息。</p></blockquote><ol><li>创建克隆对象，需要同步更新SS属性中相关信息：<ul><li>在<code>clones</code>集合中记录当前克隆对象中最新快照序列号。</li><li>在<code>clone_size</code>集合中更新当前克隆对象的大小——因为默认使用全对象克隆，所以克隆对象大小为执行克隆时head对象head对象的实时大小。</li><li>在<code>clone_overlap</code>集合中记录当前克隆对象与前一个克隆对象之间的重合部分。</li></ul></li><li>为克隆对象生成一条新的、独立的日志，更新op中日志版本号。</li><li>最后，基于SnapContext更新对象SS属性中快照信息。</li></ol><h3 id="3-finish-ctx"><a href="#3-finish-ctx" class="headerlink" title="3 finish_ctx"></a>3 finish_ctx</h3><p>顾名思义，<code>finish_ctx</code>完成事务准备阶段最后的清理工作。</p><p>1）如果创建head对象并且snapdir对象存在，则删除snapdir对象，同时生成一条删除snapdir对象的日志；如果删除head对象并且对象仍然被快照引用，则创建snapdir对象，同时生成一条创建snapdir对象的日志，并将head对象的OI和SS属性用snapdir’对象转存</p><p>2）如果对象存在，则更新对象OI属性——例如version、last_reqid、mtime等；进一步，如果是head对象，同步更新其SS属性。</p><p>3）生成一条op操作原始对象的日志，并追加至现有的OpContext中的日志集合中。</p><p>4）在OpContext关联的对象上下文中应用最新的对象状态和SS上下文。</p><h2 id="2-6-注册回调函数"><a href="#2-6-注册回调函数" class="headerlink" title="2.6  注册回调函数"></a>2.6  注册回调函数</h2><p>PG事务准备后，如果是纯粹的读操作，如果是同步读（对于多副本），op已经执行完毕，此时可以直接向客户端应答；如果是异步读（针对纠删码），则将op挂入PG内部的异步读队列，等待异步读完成之后再向客户端应答。</p><p>如果是写操作，则注册如下几类回调函数：</p><ul><li><code>on_commit</code>——执行时，向客户端发送写入完成应答；</li><li><code>on_success</code>——执行时，进行与Watch\Notify相关的处理；</li><li><code>on_finish</code>——执行时，删除OpContext。</li></ul><h2 id="2-7-事务分发与同步"><a href="#2-7-事务分发与同步" class="headerlink" title="2.7 事务分发与同步"></a>2.7 事务分发与同步</h2><p>事务的分发与同步由Primary完成，具体而言是通过RepGather实现的。RepGather被提交到PGBackend，后者负责将PG事务转化为每个副本之间的本地事务之后再进行分发。</p><p>对于纠删码而言，当涉及覆盖写时，如果改写的部分不足一个完整条带（指写入的起始地址或者数据长度没有进行条带对齐），则需要执行RMW，这期间需要多次执行补齐读、重新生成完整条带并重新计算校验块、单独生成每个副本的事务并构造消息进行分发（Write）、同时在必要时执行范围克隆和对PG日志进行修正，以支持Peering期间的回滚操作。</p><h1 id="3-PG-状态迁移详解"><a href="#3-PG-状态迁移详解" class="headerlink" title="3 PG 状态迁移详解"></a>3 PG 状态迁移详解</h1><p>PG状态分为外部状态和内部状态，其中外部状态是可以直接被用户感知的。</p><blockquote><p>​                                                                    PG外部状态表</p></blockquote><div class="table-container"><table><thead><tr><th style="text-align:center">PG状态</th><th style="text-align:left">含义</th></tr></thead><tbody><tr><td style="text-align:center">Activating</td><td style="text-align:left">Peering已经完成，PG正在等待所有PG实例同步并固化Peering结果（Info、Log）</td></tr><tr><td style="text-align:center">Active</td><td style="text-align:left">PG可以正常处理来自客户端的读写请求</td></tr><tr><td style="text-align:center">Backfilling</td><td style="text-align:left">见前面术语——Backfill部分</td></tr><tr><td style="text-align:center">Clean</td><td style="text-align:left">PG当前不存在待修复的对象，Acting Set与Up Set一致，并且大小等于存储池副本数</td></tr><tr><td style="text-align:center">Creating</td><td style="text-align:left">PG正在被创建</td></tr><tr><td style="text-align:center">Deep</td><td style="text-align:left">PG正在进行对象一致性扫描（总是与Scrubbing同时出现）</td></tr><tr><td style="text-align:center">Scrubbing</td><td style="text-align:left">PG正在进行对象一致性扫描，但Scrubbing仅扫描元数据</td></tr><tr><td style="text-align:center">Degraded</td><td style="text-align:left">Peering完成后，PG检测到任意一个PG实例存在不一致的对象；或者当前ActingSet小于存储池副本数。</td></tr><tr><td style="text-align:center">Down</td><td style="text-align:left">Peering过程中，PG检测到某个Interval中，当前剩余的OSD不足以完成数据修复</td></tr><tr><td style="text-align:center">Incomplete</td><td style="text-align:left">Peering过程中，由于：1）无法获得权威日志 2）通过<code>choose_acting</code>选出的Acting Set后续不足以完成数据修复（例如针对纠删码，存活的副本数小于k）</td></tr><tr><td style="text-align:center">Inconsistent</td><td style="text-align:left">PG通过Scrub检测到某些对象在PG实例间出现不一致（主要是因为静默错误）</td></tr><tr><td style="text-align:center">Peered</td><td style="text-align:left">指Peering完成，但是PG当前的ActingSet小于存储池规定的最小副本数。</td></tr><tr><td style="text-align:center">Recovering</td><td style="text-align:left">PG正在对不一致对象进行同步/修复。</td></tr><tr><td style="text-align:center">Remapped</td><td style="text-align:left">Peering完成，PG当前的Acting Set和Up Set出现不一致。</td></tr><tr><td style="text-align:center">Repair</td><td style="text-align:left">PG在下一次执行Scrub的过程中，如果发现存在不一致的对象，并且能够进行修复，则自动进行修复。</td></tr><tr><td style="text-align:center">Stale</td><td style="text-align:left">Monitor检测到当前Primary所在的OSD宕机；Primary超时未向Monitor上报心跳信息。</td></tr><tr><td style="text-align:center">Undersized</td><td style="text-align:left">PG当前的Acting Set小于存储池副本数</td></tr></tbody></table></div><div class="note note-primary">            <p>注意上述外部状态是可以叠加的。比如Active+clean表示一切正常。</p>          </div><h2 id="3-1-状态机描述"><a href="#3-1-状态机描述" class="headerlink" title="3.1 状态机描述"></a>3.1 状态机描述</h2><p><img src="\img\PG_DFA.png" style="zoom: 80%;" /></p><h2 id="3-2-具体流程分析"><a href="#3-2-具体流程分析" class="headerlink" title="3.2 具体流程分析"></a>3.2 具体流程分析</h2><h2 id="1-创建PG"><a href="#1-创建PG" class="headerlink" title="1 创建PG"></a>1 创建PG</h2><p>OSDMonitor收到存储池创建命令之后，最终通过PGMonitor异步向每个OSD下发批量创建PG命令。创建PG是在Primary主导下进行的。Replica会在随后由Primary发起的Peering过程中自动被创建。</p><h2 id="2-Peering"><a href="#2-Peering" class="headerlink" title="2 Peering"></a>2 Peering</h2><p>所有需要执行Peering 的PG也会专门安排一个peering_wq工作队列，当PG从peering_wq出列时：</p><ol><li>创建一个RecoveryCtx，用于批量处理所有PG与Peering相关的消息，例如Query(Log, Info等)，Notify等。</li><li>逐个PG处理：取OSD最新的OSDMap，通过advance_pg检查PG是否需要执行OSDMap更新操作。如果为否，说明直接由Peering事件触发，将该事件从PG内部的peering_queue出列，投递到PG内部的状态机进行处理；如果为是，则在advance_pg内部执行OSDMap更新操作，完成之后再将PG再次加入peering_wq队列。</li><li>检查是否需要通知Monitor设置本OSD的<code>up_thru</code>（我们规定PG在切换至新的Interval之后，成功完成Peering并重新开始接受客户端读写请求之前，必须先通知OSDMonitor设置其归属的up_thru参数）.</li><li>批量派发RecoveryCtx中积累的Query\Notify消息。</li></ol><p>下面是几个必须的操作，包括GetInfo, GetLog, GetMissing和Activate</p><p>GetInfo：获取PG元数据信息。</p><p>GetLog：开始着手进行日志同步。按照以下原则：</p><ul><li>优先选取具有最新内容的日志（即Info中的<code>last_update</code>最大）；</li><li>如果有多份满足1）的日志，优先选择保存更多日志条目的日志，即Info中<code>log_tail</code>最小；</li><li>如果有多份满足2）的日志，优先选择当前的Primary。</li></ul><p>GetMissing</p><p>Missing列表记录了自身所有需要通过Recovery进行修复的对象信息。只保留两个：</p><ul><li><code>need</code>：对象被同步的目标版本号。</li><li><code>have</code>：对象当前归属PG实例的本地版本号。</li></ul><p>当Primary收到每个Peer的本地日志之后，可以通过日志合并的方式得到每个Peer的missing列表，这一过程是通过解决日志分歧得到的。</p><p>为解决日志分歧，我们先将所有日志按照对象进行分类——即所有针对同一个对象操作的分歧日志都使用同一个队列进行管理，然后逐个队列进行管理。我们假定最老的那条分歧日志生效之前对应的版本号为prior_version，则针对每个队列的处理 都可以归结为以下五种情形：</p><ul><li>本地存在比分歧日志更新的日志。</li><li>对象此前不存在。此时可以直接删除对象。</li><li>对象当前位于missing列表之中（例如上一次peering完成之后，Primary刚更新了自己的missing列表，但是其中的对象还没来得及修复，系统发生断电）。</li><li>对象不在missing列表之中同时所有分歧日志都可以回滚。此时将所有分歧日志按照从新到老的顺序依次进行回滚。</li><li>对象不在missing列表之中并且至少存在一条分歧日志不可回滚。此时将本地对象直接删除，将其加入missing列表，同时设置其need为prior_version，have为0.</li></ul><p>Activate</p><p>在PG正式变为Active状态接受客户端请求之前，还必须先固化本次Peering的结果（也就是写入磁盘，开机bootstrap），遇到系统掉电时不会前功尽弃；同时需要初始化后续在后台执行的Recovery或者Backfill所依赖的元数据信息。以上过程便是Activate。</p><p>下面我们重点对两个元数据进行分析：</p><ul><li><code>last_epoch_started</code></li></ul><p>它本来用于指示上一次peering成功时完成的epoch，但是因为peering涉及在多个osd之间进行数据和状态同步，所以存在进度不一致的可能。 为此我们设计两个<code>last_epoch_started</code>，一个用于标识每个参与本次Peering的PG实例本地Activate已经完成，直接作为本身Info的子属性存盘；另一个保存在Info的History属性下，由Primary在检测到所有副本的Activate过程都完成后统一更新和存盘。</p><ul><li><code>MissingLoc</code></li></ul><p>在进行Recovery之前我们需要先引入一种同时包含所有missing条目和它们（目标版本）所在位置信息的全局数据结构，称为<code>MissingLoc</code>，它包含两个子表，分别是<code>needs_recovery_map</code>和<code>missing_loc</code>.它们分别保存当前PG的所有待修复对象，以及这些对象的目标版本可能同时存在于多个PG实例之上。因为目标版本可能位于多个PG实例之上，注意<code>missing_loc</code>不是一个PG而是一些PG的集合。后者由Primary统一生成。</p><p>生成<code>missing_loc</code>需要两步：首先，将所有的Peer missing列表之中的条目依次加入到needs_recovery_map之中；其次，以每个Peeri的Info和missing列表作为输入，针对<code>needs_recovery_map</code>中的每个对象逐一进行检查，以进一步确认其目标版本的位置信息并填充<code>missing_loc</code>.</p><p>成功生成<code>MissingLoc</code>之后，如果<code>needs_recovery_map</code>不为空，即存在任何需要被Recovery的对象，则Primary设置自身状态为<strong>Degraded+Activating</strong>；进一步，如果Primary检测到当前的Acting Set小于存储池副本数，则同时设置为<strong>Undersized</strong>状态。之后，Primary通过本地OS接口开始固化Peering结构；当Primary检测自身以及所有Peer的Activate操作都完成时，通过向状态机投递一个<code>AllReplicasActivated</code>事件来清除自身的Activating状态和Creating状态。同时检测PG此时Acting Set是否小于存储池最小副本数，如果小于，则设置PG状态为<strong>Peered</strong>并终止后续处理，否则将PG设置为Active，同时将之前来自客户端被阻塞的op重新入列。</p><p>随后PG进入<strong>Active</strong>状态，可以正常执行客户端的读写请求。</p><h2 id="3-Recovery"><a href="#3-Recovery" class="headerlink" title="3 Recovery"></a>3 Recovery</h2><p>Recovery是在Primary检测到自身或者任意一个peer存在待修复的对象进行的操作。为了防止集群中大量PG同时执行Recovery造成客户端响应速度过慢，需要限制Recovery。我们有几种配置项可供修改：</p><ul><li><code>osd_max_bakfills</code>: 单个OSD运行同时执行Recovery或者Backfill的PG实例个数。 注意虽然单个PG的Recovery或者Backfill不能并发，但是不同PG的Recovery和Backfill可以并发。</li><li><code>osd_max_push_cost/osd_max_push_objects</code>:指示通过Push操作执行Recovery时，以OSD为单位，单个op所能携带的字节数，对象数。</li><li><code>osd_recovery_max_active</code>: 单个OSD允许同时执行Recovery的对象数。</li><li><code>osd_recovery_op_priority</code>: 指示Recovery op的默认优先级，它将与客户端op进行竞争，优先级设置越低，竞争劣势更大。</li><li><code>osd_recovery_sleep</code>: Recovery op每次在<code>op_shardedwq</code>中被处理前，设置此参数将导致对应的服务线程先休眠对应的时间。</li></ul><p>不难看出，考虑到集群总IOPS和带宽有限，可以通过降低Recovery权重，或者通过QoS对Recovery总的IOPS和带宽加以限制，可以有效抑制Recovery对资源的消耗。</p><p>Recovery有以下两种方式：</p><ol><li><code>pull</code>: 指Primary自身存在待修复对象，由Primary按照<code>missing_loc</code>选择合适的副本去拉取待修复对象目标版本到本地，完成修复。</li><li><code>push</code>: 指Primary感知到一个或者多个Replica当前存在待修复对象，主动推送每个待修复对象目标版本至相应的Replica，然后在本地完成修复。</li></ol><p>Primary必须先完成自我修复才能修复别的Replica。它是基于日志进行的：</p><ul><li>日志中的<code>last_requested</code>用于指示Recovery的起始版本号，在Activate中生成。因此我们首先将所有待修复对象按照日志版本号进行顺序排列，找到版本号不小于<code>last_requested</code>的第一个对象，记为v；</li><li>如果不为head对象，那么检查是否需要优先修复head对象后者snapdir对象；</li><li>根据具体的PGBackend生成一个Pull类型的op；</li><li>更新last_requested，使其指向v；</li><li>如果尚未达到单次最大修复数码，则从顺序队列中处理下一个待修复对象；否则返回。</li></ul><p>以多副本为例子，因为PG日志中并未记录任何关于修改的详细信息，目前都是通过简单的全对象拷贝，因而效率低下，这也是Ceph为人所诟病的地方。</p><h2 id="4-Backfill"><a href="#4-Backfill" class="headerlink" title="4 Backfill"></a>4 Backfill</h2><p>Backfill的理论依据是“PG中所有对象可以基于全精度哈希排序”，所以是按照从小到大对当前对象进行遍历，并依次将它们按照全对象拷贝的方式写入待Backfill的PG实例。</p><p>当空的PG Temp在新的OSDMap生效之后，PG关联的Acting Set和Up Set重新变得一致，再次经过Peering之后，PG最终进入<strong>Active+Clean</strong>状态，此时PG一切恢复正常，可以删除不必要的副本（Stray）。</p><h1 id="4-总结"><a href="#4-总结" class="headerlink" title="4 总结"></a>4 总结</h1><p>PG的主要定位如下：</p><ul><li>作为存储池的基本组成单位，负责执行存储池所绑定的副本策略。</li><li>以OSD作为单位，进行副本分布，将前端应用任何针对PG中原始对象的操作，转化为OSD所能理解的事务操作，并保证副本之间的强一致性。</li></ul><p>不足之处在于：因为PG日志中并未记录任何关于修改的详细信息，目前都是通过简单的全对象拷贝，因而效率低下，这也是Ceph为人所诟病的地方。</p><hr><h1 id="——"><a href="#——" class="headerlink" title="——"></a>——</h1><h1 id="实践部分"><a href="#实践部分" class="headerlink" title="实践部分"></a>实践部分</h1><h2 id="0-官方的指导和讲解"><a href="#0-官方的指导和讲解" class="headerlink" title="0 官方的指导和讲解"></a>0 官方的指导和讲解</h2><p>数据的持久性以及所有OSD之间的均匀分配都需要更多的放置组，但应将其数量减少到最少，以节省CPU和内存。</p><h3 id="0-1-数据持久性"><a href="#0-1-数据持久性" class="headerlink" title="0.1 数据持久性"></a>0.1 数据持久性</h3><p>OSD发生故障后，数据丢失的风险会增加，直到完全恢复其中包含的数据为止。让我们想象一下在单个放置组中导致永久性数据丢失的情况：</p><ul><li>OSD失败，并且它包含的对象的所有副本均丢失。对于放置组中的所有对象，副本的数量突然从三个减少到两个。</li><li>Ceph通过选择一个新的OSD来重新创建所有对象的第三个副本，从而开始对该放置组的恢复。</li><li>在同一放置组内的另一个OSD在新OSD完全填充第三份副本之前发生故障。某些对象将只有一个幸存副本。</li><li>Ceph选择了另一个OSD并保持复制对象以恢复所需的副本数。</li><li>在同一放置组内的第三个OSD在恢复完成之前发生故障。如果此OSD包含对象的唯一剩余副本，则它将永久丢失。</li></ul><p>在三个副本池中包含10个OSD和512个放置组的群集中，CRUSH将为每个放置组提供三个OSD。最后，每个OSD将托管（512 * 3）/ 10 =〜150个放置组。当第一个OSD发生故障时，以上情形将因此同时开始恢复所有150个放置组的操作。</p><p>恢复的150个放置组可能均匀分布在剩余的9个OSD上。因此，每个剩余的OSD都有可能将对象的副本发送给所有其他对象，并且还可能接收一些要存储的新对象，因为它们已成为新放置组的一部分。</p><p>完成恢复所需的时间完全取决于Ceph集群的架构。假设每个OSD由一台机器上的1TB SSD托管，并且所有OSD都连接到10Gb / s交换机，并且单个OSD的恢复将在M分钟内完成。如果每台计算机使用不带SSD日志的微调器和1Gb / s开关的两个OSD，则速度至少要慢一个数量级。</p><p>在这种大小的群集中，放置组的数量几乎对数据持久性没有影响。可能是128或8192，恢复速度不会变慢或变快。</p><p><strong>但是，将相同的Ceph群集增加到20个OSD而不是10个OSD可能会加快恢复速度，从而显着提高数据的持久性</strong>。现在，每个OSD只能参与约75个放置组，而不是只有10个OSD时的约150个放置组，并且仍然需要全部19个剩余OSD执行相同数量的对象副本才能恢复。但是，如果10个OSD必须每个复制大约100GB，则现在它们必须每个复制50GB。如果网络是瓶颈，恢复将以两倍的速度进行。换句话说，当OSD数量增加时，恢复速度会更快。</p><p>如果该群集增长到40个OSD，则每个OSD将仅托管约35个放置组。如果OSD死亡，则恢复将保持更快的速度，除非它被另一个瓶颈阻止。但是，如果该群集增长到200个OSD，则每个OSD将仅托管约7个放置组。如果OSD死亡，则在这些放置组中最多将有约21（7 * 3）个OSD之间发生恢复：恢复将比有40个OSD时花费更长的时间，这意味着应该增加放置组的数量。</p><p>无论恢复时间有多短，第二个OSD在进行过程中都有可能发生故障。在上述10个OSD集群中，如果其中任何一个失败，则〜17个放置组（即，已恢复的〜150/9个放置组）将只有一个幸存副本。并且，如果剩余的8个OSD中的任何一个失败，则两个放置组的最后一个对象很可能会丢失（即，〜17/8个放置组，仅恢复了一个剩余副本）。</p><p>当群集的大小增加到20个OSD时，丢失三个OSD会损坏的放置组的数量会减少。第二个OSD丢失将降低〜4个（即，恢复到约75个/ 19个放置组），而不是〜17个，而第三个OSD丢失则仅在它是包含尚存副本的四个OSD之一时才丢失数据。换句话说，如果在恢复时间范围内丢失一个OSD的概率为0.0001％，则它从具有10个OSD的群集中的17 <em> 10 </em> 0.0001％变为具有20个OSD的群集中的4 <em> 20 </em> 0.0001％。</p><p><strong><u>简而言之，更多OSD意味着更快的恢复速度和更低的导致安置组的永久损失的级联故障风险。就数据持久性而言，在少于50个OSD的群集中，具有512或4096个放置组大致等效。</u></strong></p><p>注意：添加到群集中的新OSD可能需要很长时间才能分配有分配给它的放置组。但是，不会降低任何对象的质量，也不会影响群集中包含的数据的持久性。</p><h3 id="0-2-池中的对象分布"><a href="#0-2-池中的对象分布" class="headerlink" title="0.2 池中的对象分布"></a>0.2 池中的对象分布</h3><p>理想情况下，对象在每个放置组中均匀分布。由于CRUSH计算每个对象的放置组，但实际上不知道该放置组内每个OSD中存储了多少数据，因此放置组数与OSD数之比可能会显着影响数据的分布。</p><p>例如，如果在三个副本池中有一个用于十个OSD的放置组，则仅使用三个OSD，因为CRUSH别无选择。当有更多的放置组可用时，对象更有可能在其中均匀分布。CRUSH还尽一切努力在所有现有的放置组中平均分配OSD。</p><p>只要放置组比OSD多一个或两个数量级，则分布应该均匀。例如，用于3个OSD的256个放置组，用于10个OSD的512或1024个放置组等。</p><p>数据分布不均可能是由OSD与放置组之间的比率以外的因素引起的。由于CRUSH没有考虑对象的大小，因此一些非常大的对象可能会造成不平衡。假设有100万个4K对象（共4GB）均匀分布在10个OSD的1024个放置组中。他们将在每个OSD上使用4GB / 10 = 400MB。如果将一个400MB对象添加到池中，则支持放置对象的放置组的三个OSD将填充400MB + 400MB = 800MB，而其余七个将仅占据400MB。</p><h3 id="0-3-内存，CPU和网络使用率"><a href="#0-3-内存，CPU和网络使用率" class="headerlink" title="0.3 内存，CPU和网络使用率"></a>0.3 内存，CPU和网络使用率</h3><p>对于每个放置组，OSD和MON始终需要内存，网络和CPU，并且在恢复期间甚至更多。通过对放置组内的对象进行聚类来共享此开销是它们存在的主要原因之一。</p><p><strong><u>最小化放置组的数量可以节省大量资源。</u></strong></p><h3 id="0-4-选择放置组的数量"><a href="#0-4-选择放置组的数量" class="headerlink" title="0.4 选择放置组的数量"></a>0.4 选择放置组的数量</h3><p><strong><u>如果您有超过50个OSD，我们建议每个OSD大约有50-100个放置组</u></strong>，以平衡资源使用，数据持久性和分发。如果OSD少于50个，则最好在上述<a href="https://docs.ceph.com/en/latest/rados/operations/placement-groups/?#preselection">预选</a>中进行<a href="https://docs.ceph.com/en/latest/rados/operations/placement-groups/?#preselection">选择</a>。对于单个对象池，您可以使用以下公式获取基准</p><script type="math/tex; mode=display">Total \ PGs = \frac{OSDs*100}{pool\_size}</script><p>$pool_size$在副本池表示副本数，而在纠删码池表示$K+M$</p><p>然后，您应该检查结果是否与您设计Ceph集群的方式有意义，以最大程度地提高<a href="https://docs.ceph.com/en/latest/rados/operations/placement-groups/?#data-durability">数据持久性</a>， <a href="https://docs.ceph.com/en/latest/rados/operations/placement-groups/?#object-distribution">对象分配</a>并最小化<a href="https://docs.ceph.com/en/latest/rados/operations/placement-groups/?#resource-usage">资源使用</a>。</p><p>结果应始终<strong>四舍五入到最接近的2的幂</strong>。</p><p>只有2的幂可以平衡放置组中的对象数量。其他值将导致OSD上的数据分布不均。它们的使用应仅限于从两个方的一种逐步增加到另一种。</p><p>例如，对于具有200个OSD和3个副本的池大小的群集，您可以如下估算PG的数量</p><script type="math/tex; mode=display">\frac{200 \times 100}{3} = 6667</script><p>最近的2的次幂是8192.</p><p>当使用多个数据池存储对象时，您需要确保在每个池的放置组数量与每个OSD的放置组数量之间取得平衡，以便获得合理的放置组总数，从而使每个OSD的方差很小而不会增加系统资源的负担或使对等进程太慢。</p><p>例如，一个由10个池组成的群集，每个池在10个OSD上具有512个放置组，则总共有5120个放置组分布在10个OSD上，即每个OSD 512个放置组。那不会使用太多资源。但是，如果创建了1,000个池，每个池有512个放置组，则OSD将分别处理约50,000个放置组，并且将需要更多的资源和时间来进行对等。</p><p>您可能会发现<a href="http://ceph.com/pgcalc/">PGCalc</a>工具很有帮助。这是一个很有意思的工具。</p><blockquote><p><strong>建议的PG计数背后的逻辑</strong></p><p>( Target PGs per OSD ) x ( OSD # ) x ( %Data )/ ( Size )</p><ol><li>如果以上计算的值小于<strong>（OSD＃）/（Size）</strong>的值，则该值将更新为<strong>（OSD＃）/（Size）的值</strong>。这是通过为每个池向每个OSD分配至少一个主PG或辅助PG来确保均匀的负载/数据分配。</li><li>然后将输出值舍入到<strong>最接近的2的幂</strong>。<br><strong>提示：</strong>最接近的2的幂提供了<a href="http://ceph.com/docs/master/rados/operations/crush-map/">CRUSH</a>算法效率的少量提高。</li><li>如果最接近的2的幂比原始值低<strong>25％</strong>以上，则使用下一个更高的2的幂。</li></ol><p><strong>目的</strong></p><ul><li>此计算的目的和上面“关键”部分所述的目标范围是为了确保有足够的放置组，以便在整个群集中进行均匀的数据分布，同时每个OSD PG的PG值不够高，从而在恢复期间引起问题和/或回填操作。</li></ul><p><strong>无效或无效池的影响：</strong></p><ul><li>空池或其他非活动池不应被认为有助于整个群集中的数据均匀分布。</li><li>但是，与这些空/非活动池关联的PG仍会消耗内存和CPU开销。</li></ul></blockquote><hr><h2 id="1-设置放置组数"><a href="#1-设置放置组数" class="headerlink" title="1 设置放置组数"></a>1 设置放置组数</h2><p>要设置池中的放置组数量，必须在创建池时指定放置组的数量。有关详细信息，请参见<a href="https://docs.ceph.com/en/latest/rados/operations/pools#createpool">创建池</a>。即使在创建池之后，您也可以使用以下方法更改放置组的数量：</p><pre><code class="hljs pgsql">ceph osd pool <span class="hljs-keyword">set</span> &#123;pool-<span class="hljs-type">name</span>&#125; pg_num &#123;pg_num&#125;</code></pre><p>增加放置组的数量之后，还必须增加放置（<code>pgp_num</code>）的放置组的数量，群集才能重新平衡。该<code>pgp_num</code>会是将由CRUSH算法可考虑放置位置的组数。增加会<code>pg_num</code>拆分放置组，但数据将不会迁移到较新的放置组，直到放置的放置组，即<code>pgp_num</code>增加的值<code>pgp_num</code> 应等于<code>pg_num</code>。要增加用于放置的放置组的数量，请执行以下操作：</p><pre><code class="hljs routeros">ceph osd<span class="hljs-built_in"> pool </span><span class="hljs-builtin-name">set</span> &#123;pool-name&#125; pgp_num &#123;pgp_num&#125;</code></pre><p>减少PG数量时，<code>pgp_num</code>将自动为您调整。</p><div class="note note-primary">            <p>笔者注：</p><p>关于PG和PGP：</p><ul><li><p>PG =放置组( Placement Group)<br>PGP =用于放置的放置组(Placement Group for Placement purpose)</p><p>pg_num = 映射到OSD的PG的数量，它必须是2的幂次。</p><p>当对任何一个池增加pg_num时，该池的每个PG都会<strong>分裂</strong>成一半，但它们都将始终映射到其父OSD。</p><p>在此之前，Ceph不会开始重新平衡。 现在，当您为同一池增加pgp_num值时，PG开始从父级迁移到其他OSD，并且群集重新平衡开始。 这就是PGP扮演重要角色的方式。</p></li></ul>          </div><h2 id="2-获取放置组的数量"><a href="#2-获取放置组的数量" class="headerlink" title="2 获取放置组的数量"></a>2 获取放置组的数量</h2><p>要获取池中的放置组数，请执行以下操作：</p><pre><code class="hljs pgsql">ceph osd pool <span class="hljs-keyword">get</span> &#123;pool-<span class="hljs-type">name</span>&#125; pg_num</code></pre><h2 id="3-Auto-scaling"><a href="#3-Auto-scaling" class="headerlink" title="3 Auto_scaling"></a>3 Auto_scaling</h2><h3 id="自动调整"><a href="#自动调整" class="headerlink" title="自动调整"></a>自动调整</h3><p>这是一种自动调整PG的方式。有三个参数<code>off</code>, <code>on</code>,<code>warn</code>. 当设置为off就需要人为控制PG数目。</p><p>要为现有池设置自动缩放模式，请执行以下操作：</p><pre><code class="hljs pgsql">ceph osd pool <span class="hljs-keyword">set</span> &lt;pool-<span class="hljs-type">name</span>&gt; pg_autoscale_mode &lt;mode&gt;</code></pre><p>例如，要在pool上启用自动缩放<code>foo</code>，请执行以下操作：</p><pre><code class="hljs pgsql">ceph osd pool <span class="hljs-keyword">set</span> foo pg_autoscale_mode <span class="hljs-keyword">on</span></code></pre><p>您还可以使用以下命令配置<code>pg_autoscale_mode</code>应用于以后创建的任何池的默认值：</p><pre><code class="hljs routeros">ceph<span class="hljs-built_in"> config </span><span class="hljs-builtin-name">set</span> global osd_pool_default_pg_autoscale_mode &lt;mode&gt;</code></pre><h2 id="4-Autoscale-status"><a href="#4-Autoscale-status" class="headerlink" title="4 Autoscale_status"></a>4 Autoscale_status</h2><h3 id="查看PG缩放建议"><a href="#查看PG缩放建议" class="headerlink" title="查看PG缩放建议"></a>查看PG缩放建议</h3><p>您可以使用以下命令查看每个池，池的相对利用率以及对PG计数的任何建议更改：</p><pre><code class="hljs routeros">ceph osd<span class="hljs-built_in"> pool </span>autoscale-status</code></pre><p>输出将类似于：</p><pre><code class="hljs apache"><span class="hljs-attribute">POOL</span>    SIZE  TARGET SIZE  RATE  RAW CAPACITY   RATIO  TARGET RATIO  EFFECTIVE RATIO PG_NUM  NEW PG_NUM  AUTOSCALE<span class="hljs-attribute">a</span>     <span class="hljs-number">12900</span>M                <span class="hljs-number">3</span>.<span class="hljs-number">0</span>        <span class="hljs-number">82431</span>M  <span class="hljs-number">0</span>.<span class="hljs-number">4695</span>                                     <span class="hljs-number">8</span>         <span class="hljs-number">128</span>  warn<span class="hljs-attribute">c</span>         <span class="hljs-number">0</span>                 <span class="hljs-number">3</span>.<span class="hljs-number">0</span>        <span class="hljs-number">82431</span>M  <span class="hljs-number">0</span>.<span class="hljs-number">0000</span>        <span class="hljs-number">0</span>.<span class="hljs-number">2000</span>           <span class="hljs-number">0</span>.<span class="hljs-number">9884</span>      <span class="hljs-number">1</span>          <span class="hljs-number">64</span>  warn<span class="hljs-attribute">b</span>         <span class="hljs-number">0</span>        <span class="hljs-number">953</span>.<span class="hljs-number">6</span>M   <span class="hljs-number">3</span>.<span class="hljs-number">0</span>        <span class="hljs-number">82431</span>M  <span class="hljs-number">0</span>.<span class="hljs-number">0347</span>                                     <span class="hljs-number">8</span>              warn</code></pre><p><strong>SIZE</strong>是存储在池中的数据量。<strong>TARGET SIZE</strong>（如果存在）是管理员指定的数据量，管理员希望最终将其存储在此池中。系统使用两个值中的较大者进行计算。</p><p><strong>RATE</strong>是池的乘数，它确定要消耗多少原始存储容量。例如，3个副本池的比率为3.0，而k = 4，m = 2纠删码池的比率为1.5。</p><p><strong>RAW CAPACITY</strong>是OSD上负责存储此池（可能还有其他池）数据的原始存储容量的总量。 <strong>比率</strong>是该池消耗的总容量的比率（即比率=大小*比率/原始容量）。</p><p><strong>TARGET RATIO</strong>（如果存在）是管理员已指定他们希望该池相对于设置了目标比率的其他池消耗的存储比率。如果同时指定了目标大小字节和比率，则比率优先。</p><p><strong>EFFECTIVE RATIO</strong>是通过两种方式进行调整后的目标比率：</p><ol><li>减去设置了目标大小的池预期使用的任何容量</li><li>设定目标比率后，对池中的目标比率进行标准化，以便它们共同针对其余空间。例如，target_ratio 1.0的4个池的有效比率为0.25。</li></ol><p>系统使用实际比率和有效比率中的较大者进行计算。</p><p><strong>PG_NUM</strong>是该池的当前PG数量（如果<code>pg_num</code> 正在进行更改，则为该池正在使用的PG的当前数量）。 系统认为应该将<code>pg_num</code>更改为<strong>NEW PG_NUM</strong>。它始终是2的幂，并且仅在“理想”值与当前值的差异大于3时才存在。</p><p>最后一列，<strong>AUTOSCALE</strong>，是池<code>pg_autoscale_mode</code>，并将于要么<code>on</code>，<code>off</code>或<code>warn</code>。</p><h2 id="5-Automated-scaling"><a href="#5-Automated-scaling" class="headerlink" title="5 Automated_scaling"></a>5 Automated_scaling</h2><h3 id="自动缩放"><a href="#自动缩放" class="headerlink" title="自动缩放"></a>自动缩放</h3><p>最简单的方法是允许群集根据使用情况自动扩展PG。Ceph将查看整个系统的PG的总可用存储量和目标数量，查看每个池中存储了多少数据，并尝试相应地分配PG。该系统的方法相对保守，仅当当前PG（<code>pg_num</code>）数量比其认为的数量多3倍时才对池进行更改。</p><p>每个OSD的PG的目标数量基于可 <code>mon_target_pg_per_osd</code>配置（默认值：100），可以通过以下方式进行调整：</p><pre><code class="hljs routeros">ceph<span class="hljs-built_in"> config </span><span class="hljs-builtin-name">set</span> global mon_target_pg_per_osd 100</code></pre><p>自动缩放器将分析池并在每个子树的基础上进行调整。因为每个池可能映射到不同的CRUSH规则，并且每个规则可能在不同的设备之间分配数据，所以Ceph将考虑独立使用层次结构的每个子树。例如，映射到ssd类的OSD的池和映射到hdd类的OSD的池将分别具有最佳PG计数，这取决于这些相应设备类型的数量。</p><h2 id="6-指定期望池大小"><a href="#6-指定期望池大小" class="headerlink" title="6 指定期望池大小"></a>6 指定期望池大小</h2><p>首次创建集群或池时，它将消耗集群总容量的一小部分，并且在系统中似乎只需要少量的放置组。但是，在大多数情况下，群集管理员会很好地知道哪些池会随着时间消耗掉大部分系统容量。通过将此信息提供给Ceph，可以从一开始就使用更合适数量的PG，从而避免进行后续调整 <code>pg_num</code>以及在进行这些调整时与移动数据相关的开销。</p><p>池的<em>目标大小</em>可以通过两种方式指定：要么以池的绝对大小（即字节）为单位，要么以相对于具有一<code>target_size_ratio</code>组其他池的权重为单位。</p><p>例如，：</p><pre><code class="hljs routeros">ceph osd<span class="hljs-built_in"> pool </span><span class="hljs-builtin-name">set</span> mypool target_size_bytes 100T</code></pre><p>会告诉系统mypool预计会占用100 TiB的空间。或者，：</p><pre><code class="hljs routeros">ceph osd<span class="hljs-built_in"> pool </span><span class="hljs-builtin-name">set</span> mypool target_size_ratio 1.0</code></pre><p>会告诉系统mypool与<code>target_size_ratio</code>set的其他池相比预期消耗1.0 。如果mypool是群集中唯一的池，则意味着预期使用了总容量的100％。如果第二个池的<code>target_size_ratio</code> 1.0，则两个池都将使用50％的群集容量。</p><p>您还可以在创建时使用命令的可选参数<code>--target-size-bytes &lt;bytes&gt;</code>或参数<code>--target-size-ratio &lt;ratio&gt;</code>设置池的目标大小。</p><p>请注意，如果指定了不可能的目标大小值（例如，容量大于整个群集的容量），则会发出健康警告（<code>POOL_TARGET_SIZE_BYTES_OVERCOMMITTED</code>）。</p><p>如果为池指定了<code>target_size_ratio</code>和<code>target_size_bytes</code>，则仅考虑比率，并发出运行状况警告（<code>POOL_HAS_TARGET_SIZE_BYTES_AND_RATIO</code>）。</p><h2 id="7-设置PG的下界"><a href="#7-设置PG的下界" class="headerlink" title="7 设置PG的下界"></a>7 设置PG的下界</h2><p>也可以为一个池指定最小数量的PG。这对于确定执行IO时客户端将看到的并行度的数量的下限很有用，即使池中大多数都是空的。设置下限可以防止Ceph将PG编号减少（或建议减少）到配置的编号以下。</p><p>您可以使用以下方法设置池的最小PG数量：</p><pre><code class="hljs pgsql">ceph osd pool <span class="hljs-keyword">set</span> &lt;pool-<span class="hljs-type">name</span>&gt; pg_num_min &lt;num&gt;</code></pre><p>您还可以使用命令的可选参数在创建池时指定最小PG计数。<code>--pg-num-min &lt;num&gt;``ceph osd pool create</code></p><h2 id="8-获取集群的PG统计信息"><a href="#8-获取集群的PG统计信息" class="headerlink" title="8 获取集群的PG统计信息"></a>8 获取集群的PG统计信息</h2><p>要获取集群中放置组的统计信息，请执行以下操作：</p><pre><code class="hljs dos">ceph pg dump [--<span class="hljs-built_in">format</span> &#123;<span class="hljs-built_in">format</span>&#125;]</code></pre><p>有效格式为<code>plain</code>（默认）和<code>json</code>。</p><h2 id="9-获取卡住的PG的统计信息"><a href="#9-获取卡住的PG的统计信息" class="headerlink" title="9 获取卡住的PG的统计信息"></a>9 获取卡住的PG的统计信息</h2><p>要获取处于指定状态的所有放置组的统计信息，请执行以下操作：</p><pre><code class="hljs coq">ceph pg dump_stuck inactive|<span class="hljs-type">unclean</span>|<span class="hljs-type">stale</span>|<span class="hljs-type">undersized</span>|<span class="hljs-type">degraded</span> [--format &lt;format&gt;] [-t|<span class="hljs-type">--threshold</span> &lt;seconds&gt;]</code></pre><p><strong>inactive</strong>放置组无法处理读写，因为它们正在等待OSD包含最新数据。</p><p><strong>unclean</strong>放置组包含未复制所需次数的对象。他们应该正在恢复。</p><p><strong>stale</strong>放置组处于未知状态-承载它们的OSD暂时未向监视集群报告（由配置<code>mon_osd_report_timeout</code>）。</p><p>有效格式为<code>plain</code>（默认）和<code>json</code>。阈值定义了放置组停留在返回的统计信息中之前所停留的最小秒数（默认为300秒）。</p><h2 id="10-获取PG-Map"><a href="#10-获取PG-Map" class="headerlink" title="10 获取PG Map"></a>10 获取PG Map</h2><p>要获取特定放置组的放置组映射，请执行以下操作：</p><pre><code class="hljs applescript">ceph pg map &#123;pg-<span class="hljs-built_in">id</span>&#125;</code></pre><p>例如：</p><pre><code class="hljs apache"><span class="hljs-attribute">ceph</span> pg map <span class="hljs-number">1</span>.<span class="hljs-number">6</span>c</code></pre><p>Ceph将返回放置组图，放置组和OSD状态：</p><pre><code class="hljs angelscript">osdmap e13 pg <span class="hljs-number">1.6</span>c (<span class="hljs-number">1.6</span>c) -&gt; up [<span class="hljs-number">1</span>,<span class="hljs-number">0</span>] acting [<span class="hljs-number">1</span>,<span class="hljs-number">0</span>]</code></pre><p>解释一下，这里表示 pg 1.6c被映射到 编号为 [1,0]的两个OSD上。 Acting 表示Acting Set。</p><h2 id="11-获取PG统计信息"><a href="#11-获取PG统计信息" class="headerlink" title="11 获取PG统计信息"></a>11 获取PG统计信息</h2><p>要检索特定放置组的统计信息，请执行以下操作：</p><pre><code class="hljs puppet">ceph <span class="hljs-keyword">pg</span> &#123;pg-<span class="hljs-built_in">id</span>&#125; <span class="hljs-keyword">query</span></code></pre><p> 这里的query其实是一种元数据信息，部分形式如下：</p><pre><code class="hljs clojure">&#123;<span class="hljs-string">&quot;snap_trimq&quot;</span>: <span class="hljs-string">&quot;[]&quot;</span>,    <span class="hljs-string">&quot;snap_trimq_len&quot;</span>: <span class="hljs-number">0</span>,    <span class="hljs-string">&quot;state&quot;</span>: <span class="hljs-string">&quot;active+clean&quot;</span>,    <span class="hljs-string">&quot;epoch&quot;</span>: <span class="hljs-number">236</span>,    <span class="hljs-string">&quot;up&quot;</span>: [        <span class="hljs-number">1</span>,        <span class="hljs-number">2</span>,        <span class="hljs-number">0</span>    ],    <span class="hljs-string">&quot;acting&quot;</span>: [        <span class="hljs-number">1</span>,        <span class="hljs-number">2</span>,        <span class="hljs-number">0</span>    ],    <span class="hljs-string">&quot;acting_recovery_backfill&quot;</span>: [        <span class="hljs-string">&quot;0&quot;</span>,        <span class="hljs-string">&quot;1&quot;</span>,        <span class="hljs-string">&quot;2&quot;</span>    ],    <span class="hljs-string">&quot;info&quot;</span>: &#123;        <span class="hljs-string">&quot;pgid&quot;</span>: <span class="hljs-string">&quot;1.0&quot;</span>,        <span class="hljs-string">&quot;last_update&quot;</span>: <span class="hljs-string">&quot;223&#x27;23&quot;</span>,        <span class="hljs-string">&quot;last_complete&quot;</span>: <span class="hljs-string">&quot;223&#x27;23&quot;</span>,        <span class="hljs-string">&quot;log_tail&quot;</span>: <span class="hljs-string">&quot;0&#x27;0&quot;</span>,        <span class="hljs-string">&quot;last_user_version&quot;</span>: <span class="hljs-number">23</span>,        <span class="hljs-string">&quot;last_backfill&quot;</span>: <span class="hljs-string">&quot;MAX&quot;</span>,        <span class="hljs-string">&quot;purged_snaps&quot;</span>: [],        <span class="hljs-string">&quot;history&quot;</span>: &#123;            <span class="hljs-string">&quot;epoch_created&quot;</span>: <span class="hljs-number">2</span>,            <span class="hljs-string">&quot;epoch_pool_created&quot;</span>: <span class="hljs-number">2</span>,            <span class="hljs-string">&quot;last_epoch_started&quot;</span>: <span class="hljs-number">221</span>,            <span class="hljs-string">&quot;last_interval_started&quot;</span>: <span class="hljs-number">220</span>,            <span class="hljs-string">&quot;last_epoch_clean&quot;</span>: <span class="hljs-number">221</span>,            <span class="hljs-string">&quot;last_interval_clean&quot;</span>: <span class="hljs-number">220</span>,            <span class="hljs-string">&quot;last_epoch_split&quot;</span>: <span class="hljs-number">0</span>,            <span class="hljs-string">&quot;last_epoch_marked_full&quot;</span>: <span class="hljs-number">0</span>,            <span class="hljs-string">&quot;same_up_since&quot;</span>: <span class="hljs-number">220</span>,            <span class="hljs-string">&quot;same_interval_since&quot;</span>: <span class="hljs-number">220</span>,            <span class="hljs-string">&quot;same_primary_since&quot;</span>: <span class="hljs-number">212</span>,            <span class="hljs-string">&quot;last_scrub&quot;</span>: <span class="hljs-string">&quot;189&#x27;21&quot;</span>,            <span class="hljs-string">&quot;last_scrub_stamp&quot;</span>: <span class="hljs-string">&quot;2020-12-14T06:56:57.181447+0800&quot;</span>,            <span class="hljs-string">&quot;last_deep_scrub&quot;</span>: <span class="hljs-string">&quot;189&#x27;20&quot;</span>,            <span class="hljs-string">&quot;last_deep_scrub_stamp&quot;</span>: <span class="hljs-string">&quot;2020-12-13T04:09:17.431508+0800&quot;</span>,            <span class="hljs-string">&quot;last_clean_scrub_stamp&quot;</span>: <span class="hljs-string">&quot;2020-12-14T06:56:57.181447+0800&quot;</span>,            <span class="hljs-string">&quot;prior_readable_until_ub&quot;</span>: <span class="hljs-number">0</span>        &#125;,...&#125;</code></pre><p>我们可以看到很多理论部分讲过的元数据，比如 up 、acting、info、epoch、peer、interval等。<code>snap_trimq</code>表示快照删除队列。</p><p>当前的版本号为236.</p><h2 id="12-Scrub一个放置组"><a href="#12-Scrub一个放置组" class="headerlink" title="12 Scrub一个放置组"></a>12 Scrub一个放置组</h2><p>关于Scrub的含义，我们在《Ceph纠删码部署》已经介绍了，Scrub指数据扫描，通过读取对象数据并重新计算校验和，再与之前存储在对象属性的校验和进行比对，以判断有无静默错误（磁盘自身无法感知的错误）。要Scrub，请执行以下操作：</p><pre><code class="hljs pf">ceph pg <span class="hljs-keyword">scrub</span> &#123;pg-id&#125;</code></pre><p>Ceph检查主节点和任何副本节点，生成放置组中所有对象的目录并进行比较，以确保没有丢失或不匹配的对象，并且它们的内容一致。假设所有副本都匹配，则最终的语义扫描可确保所有与快照相关的对象元数据都是一致的。通过日志报告错误。</p><p>要从特定池中清理所有放置组，请执行以下操作：</p><pre><code class="hljs routeros">ceph osd<span class="hljs-built_in"> pool </span>scrub &#123;pool-name&#125;</code></pre><h2 id="12-设置PG-Backfill-Recovery的优先级"><a href="#12-设置PG-Backfill-Recovery的优先级" class="headerlink" title="12 设置PG Backfill/Recovery的优先级"></a>12 设置PG Backfill/Recovery的优先级</h2><p>请注意，这些命令可能会破坏Ceph内部优先级计算的顺序，因此请谨慎使用！特别是，如果您有多个当前共享相同底层OSD的池，并且某些特定的池比其他池更重要，则建议您使用以下命令以更好的顺序重新排列所有池的恢复/回填优先级：</p><pre><code class="hljs routeros">ceph osd<span class="hljs-built_in"> pool </span><span class="hljs-builtin-name">set</span> &#123;pool-name&#125; recovery_priority &#123;value&#125;</code></pre><p>例如，如果您有10个池，则可以将最重要的一个优先级设置为10，下一个9，等等。或者您可以不理会大多数池，而说3个重要的池分别设置为优先级1或优先级3、2、1。</p><p>在恢复或者回填比用户op的优先级更高的时候。我们可以执行：</p><pre><code class="hljs applescript">ceph pg force-recovery &#123;pg-<span class="hljs-built_in">id</span>&#125; [&#123;pg-<span class="hljs-built_in">id</span> <span class="hljs-comment">#2&#125;] [&#123;pg-id #3&#125; ...]</span>ceph pg force-backfill &#123;pg-<span class="hljs-built_in">id</span>&#125; [&#123;pg-<span class="hljs-built_in">id</span> <span class="hljs-comment">#2&#125;] [&#123;pg-id #3&#125; ...]</span></code></pre><p>如果您认为这是一个不好的决定，请使用：</p><pre><code class="hljs applescript">ceph pg cancel-force-recovery &#123;pg-<span class="hljs-built_in">id</span>&#125; [&#123;pg-<span class="hljs-built_in">id</span> <span class="hljs-comment">#2&#125;] [&#123;pg-id #3&#125; ...]</span>ceph pg cancel-force-backfill &#123;pg-<span class="hljs-built_in">id</span>&#125; [&#123;pg-<span class="hljs-built_in">id</span> <span class="hljs-comment">#2&#125;] [&#123;pg-id #3&#125; ...]</span></code></pre><p>这将从这些PG中删除“ force”标志，并将以默认顺序对其进行处理。同样，这不会影响当前正在处理的放置组，只会影响仍在排队的放置组。</p><p>恢复或回填组后，将自动清除“ force”标志。</p><p>同样，您可以使用以下命令强制Ceph首先对指定池中的所有放置组执行恢复或回填：</p><pre><code class="hljs routeros">ceph osd<span class="hljs-built_in"> pool </span>force-recovery &#123;pool-name&#125;ceph osd<span class="hljs-built_in"> pool </span>force-backfill &#123;pool-name&#125;</code></pre><p>要么：</p><pre><code class="hljs routeros">ceph osd<span class="hljs-built_in"> pool </span>cancel-force-recovery &#123;pool-name&#125;ceph osd<span class="hljs-built_in"> pool </span>cancel-force-backfill &#123;pool-name&#125;</code></pre><p>如果您改变主意，则可以恢复到默认的恢复或回填优先级。</p><h2 id="13-还原丢失"><a href="#13-还原丢失" class="headerlink" title="13 还原丢失"></a>13 还原丢失</h2><p>如果群集丢失了一个或多个对象，并且您决定放弃对丢失数据的搜索，则必须将未找到的对象标记为<code>lost</code>。</p><p>如果已查询所有可能的位置并且仍然丢失了对象，则可能必须放弃丢失的对象。鉴于异常的异常组合使集群能够了解恢复写本身之前执行的写，这是可能的。</p><p>当前唯一受支持的选项是“还原”，它可以回滚到该对象的先前版本，或者（如果是新对象）则完全忘记它。要将“未找到”的对象标记为“丢失”，请执行以下操作：</p><pre><code class="hljs puppet">ceph <span class="hljs-keyword">pg</span> &#123;pg-<span class="hljs-built_in">id</span>&#125; <span class="hljs-keyword">mark_unfound_lost</span> <span class="hljs-keyword">revert</span>|delete</code></pre><div class="note note-danger">            <p>重要：</p><p>请谨慎使用此功能，因为它可能会使期望对象存在的应用程序感到困惑(confused)。</p>          </div><hr><p>EOF</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;迁移之美——PG读写流程与状态迁移详解&quot;&gt;&lt;a href=&quot;#迁移之美——PG读写流程与状态迁移详解&quot; class=&quot;headerlink&quot; title=&quot;迁移之美——PG读写流程与状态迁移详解&quot;&gt;&lt;/a&gt;迁移之美——PG读写流程与状态迁移详解&lt;/h1&gt;&lt;div </summary>
      
    
    
    
    <category term="ceph" scheme="http://durantthorvalds.top/categories/ceph/"/>
    
    <category term="PG" scheme="http://durantthorvalds.top/categories/ceph/PG/"/>
    
    
    <category term="Ceph理论" scheme="http://durantthorvalds.top/tags/Ceph%E7%90%86%E8%AE%BA/"/>
    
  </entry>
  
  <entry>
    <title>距离向量路由选择算法——DV</title>
    <link href="http://durantthorvalds.top/2020/12/15/%E8%B7%9D%E7%A6%BB%E5%90%91%E9%87%8F%E8%B7%AF%E7%94%B1%E9%80%89%E6%8B%A9%E7%AE%97%E6%B3%95DV/"/>
    <id>http://durantthorvalds.top/2020/12/15/%E8%B7%9D%E7%A6%BB%E5%90%91%E9%87%8F%E8%B7%AF%E7%94%B1%E9%80%89%E6%8B%A9%E7%AE%97%E6%B3%95DV/</id>
    <published>2020-12-14T16:00:00.000Z</published>
    <updated>2020-12-16T12:56:51.068Z</updated>
    
    <content type="html"><![CDATA[<h1 id="DV"><a href="#DV" class="headerlink" title="DV"></a><em>DV</em></h1><p>获取全局的路由信息是代价很高的，我们需要一种分布式的、异步的、迭代的、自我终止的算法，这便是DV（$Distance  vector$），它是路由器中真正运行的算法，直到现在！</p><p>这儿的自我终止指：即没有计算应该停止的信号，它就停止了。</p><p>我们首先讨论最低路径开销之间的一种重要关系。令$d_x(y)$是从节点x到y的最低开销，由著名的Bellman-Ford方程：</p><script type="math/tex; mode=display">d_x(y)=min_v\{c(x,v)+d_v(y)\}</script><p>$c(x,y)$表示x与y之间的边的开销。这正是Dijkstra算法的核心思路。</p><p>我们令$D_x(y)$为节点x到邻近的每一个节点y的<strong>距离向量</strong>。</p><p>每个节点不时的向每个邻居发送它的距离向量副本。当节点x从它任何一个邻居v收到一个新的距离向量，它就保存，然后根据Bellford-man方程更新自己的距离向量，如果自己的距离向量发生改变，那么它向所有邻居广播更新后的距离向量。令人惊奇的是，只要所有节点都异步地交换它们的距离向量，每个开销$D_x(y)$都会收敛到$d_x(y)$!!!</p><p>当然，这样的异步策略也有问题：比如我们有一个这样的图，$c(x,y)=4,c(y,z)=1,c(x,z)=5$,然后在某一时刻c(x,y)变为60.然后节点y重新计算开销$D_y(x)=min\{c(y,x)+D_x(x),c(y,z)+D_z(x)\}=min\{60+0,1+5\}=6$。这显然是不对的。那么这种现象被称为<strong>路由选择环路</strong>，报文会不断在y，z节点循环，他将持续60-6=44次！这个问题有时被称为<strong>无穷计数</strong>。</p><p>除此之外，在DV算法，一个节点可向任何节点通告其不正确的最低路径开销（有意或无意的）,从而导致整个网络发生故障！</p><h1 id="OSPF"><a href="#OSPF" class="headerlink" title="OSPF"></a><em>OSPF</em></h1><p>随着路由器数目变得巨大，涉及路由选择信息的通信计算和存储开销将接近无穷大。为此人们涉及了自治系统（AS）,它在一个ISP中由路由器和链路互联而形成。在一个自治系统内运行的路由算法称为自治系统内部路由选择协议。</p><p>OSPF全称是 Open Shortest Path First 开放最短路优先。OSPF是一种链路状态协议，它使用洪泛链路状态协议以及Dijkstra最低开销路径算法，每台路由器都有整个自治网络的完整拓扑图。由管理员确定各条链路的开销，比如把所有链路开销设为1从而实现最小跳数；或者选择将链路权重与链路容量成反比来配置，不鼓励使用低带宽链路。</p><p>OSPF的优势如下：</p><ol><li>安全。能够鉴别OSPF路由器之间的交换，使用MD5之类的加密算法。</li><li>多条相同开销的路径。</li><li>对单播和多播路由的综合支持。</li><li>支持在单个AS中 的层次结构。</li></ol><h1 id="BGP"><a href="#BGP" class="headerlink" title="BGP"></a><em>BGP</em></h1><p>BGP Border Gateway protocol , 边界网关协议是AS间选择协议</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;DV&quot;&gt;&lt;a href=&quot;#DV&quot; class=&quot;headerlink&quot; title=&quot;DV&quot;&gt;&lt;/a&gt;&lt;em&gt;DV&lt;/em&gt;&lt;/h1&gt;&lt;p&gt;获取全局的路由信息是代价很高的，我们需要一种分布式的、异步的、迭代的、自我终止的算法，这便是DV（$Distance  v</summary>
      
    
    
    
    <category term="算法" scheme="http://durantthorvalds.top/categories/%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="分布式" scheme="http://durantthorvalds.top/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
    <category term="图算法" scheme="http://durantthorvalds.top/tags/%E5%9B%BE%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>小白投资入门（煎炸卤炖）</title>
    <link href="http://durantthorvalds.top/2020/12/14/%E5%B0%8F%E7%99%BD%E6%8A%95%E8%B5%84%E5%85%A5%E9%97%A8/"/>
    <id>http://durantthorvalds.top/2020/12/14/%E5%B0%8F%E7%99%BD%E6%8A%95%E8%B5%84%E5%85%A5%E9%97%A8/</id>
    <published>2020-12-13T16:00:00.000Z</published>
    <updated>2020-12-15T11:03:47.752Z</updated>
    
    <content type="html"><![CDATA[<h1 id="小白投资入门（煎炸卤炖）"><a href="#小白投资入门（煎炸卤炖）" class="headerlink" title="小白投资入门（煎炸卤炖）"></a>小白投资入门（煎炸卤炖）</h1><p>写这个博客表明笔者准备开始炒股了，当然任何事情都是万事开头难，希望与君共勉，踩坑。</p><p>假如小Q有1W元，如何投资才能获得最大收益：</p><ol><li>存银行</li><li>炒股</li><li>购买保险</li><li>购买基金</li><li>购买黄金</li><li>购买房地产</li><li>购买期货</li><li>全部换成美元</li><li>全部换成日元</li><li>研究“如何让钱无性生殖”</li></ol><p>上面问题便是笔者写博客的初衷，弄清楚这些问题不是简单是事，除了问题10以外。</p><h1 id="1基本概念"><a href="#1基本概念" class="headerlink" title="1基本概念"></a>1基本概念</h1><p>所谓股票，就是股份制公司发行的所有权凭证。那么公司为什么要发行股票呢？一般是为了凑集资金，非上市公司和上市公司都可以发行股票，只是非上市公司的股票不能在证券交易所交易而已。</p><h2 id="证券"><a href="#证券" class="headerlink" title="证券"></a>证券</h2><p>证券是多种经济权益凭证的统称，有很多分类，比如股票和证券。</p><h2 id="证券交易所"><a href="#证券交易所" class="headerlink" title="证券交易所"></a>证券交易所</h2><p>专门进行证券交易的场所。我国目前有四大证券交易所，分别是1990年11月26日成立的上海证券交易所，简称上交所或者沪市；1990年12月1日成立的深圳证券交易所，简称深交所；此外还有香港证券交易所和成都证券交易所。</p><p>证券交易所本身是一家公司，也可以上市，比如香港证券交易所的股票代码是00388。</p><h2 id="股市"><a href="#股市" class="headerlink" title="股市"></a>股市</h2><p>股市就是股票市场，是已经发行的股票转让、买卖和流通的场所，在我国就是指证券交易所，又称二级市场。</p><h2 id="IPO"><a href="#IPO" class="headerlink" title="IPO"></a>IPO</h2><p>指首次公开募股。通常是新上市的公司第一次发售股票。</p><h2 id="如何炒股？"><a href="#如何炒股？" class="headerlink" title="如何炒股？"></a>如何炒股？</h2><p>炒股时盈亏是兵家常事，但是如果想长期盈利，我们需要掌握正确的投资理念和搭建适合自己的交易体系，并严格指定好交易策略才能实现长期盈利。</p><h2 id="证券公司"><a href="#证券公司" class="headerlink" title="证券公司"></a>证券公司</h2><p>就是专门经营证券交易所的交易商，俗称券商。投资者通过在券商处开设账户从而去交易放在证券交易所的股票。</p><h2 id="证券账户"><a href="#证券账户" class="headerlink" title="证券账户"></a>证券账户</h2><p>我国规定每个投资者最多拥有三个账户。目前有线上和线下两种方式，最常用线上的券商包括【同花顺APP】和【东方财富】。它们的区别是，同花顺本身是一家上市公司，股票代码300033，而不是一家券商。而东方财富本身是一家券商，股票代码300059.</p><h2 id="三方管存"><a href="#三方管存" class="headerlink" title="三方管存"></a>三方管存</h2><p>全称是<strong><em>客户交易结算资金第三方管存</em></strong>，在券商开户时必须指定一张银行卡，买卖股票的资金都通过这个银行账户。</p><h1 id="2-如何看数据"><a href="#2-如何看数据" class="headerlink" title="2 如何看数据"></a>2 如何看数据</h1><h2 id="指数与点数"><a href="#指数与点数" class="headerlink" title="指数与点数"></a>指数与点数</h2><p>指数指金融机构事先制定好规则，选取一部分股票作为样本，按照某种计算方法编制乘的一组数字，目的是为了反映市场情况的变动。</p><ul><li><p>上证指数</p><p>全称【上海证券综合指数】，俗称<strong>大盘</strong>。它是把在上海证券交易所上市的所有股票按照特定的规则编制而成的一个指数，自1991年7月15日开始实时发布，一开始就是100点。其中中石油、中石化称为【两桶油】。银行、证券、保险俗称为【金三胖】。</p></li><li><p>深圳成指</p><p>全称【深圳成分股指数】，代码为399001. 它是取市场上最有代表性的500家公司编制的股票，基点为1000。</p></li><li><p>中小板指</p><p>从深交所中小板上市的所有股票中选取100只编制的股票。代码399005.</p></li><li><p>创业板指</p><p>从深交所创业板上市的所有股票中选取100只编制的股票。代码399006.</p></li><li><p>沪深300</p><p>000300</p></li><li><p>上证50</p><p>000016</p></li><li><p>中证500</p><p>主要挑选沪深两地的中小盘股票，在上海为000905，在深圳为399905.</p></li></ul><p><img src="/img/1607942239006.jpeg" alt="1607942239006"></p><h2 id="板块"><a href="#板块" class="headerlink" title="板块"></a>板块</h2><p>指证券交易所不同的交易市场，目的是支持不同类型的公司。</p><p>上交所包含主板和科创板两个。上交所主板股票代码一般以「600」、「601」或「603」开头。科创板一般以「688」开头，比如「金山办公」为688111.</p><p>深交所包括主板、中小板、创业板三个板块，中小板是专为中小型公司所开设的，创业板与其它板块区别较大，主要是因为公司在创业板上市要求更加宽松，所以上市后股票风险性更大，因而开通创业板必须去线下营业点而且要签一堆风险声明并且录视频。 深市主板代码以「000」开头，中小板以「002」开头，创业板以「300」开头。</p><h2 id="全球"><a href="#全球" class="headerlink" title="全球"></a>全球</h2><p>可以查看全球市场的大概情况，最上面是我国的沪深指数，往下依次是股指期货、汇率指数、全球商品。</p><p>「恒生指数」是香港的，「日经指数」是日本的，「富时A50」是富时指数公司选取中国市值最大的50家公司创建的指数，</p><p>「道琼斯」「纳斯达克」「标普500」是美国的，美股和「富时A50」可以影响A股走势。</p><h2 id="A股"><a href="#A股" class="headerlink" title="A股"></a>A股</h2><p>我国股市的大概情况，有三个板块，「沪深」「板块」「科创板」。</p><p>「沪深」页面最上面的上证指数、深证成指和创业板是我国A股最主要的三个指数，下面是市场概况，全部股票的涨停比例和跌停涨停比，再往下是股票排行，比如涨幅榜和跌幅榜等。</p><p>「板块」主要显示行业板块的情况，最上面是涨幅前三名的板块和跌幅前三名的板块。其中第三个页面是「创业板」，这个板块是2019年刚刚创立的，主要是支持我国科技创新的发展，对投资者而言具有较高的<strong>门槛</strong>和较大的<strong>风险</strong>。</p><h2 id="交易界面"><a href="#交易界面" class="headerlink" title="交易界面"></a>交易界面</h2><p>我们可以看到账户情况，比如总资产、浮动盈亏和当日盈亏等情况，也可以买入、卖出、撤单、持仓和查询，申购新股、国债逆回购、可转债、银证转账等功能</p><p>那么我们如何进行交易呢？</p><ul><li>通过搜索或自选解码进入股票的详情页面</li><li>点击最下面的「下单」按钮，有「分时下单」和「交易下单」两种。默认为「分时下单」。</li><li>点击「买」按钮，再次跳出新的买卖页面。</li><li>设定好合适价格，输入想要买入的数量，然后点击最下面的红色「买入」按钮。</li><li>最后会跳出询问页面，确认之前提交的信息是否有误等。点击「确认买入」</li></ul><h2 id="股市开市时间及股票交易费用"><a href="#股市开市时间及股票交易费用" class="headerlink" title="股市开市时间及股票交易费用"></a>股市开市时间及股票交易费用</h2><p>我国A股交易时间为周一至周五上午9：30-11：30，下午13：00-15：00， 其中9：15-9：25，下午14：57-15：00是集合竞价的时间，具体概念我们后面再讲。</p><p>股票交易一般包含以下几种费用：</p><ul><li>佣金：券商收取，买卖都要收，每笔最低5元，如果超过后则按万分之3或万分之2.5.</li><li>印花税：国税局收取，卖出股票时收取，比例为成交金额的0.1%</li><li>过户费：上交所收取，交易沪市股票时收取，为成交金额的0.002%。</li><li>规费：视券商而定。</li></ul><h2 id="K线"><a href="#K线" class="headerlink" title="K线"></a>K线</h2><p>k线图又称为蜡烛图、日本线、阴阳线。它源于日本德川幕府时代。它是一个竖着的长方体，如下图所示。</p><p><img src="/img/1607943017765.jpeg" alt="1607943017765" style="zoom:67%;" /></p><p>如果上影线较长，上方抛压较大，股价上涨后又被砸了回来，有长下影线则表示下方买盘较强，股价跌了下去又被买了回来。红色表示阳K线，绿色表示阴K线。红表示涨，即开盘价小于收盘价，绿表示跌。</p><p><img src="/img/1607943072426.jpeg" alt="1607943072426"></p><p><strong>注意不要把技术指标当作买卖的标准。</strong></p><p>我们有【假阴线】和【假阳线】两种说法，假阳线指收盘价比前一根K线的收盘价要低。假阳线表示收盘价全部比各自前一天的收盘价要高</p><p><img src="/img/v2-e6d2b7984a2228522f4012c2af96436e.jpg" alt="v2-e6d2b7984a2228522f4012c2af96436e"></p><h2 id="均线"><a href="#均线" class="headerlink" title="均线"></a>均线</h2><h2 id="成交量"><a href="#成交量" class="headerlink" title="成交量"></a>成交量</h2><h2 id="MACD"><a href="#MACD" class="headerlink" title="MACD"></a>MACD</h2>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;小白投资入门（煎炸卤炖）&quot;&gt;&lt;a href=&quot;#小白投资入门（煎炸卤炖）&quot; class=&quot;headerlink&quot; title=&quot;小白投资入门（煎炸卤炖）&quot;&gt;&lt;/a&gt;小白投资入门（煎炸卤炖）&lt;/h1&gt;&lt;p&gt;写这个博客表明笔者准备开始炒股了，当然任何事情都是万事开头难</summary>
      
    
    
    
    <category term="投资" scheme="http://durantthorvalds.top/categories/%E6%8A%95%E8%B5%84/"/>
    
    
    <category term="股票" scheme="http://durantthorvalds.top/tags/%E8%82%A1%E7%A5%A8/"/>
    
  </entry>
  
  <entry>
    <title>「参考」Ceph Pool</title>
    <link href="http://durantthorvalds.top/2020/12/14/ceph%20pool/"/>
    <id>http://durantthorvalds.top/2020/12/14/ceph%20pool/</id>
    <published>2020-12-13T16:00:00.000Z</published>
    <updated>2020-12-16T08:33:23.524Z</updated>
    
    <content type="html"><![CDATA[<div class="note note-primary">            <p>本期主要介绍Ceph pool的操作，需要您事先部署成功Ceph集群！</p><p>主要参考<a href="https://docs.ceph.com/en/latest/rados/operations/pools/">官方pool教程</a>。</p>          </div><h1 id="Ceph-pool"><a href="#Ceph-pool" class="headerlink" title="Ceph pool"></a>Ceph pool</h1><p>池是用于存储对象的逻辑分区。</p><p>当首次部署群集而不创建池时，Ceph使用默认池来存储数据。pool具有以下特性：</p><ul><li><strong>弹性</strong>：可以设置允许多少OSD发生故障而不丢失数据。对于复制池，它是对象的所需副本数/副本数。典型的配置存储一个对象和一个附加副本（即<code>size=2</code>），但是您可以确定副本/副本的数量。对于<a href="https://docs.ceph.com/en/latest/rados/operations/erasure-code">纠删码池</a>，它是编码块的数量<code>m=2</code></li><li><strong>放置组</strong>(PG)：可以设置池的放置组数。一个典型的配置每个OSD使用大约100个放置组，以提供最佳的平衡，而不会消耗太多的计算资源。设置多个池时，请确保为池和整个群集设置合理数量的放置组。</li><li><strong>CRUSH规则</strong>：将数据存储在池中时，对象及其副本（或用于纠删码池的块）在群集中的位置由CRUSH规则控制。如果默认规则不适用于您的用例，则可以为池创建自定义的CRUSH规则。</li><li><strong>快照</strong>：使用创建快照时，可以有效地为特定池拍摄快照。<code>ceph osd pool mksnap</code></li></ul><p>要列出群集的池，请执行：</p><pre><code class="hljs ebnf"><span class="hljs-attribute">ceph osd lspools</span></code></pre><h2 id="1-创建一个池"><a href="#1-创建一个池" class="headerlink" title="1 创建一个池"></a>1 创建一个池</h2><p>在创建池之前，我们有必要修改Ceph配置文件，它位于<code>\etc\ceph.conf</code>，典型的ceph配置文件如下：</p><pre><code class="hljs routeros">ceph osd<span class="hljs-built_in"> pool </span>create &#123;pool-name&#125; [&#123;pg-num&#125; [&#123;pgp-num&#125;]] [replicated] \     [crush-rule-name] [expected-num-objects]ceph osd<span class="hljs-built_in"> pool </span>create &#123;pool-name&#125; [&#123;pg-num&#125; [&#123;pgp-num&#125;]]   erasure \     [erasure-code-profile] [crush-rule-name] [expected_num_objects] [<span class="hljs-attribute">--autoscale-mode</span>=&lt;on,off,warn&gt;]</code></pre><p>我们对几个比较重要的参数进行讲解：</p><ul><li><code>replicated|erasure</code></li></ul><p>官方有这样一句话。副本池需要更多的空间但是实现了Ceph所有操作，而纠删码池只实现了一部分的功能。主要是因为纠删码不支持omap，所以只能采用FileStore。</p><p>实例：</p><pre><code class="hljs routeros">ceph osd<span class="hljs-built_in"> pool </span>create ecpool erasure</code></pre><h2 id=""><a href="#" class="headerlink" title=" "></a> </h2><h2 id="2-将池关联到应用程序"><a href="#2-将池关联到应用程序" class="headerlink" title="2 将池关联到应用程序"></a>2 将池关联到应用程序</h2><p>池在使用前需要与应用程序关联。将与CephFS一起使用的池或RGW自动创建的池自动关联。打算与RBD一起使用的池应使用该<code>rbd</code>工具进行初始化（有关更多信息，请参见<a href="https://docs.ceph.com/en/latest/rbd/rados-rbd-cmds/#create-a-block-device-pool">块设备命令</a>）。</p><p>对于其他情况，您可以手动将自由格式的应用程序名称关联到池。</p><pre><code class="hljs routeros">ceph osd<span class="hljs-built_in"> pool </span>application <span class="hljs-builtin-name">enable</span> &#123;pool-name&#125; &#123;application-name&#125;</code></pre><p>提示：</p><p>CephFS使用应用程序名称<code>cephfs</code>，RBD使用应用程序名称<code>rbd</code>，而RGW使用应用程序名称<code>rgw</code>。</p><h2 id="3-设置池配额"><a href="#3-设置池配额" class="headerlink" title="3 设置池配额"></a>3 设置池配额</h2><p>您可以将池配额设置为每个池的最大字节数和/或最大对象数。</p><pre><code class="hljs dsconfig"><span class="hljs-string">ceph </span><span class="hljs-string">osd </span><span class="hljs-string">pool </span><span class="hljs-built_in">set-quota</span> &#123;<span class="hljs-string">pool-name&#125;</span> [<span class="hljs-string">max_objects </span>&#123;<span class="hljs-string">obj-count&#125;</span>] [<span class="hljs-string">max_bytes </span>&#123;<span class="hljs-string">bytes&#125;</span>]</code></pre><p>例如：</p><pre><code class="hljs dsconfig"><span class="hljs-string">ceph </span><span class="hljs-string">osd </span><span class="hljs-string">pool </span><span class="hljs-built_in">set-quota</span> <span class="hljs-string">data </span><span class="hljs-string">max_objects </span><span class="hljs-string">10000</span></code></pre><p>要删除配额，请将其值设置为<code>0</code>。</p><h2 id="4-删除池"><a href="#4-删除池" class="headerlink" title="4 删除池"></a>4 删除池</h2><p>要删除池，请执行：</p><pre><code class="hljs routeros">ceph osd<span class="hljs-built_in"> pool </span>delete &#123;pool-name&#125; [&#123;pool-name&#125; --yes-i-really-really-mean-it]</code></pre><p>要删除池，必须在Monitor的配置中将mon_allow_pool_delete标志设置为true。否则，他们将拒绝删除池。</p><p>有关更多信息，请参见<a href="https://docs.ceph.com/en/latest/rados/configuration/mon-config-ref">Monitor Configuration</a>。</p><p>如果您为自己创建的池创建了自己的规则，则在不再需要池时应考虑删除它们：</p><pre><code class="hljs routeros">ceph osd<span class="hljs-built_in"> pool </span><span class="hljs-builtin-name">get</span> &#123;pool-name&#125; crush_rule</code></pre><p>例如，如果规则是“ 123”，则可以像这样检查其他池：</p><pre><code class="hljs gradle">ceph osd <span class="hljs-keyword">dump</span> | <span class="hljs-keyword">grep</span> <span class="hljs-string">&quot;^pool&quot;</span> | <span class="hljs-keyword">grep</span> <span class="hljs-string">&quot;crush_rule 123&quot;</span></code></pre><p>通过ceph osd dump可以查看monitor的映射信息，包括epoch，fsid和crush rule等。返回结果如下</p><pre><code class="hljs pgsql">pool <span class="hljs-number">1</span> <span class="hljs-string">&#x27;device_health_metrics&#x27;</span> replicated size <span class="hljs-number">3</span> min_size <span class="hljs-number">2</span> crush_rule <span class="hljs-number">0</span> object_hash rjenkins pg_num <span class="hljs-number">1</span> pgp_num <span class="hljs-number">1</span> autoscale_mode <span class="hljs-keyword">on</span> last_change <span class="hljs-number">13</span> flags hashpspool stripe_width <span class="hljs-number">0</span> pg_num_min <span class="hljs-number">1</span> application mgr_devicehealthpool <span class="hljs-number">2</span> <span class="hljs-string">&#x27;cephfs_data&#x27;</span> replicated size <span class="hljs-number">3</span> min_size <span class="hljs-number">2</span> crush_rule <span class="hljs-number">0</span> object_hash rjenkins pg_num <span class="hljs-number">32</span> pgp_num <span class="hljs-number">32</span> autoscale_mode <span class="hljs-keyword">on</span> last_change <span class="hljs-number">17</span> flags hashpspool stripe_width <span class="hljs-number">0</span> application cephfspool <span class="hljs-number">3</span> <span class="hljs-string">&#x27;cephfs_metadata&#x27;</span> replicated size <span class="hljs-number">3</span> min_size <span class="hljs-number">2</span> crush_rule <span class="hljs-number">0</span> object_hash rjenkins pg_num <span class="hljs-number">32</span> pgp_num <span class="hljs-number">32</span> autoscale_mode <span class="hljs-keyword">on</span> last_change <span class="hljs-number">18</span> flags hashpspool stripe_width <span class="hljs-number">0</span> pg_autoscale_bias <span class="hljs-number">4</span> pg_num_min <span class="hljs-number">16</span> recovery_priority <span class="hljs-number">5</span> application cephfspool <span class="hljs-number">4</span> <span class="hljs-string">&#x27;ecpool&#x27;</span> erasure profile toy_ec size <span class="hljs-number">3</span> min_size <span class="hljs-number">2</span> crush_rule <span class="hljs-number">1</span> object_hash rjenkins pg_num <span class="hljs-number">16</span> pgp_num <span class="hljs-number">16</span> autoscale_mode <span class="hljs-keyword">on</span> last_change <span class="hljs-number">65</span> flags hashpspool,ec_overwrites stripe_width <span class="hljs-number">8192</span> application rgw</code></pre><p>如果没有其他池使用该自定义规则，则可以从群集中删除该规则。</p><p>如果您创建的用户严格地具有不再存在的池的权限，则也应该考虑删除这些用户：</p><pre><code class="hljs dust"><span class="xml">ceph auth ls | grep -C 5 </span><span class="hljs-template-variable">&#123;pool-name&#125;</span><span class="xml">ceph auth del </span><span class="hljs-template-variable">&#123;user&#125;</span></code></pre><h2 id="5-重命名池"><a href="#5-重命名池" class="headerlink" title="5 重命名池"></a>5 重命名池</h2><p>要重命名池，请执行：</p><pre><code class="hljs routeros">ceph osd<span class="hljs-built_in"> pool </span>rename &#123;current-pool-name&#125; &#123;new-pool-name&#125;</code></pre><p>如果重命名池，并且您具有针对经过身份验证的用户的每个池功能，则必须使用新的池名称更新用户的功能（即上限）。</p><h2 id="6-显示池统计信息"><a href="#6-显示池统计信息" class="headerlink" title="6 显示池统计信息"></a>6 显示池统计信息</h2><p>要显示池的利用率统计信息，请执行：</p><pre><code class="hljs ebnf"><span class="hljs-attribute">rados df</span></code></pre><p>返回如下</p><pre><code class="hljs apache"><span class="hljs-attribute">POOL_NAME</span>                 USED  OBJECTS  CLONES  COPIES  MISSING_ON_PRIMARY  UNFOUND  DEGRADED  RD_OPS       RD  WR_OPS      WR  USED COMPR  UNDER COMPR<span class="hljs-attribute">cephfs_data</span>                <span class="hljs-number">0</span> B        <span class="hljs-number">0</span>       <span class="hljs-number">0</span>       <span class="hljs-number">0</span>                   <span class="hljs-number">0</span>        <span class="hljs-number">0</span>         <span class="hljs-number">0</span>       <span class="hljs-number">0</span>      <span class="hljs-number">0</span> B       <span class="hljs-number">0</span>     <span class="hljs-number">0</span> B         <span class="hljs-number">0</span> B          <span class="hljs-number">0</span> B<span class="hljs-attribute">cephfs_metadata</span>        <span class="hljs-number">156</span> KiB       <span class="hljs-number">22</span>       <span class="hljs-number">0</span>      <span class="hljs-number">66</span>                   <span class="hljs-number">0</span>        <span class="hljs-number">0</span>         <span class="hljs-number">0</span>     <span class="hljs-number">362</span>  <span class="hljs-number">382</span> KiB      <span class="hljs-number">70</span>  <span class="hljs-number">24</span> KiB         <span class="hljs-number">0</span> B          <span class="hljs-number">0</span> B<span class="hljs-attribute">device_health_metrics</span>   <span class="hljs-number">15</span> KiB        <span class="hljs-number">1</span>       <span class="hljs-number">0</span>       <span class="hljs-number">3</span>                   <span class="hljs-number">0</span>        <span class="hljs-number">0</span>         <span class="hljs-number">0</span>      <span class="hljs-number">17</span>   <span class="hljs-number">17</span> KiB      <span class="hljs-number">19</span>  <span class="hljs-number">19</span> KiB         <span class="hljs-number">0</span> B          <span class="hljs-number">0</span> B<span class="hljs-attribute">ecpool</span>                  <span class="hljs-number">12</span> KiB        <span class="hljs-number">1</span>       <span class="hljs-number">0</span>       <span class="hljs-number">3</span>                   <span class="hljs-number">0</span>        <span class="hljs-number">0</span>         <span class="hljs-number">0</span>       <span class="hljs-number">0</span>      <span class="hljs-number">0</span> B       <span class="hljs-number">3</span>   <span class="hljs-number">3</span> KiB         <span class="hljs-number">0</span> B          <span class="hljs-number">0</span> B<span class="hljs-attribute">total_objects</span>    <span class="hljs-number">24</span><span class="hljs-attribute">total_used</span>       <span class="hljs-number">3</span>.<span class="hljs-number">0</span> GiB<span class="hljs-attribute">total_avail</span>      <span class="hljs-number">57</span> GiB<span class="hljs-attribute">total_space</span>      <span class="hljs-number">60</span> GiB</code></pre><p>此外，要获取特定池或全部池的I / O信息，请执行以下操作：</p><pre><code class="hljs routeros">ceph osd<span class="hljs-built_in"> pool </span>stats [&#123;pool-name&#125;]</code></pre><h2 id="7-制作池快照"><a href="#7-制作池快照" class="headerlink" title="7 制作池快照"></a>7 制作池快照</h2><p>要制作池的快照，请执行：</p><pre><code class="hljs routeros">ceph osd<span class="hljs-built_in"> pool </span>mksnap &#123;pool-name&#125; &#123;snap-name&#125;</code></pre><h2 id="8-删除池快照"><a href="#8-删除池快照" class="headerlink" title="8 删除池快照"></a>8 删除池快照</h2><p>要删除池的快照，请执行：</p><pre><code class="hljs routeros">ceph osd<span class="hljs-built_in"> pool </span>rmsnap &#123;pool-name&#125; &#123;snap-name&#125;</code></pre><h2 id="9-设置对象副本数"><a href="#9-设置对象副本数" class="headerlink" title="9 设置对象副本数"></a>9 设置对象副本数</h2><p>要设置复制池上对象副本的数量，请执行以下操作：</p><pre><code class="hljs routeros">ceph osd<span class="hljs-built_in"> pool </span><span class="hljs-builtin-name">set</span> &#123;poolname&#125; size &#123;num-replicas&#125;</code></pre><div class="note note-danger">            <p>重要</p><p><code>{num-replicas}</code>包括所述对象本身。如果您需要该对象和该对象的两个副本，以总共三个对象的实例，请指定<code>3</code>。</p>          </div><p>例如：</p><pre><code class="hljs routeros">ceph osd<span class="hljs-built_in"> pool </span><span class="hljs-builtin-name">set</span> data size 3</code></pre><p>您可以为每个池执行此命令。<strong>注意：</strong>对象在降级模式下接受的I / O可能少于副本。要设置I / O所需的最小副本数，应使用该设置。例如：<code>pool size``min_size</code></p><pre><code class="hljs routeros">ceph osd<span class="hljs-built_in"> pool </span><span class="hljs-builtin-name">set</span> data min_size 2</code></pre><p>这样可以确保数据池中的任何对象都不会收到少于<code>min_size</code>副本的I / O。</p><h2 id="10-获取对象副本数"><a href="#10-获取对象副本数" class="headerlink" title="10 获取对象副本数"></a>10 获取对象副本数</h2><p>要获取对象副本的数量，请执行以下操作：</p><pre><code class="hljs gradle">ceph osd <span class="hljs-keyword">dump</span> | <span class="hljs-keyword">grep</span> <span class="hljs-string">&#x27;replicated size&#x27;</span></code></pre><p>Ceph将列出池，并突出显示该属性。默认情况下，ceph创建一个对象的两个副本（共三个副本，或3个大小）。<code>replicated size</code></p><h2 id="11-池值"><a href="#11-池值" class="headerlink" title="11 池值"></a>11 池值</h2><p>要将值设置为池，请执行以下操作：</p><pre><code class="hljs routeros">ceph osd<span class="hljs-built_in"> pool </span><span class="hljs-builtin-name">set</span> &#123;pool-name&#125; &#123;key&#125; &#123;value&#125;</code></pre><p>您可以为以下键设置值：</p><pre><code class="hljs ebnf"><span class="hljs-attribute">compression_algorithm</span></code></pre><ul><li><p>描述</p><p>设置用于基础BlueStore的内联压缩算法。此设置将覆盖<a href="https://docs.ceph.com/en/latest/rados/configuration/bluestore-config-ref/#inline-compression">全局设置</a>的。<code>bluestore compression algorithm</code></p></li><li><p>类型</p><p>串</p></li><li><p>有效设定</p><p><code>lz4</code>，<code>snappy</code>，<code>zlib</code>，<code>zstd</code></p></li></ul><pre><code class="hljs ebnf"><span class="hljs-attribute">compression_mode</span></code></pre><ul><li><p>描述</p><p>为基础BlueStore设置嵌入式压缩算法的策略。此设置将覆盖<a href="http://docs.ceph.com/en/latest/rados/configuration/bluestore-config-ref/#inline-compression">全局设置</a>的。<code>bluestore compression mode</code></p></li><li><p>类型</p><p>串</p></li><li><p>有效设定</p><p><code>none</code>，<code>passive</code>，<code>aggressive</code>，<code>force</code></p></li></ul><pre><code class="hljs ebnf"><span class="hljs-attribute">compression_min_blob_size</span></code></pre><ul><li><p>描述</p><p>小于此大小的块永远不会被压缩。此设置将覆盖<a href="http://docs.ceph.com/en/latest/rados/configuration/bluestore-config-ref/#inline-compression">全局设置</a>的。<code>bluestore compression min blob *</code></p></li><li><p>类型</p><p>无符号整数</p></li></ul><pre><code class="hljs ebnf"><span class="hljs-attribute">compression_max_blob_size</span></code></pre><ul><li><p>描述</p><p>大于此的块<code>compression_max_blob_size</code>在压缩之前会分解为较小的斑点大小 。</p></li><li><p>类型</p><p>无符号整数</p></li></ul><pre><code class="hljs arduino"><span class="hljs-built_in">size</span></code></pre><ul><li><p>描述</p><p>设置池中对象的副本数。有关更多详细信息，请参见<a href="https://docs.ceph.com/en/latest/rados/operations/pools/#set-the-number-of-object-replicas">设置对象副本数</a>。仅复制池。</p></li><li><p>类型</p><p>整数</p></li></ul><pre><code class="hljs ebnf"><span class="hljs-attribute">min_size</span></code></pre><ul><li><p>描述</p><p>设置I / O所需的最小副本数。有关更多详细信息，请参见<a href="https://docs.ceph.com/en/latest/rados/operations/pools/#set-the-number-of-object-replicas">设置对象副本数</a>。对于擦除编码池，应将其设置为大于“ k”的值，因为如果我们将IO的值设置为“ k”，则不会出现冗余，并且如果OSD永久性故障，数据将会丢失。有关更多信息，请参见<a href="https://docs.ceph.com/en/latest/rados/operations/erasure-code">擦除代码</a></p></li><li><p>类型</p><p>整数</p></li><li><p>版</p><p><code>0.54</code> 以上</p></li></ul><pre><code class="hljs pgsql">pg_num</code></pre><ul><li><p>描述</p><p>计算数据放置时要使用的放置组的有效数量。</p></li><li><p>类型</p><p>整数</p></li><li><p>有效范围</p><p>优于<code>pg_num</code>当前值。</p></li></ul><pre><code class="hljs ebnf"><span class="hljs-attribute">pgp_num</span></code></pre><ul><li><p>描述</p><p>计算数据放置时要使用的放置的有效放置组数。</p></li><li><p>类型</p><p>整数</p></li><li><p>有效范围</p><p>等于或小于<code>pg_num</code>。</p></li></ul><pre><code class="hljs ebnf"><span class="hljs-attribute">crush_rule</span></code></pre><ul><li><p>描述</p><p>用于在集群中映射对象放置的规则。</p></li><li><p>类型</p><p>串</p></li></ul><pre><code class="hljs ebnf"><span class="hljs-attribute">allow_ec_overwrites</span></code></pre><ul><li><p>描述</p><p>是否写入擦除代码池可以更新对象的一部分，因此cephfs和rbd可以使用它。有关更多详细信息，请参见 <a href="https://docs.ceph.com/en/latest/rados/operations/erasure-code#erasure-coding-with-overwrites">带覆盖的擦除编码</a>。</p></li><li><p>类型</p><p>布尔型</p></li><li><p>版</p><p><code>12.2.0</code> 以上</p></li></ul><pre><code class="hljs ebnf"><span class="hljs-attribute">hashpspool</span></code></pre><ul><li><p>描述</p><p>在给定的池上设置/取消设置HASHPSPOOL标志。</p></li><li><p>类型</p><p>整数</p></li><li><p>有效范围</p><p>1个设置标志，0个未设置标志</p></li></ul><pre><code class="hljs ebnf"><span class="hljs-attribute">nodelete</span></code></pre><ul><li><p>描述</p><p>在给定的池上设置/取消设置NODELETE标志。</p></li><li><p>类型</p><p>整数</p></li><li><p>有效范围</p><p>1个设置标志，0个未设置标志</p></li><li><p>版</p><p>版 <code>FIXME</code></p></li></ul><pre><code class="hljs ebnf"><span class="hljs-attribute">nopgchange</span></code></pre><ul><li><p>描述</p><p>在给定的池上设置/取消设置NOPGCHANGE标志。</p></li><li><p>类型</p><p>整数</p></li><li><p>有效范围</p><p>1个设置标志，0个未设置标志</p></li><li><p>版</p><p>版 <code>FIXME</code></p></li></ul><pre><code class="hljs ebnf"><span class="hljs-attribute">nosizechange</span></code></pre><ul><li><p>描述</p><p>在给定的池上设置/取消设置NOSIZECHANGE标志。</p></li><li><p>类型</p><p>整数</p></li><li><p>有效范围</p><p>1个设置标志，0个未设置标志</p></li><li><p>版</p><p>版 <code>FIXME</code></p></li></ul><pre><code class="hljs ebnf"><span class="hljs-attribute">write_fadvise_dontneed</span></code></pre><ul><li><p>描述</p><p>在给定的池上设置/取消设置WRITE_FADVISE_DONTNEED标志。</p></li><li><p>类型</p><p>整数</p></li><li><p>有效范围</p><p>1个设置标志，0个未设置标志</p></li></ul><pre><code class="hljs ebnf"><span class="hljs-attribute">noscrub</span></code></pre><ul><li><p>描述</p><p>在给定的池上设置/取消设置NOSCRUB标志。</p></li><li><p>类型</p><p>整数</p></li><li><p>有效范围</p><p>1个设置标志，0个未设置标志</p></li></ul><pre><code class="hljs ebnf"><span class="hljs-attribute">nodeep-scrub</span></code></pre><ul><li><p>描述</p><p>在给定的池上设置/取消设置NODEEP_SCRUB标志。</p></li><li><p>类型</p><p>整数</p></li><li><p>有效范围</p><p>1个设置标志，0个未设置标志</p></li></ul><pre><code class="hljs ebnf"><span class="hljs-attribute">hit_set_type</span></code></pre><ul><li><p>描述</p><p>对高速缓存池启用命中集跟踪。有关其他信息，请参见<a href="https://en.wikipedia.org/wiki/Bloom_filter">Bloom Filter</a>。</p></li><li><p>类型</p><p>串</p></li><li><p>有效设定</p><p><code>bloom</code>，<code>explicit_hash</code>，<code>explicit_object</code></p></li><li><p>默认</p><p><code>bloom</code>。其他值用于测试。</p></li></ul><pre><code class="hljs ebnf"><span class="hljs-attribute">hit_set_count</span></code></pre><ul><li><p>描述</p><p>要为高速缓存池存储的命中集的数量。该数字越高，<code>ceph-osd</code>守护程序消耗的RAM就越多。</p></li><li><p>类型</p><p>整数</p></li><li><p>有效范围</p><p><code>1</code>。代理尚未处理&gt; 1。</p></li></ul><pre><code class="hljs ebnf"><span class="hljs-attribute">hit_set_period</span></code></pre><ul><li><p>描述</p><p>高速缓存池的命中设置周期的持续时间（以秒为单位）。该数字越高，<code>ceph-osd</code>守护程序消耗的RAM就越多 。</p></li><li><p>类型</p><p>整数</p></li><li><p>例</p><p><code>3600</code> 1小时</p></li></ul><pre><code class="hljs ebnf"><span class="hljs-attribute">hit_set_fpp</span></code></pre><ul><li><p>描述</p><p><code>bloom</code>匹配集类型的误报概率。有关其他信息，请参见<a href="https://en.wikipedia.org/wiki/Bloom_filter">Bloom Filter</a>。</p></li><li><p>类型</p><p>双</p></li><li><p>有效范围</p><p>0.0-1.0</p></li><li><p>默认</p><p><code>0.05</code></p></li></ul><pre><code class="hljs ebnf"><span class="hljs-attribute">cache_target_dirty_ratio</span></code></pre><ul><li><p>描述</p><p>在缓存分层代理将其刷新到后备存储池之前，包含修改后的（脏）对象的缓存池的百分比。</p></li><li><p>类型</p><p>双</p></li><li><p>默认</p><p><code>.4</code></p></li></ul><pre><code class="hljs ebnf"><span class="hljs-attribute">cache_target_dirty_high_ratio</span></code></pre><ul><li><p>描述</p><p>在缓存分层代理将其以较高速度刷新到后备存储池之前，包含修改后的（脏）对象的缓存池的百分比。</p></li><li><p>类型</p><p>双</p></li><li><p>默认</p><p><code>.6</code></p></li></ul><pre><code class="hljs ebnf"><span class="hljs-attribute">cache_target_full_ratio</span></code></pre><ul><li><p>描述</p><p>在缓存分层代理将其从缓存池中驱逐之前，包含未修改（干净）对象的缓存池的百分比。</p></li><li><p>类型</p><p>双</p></li><li><p>默认</p><p><code>.8</code></p></li></ul><pre><code class="hljs ebnf"><span class="hljs-attribute">target_max_bytes</span></code></pre><ul><li><p>描述</p><p><code>max_bytes</code>触发阈值时，Ceph将开始刷新或逐出对象 。</p></li><li><p>类型</p><p>整数</p></li><li><p>例</p><p><code>1000000000000</code> ＃1-TB</p></li></ul><pre><code class="hljs ebnf"><span class="hljs-attribute">target_max_objects</span></code></pre><ul><li><p>描述</p><p><code>max_objects</code>触发阈值时，Ceph将开始刷新或逐出对象 。</p></li><li><p>类型</p><p>整数</p></li><li><p>例</p><p><code>1000000</code> ＃1M个对象</p></li></ul><pre><code class="hljs ebnf"><span class="hljs-attribute">hit_set_grade_decay_rate</span></code></pre><ul><li><p>描述</p><p>两个连续命中点之间的温度衰减率</p></li><li><p>类型</p><p>整数</p></li><li><p>有效范围</p><p>0-100</p></li><li><p>默认</p><p><code>20</code></p></li></ul><pre><code class="hljs ebnf"><span class="hljs-attribute">hit_set_search_last_n</span></code></pre><ul><li><p>描述</p><p>计算hit_sets中最多N个出现以进行温度计算</p></li><li><p>类型</p><p>整数</p></li><li><p>有效范围</p><p>0-hit_set_count</p></li><li><p>默认</p><p><code>1</code></p></li></ul><pre><code class="hljs ebnf"><span class="hljs-attribute">cache_min_flush_age</span></code></pre><ul><li><p>描述</p><p>缓存分层代理将对象从缓存池刷新到存储池之前的时间（以秒为单位）。</p></li><li><p>类型</p><p>整数</p></li><li><p>例</p><p><code>600</code> 10分钟</p></li></ul><pre><code class="hljs ebnf"><span class="hljs-attribute">cache_min_evict_age</span></code></pre><ul><li><p>描述</p><p>缓存分层代理将对象从缓存池中逐出之前的时间（以秒为单位）。</p></li><li><p>类型</p><p>整数</p></li><li><p>例</p><p><code>1800</code> 30分钟</p></li></ul><pre><code class="hljs ebnf"><span class="hljs-attribute">fast_read</span></code></pre><ul><li><p>描述</p><p>在擦除编码池上，如果打开此标志，则读取请求将对所有分片发出子读取，并等待直到接收到足够的分片以解码以服务于客户端。对于jerasure和isaerasure插件，一旦返回第一个K答复，就会使用从这些答复中解码的数据立即满足客户的请求。这有助于权衡一些资源以获得更好的性能。当前，仅擦除编码池支持此标志。</p></li><li><p>类型</p><p>布尔型</p></li><li><p>默认值</p><p><code>0</code></p></li></ul><pre><code class="hljs ebnf"><span class="hljs-attribute">scrub_min_interval</span></code></pre><ul><li><p>描述</p><p>负载低时最小时间间隔（以秒为单位）。如果为0，则使用config中的osd_scrub_min_interval值。</p></li><li><p>类型</p><p>双</p></li><li><p>默认</p><p><code>0</code></p></li></ul><pre><code class="hljs ebnf"><span class="hljs-attribute">scrub_max_interval</span></code></pre><ul><li><p>描述</p><p>池清理的最大时间间隔（以秒为单位），与群集负载无关。如果为0，则使用config中的osd_scrub_max_interval值。</p></li><li><p>类型</p><p>双</p></li><li><p>默认</p><p><code>0</code></p></li></ul><pre><code class="hljs ebnf"><span class="hljs-attribute">deep_scrub_interval</span></code></pre><ul><li><p>描述</p><p>池“深度”清理的时间间隔（以秒为单位）。如果为0，则使用config中的osd_deep_scrub_interval值。</p></li><li><p>类型</p><p>双</p></li><li><p>默认</p><p><code>0</code></p></li></ul><pre><code class="hljs ebnf"><span class="hljs-attribute">recovery_priority</span></code></pre><ul><li><p>描述</p><p>设置值后，它将增加或减少计算出的预留优先级。此值的范围必须在-10到10之间。对于不太重要的池，请使用负优先级，以使它们的优先级低于任何新池。</p></li><li><p>类型</p><p>整数</p></li><li><p>默认</p><p><code>0</code></p></li></ul><pre><code class="hljs ebnf"><span class="hljs-attribute">recovery_op_priority</span></code></pre><ul><li><p>描述</p><p>指定该池的恢复操作优先级，而不要指定<code>osd_recovery_op_priority</code>。</p></li><li><p>类型</p><p>整数</p></li><li><p>默认</p><p><code>0</code></p></li></ul><h2 id="12-获取池值"><a href="#12-获取池值" class="headerlink" title="12 获取池值"></a>12 获取池值</h2><p>要从池中获取值，请执行以下操作：</p><pre><code class="hljs routeros">ceph osd<span class="hljs-built_in"> pool </span><span class="hljs-builtin-name">get</span> &#123;pool-name&#125; &#123;key&#125;</code></pre><p>您可能会获得以下键的值：</p><pre><code class="hljs arduino"><span class="hljs-built_in">size</span></code></pre><ul><li><p>描述</p><p>看<a href="https://docs.ceph.com/en/latest/rados/operations/pools/#size">大小</a></p></li><li><p>类型</p><p>整数</p></li></ul><pre><code class="hljs ebnf"><span class="hljs-attribute">min_size</span></code></pre><ul><li><p>描述</p><p>见<a href="https://docs.ceph.com/en/latest/rados/operations/pools/#min-size">min_size</a></p></li><li><p>类型</p><p>整数</p></li><li><p>版</p><p><code>0.54</code> 以上</p></li></ul><pre><code class="hljs pgsql">pg_num</code></pre><ul><li><p>描述</p><p>见<a href="https://docs.ceph.com/en/latest/rados/operations/pools/#pg-num">pg_num</a></p></li><li><p>类型</p><p>整数</p></li></ul><pre><code class="hljs ebnf"><span class="hljs-attribute">pgp_num</span></code></pre><ul><li><p>描述</p><p>见<a href="https://docs.ceph.com/en/latest/rados/operations/pools/#pgp-num">pgp_num</a></p></li><li><p>类型</p><p>整数</p></li><li><p>有效范围</p><p>等于或小于<code>pg_num</code>。</p></li></ul><pre><code class="hljs ebnf"><span class="hljs-attribute">crush_rule</span></code></pre><ul><li><p>描述</p><p>见<a href="https://docs.ceph.com/en/latest/rados/operations/pools/#crush-rule">rush_rule</a></p></li></ul><pre><code class="hljs ebnf"><span class="hljs-attribute">hit_set_type</span></code></pre><ul><li><p>描述</p><p>见<a href="https://docs.ceph.com/en/latest/rados/operations/pools/#hit-set-type">hit_set_type</a></p></li><li><p>类型</p><p>串</p></li><li><p>有效设定</p><p><code>bloom</code>，<code>explicit_hash</code>，<code>explicit_object</code></p></li></ul><pre><code class="hljs ebnf"><span class="hljs-attribute">hit_set_count</span></code></pre><ul><li><p>描述</p><p>见<a href="https://docs.ceph.com/en/latest/rados/operations/pools/#hit-set-count">hit_set_count</a></p></li><li><p>类型</p><p>整数</p></li></ul><pre><code class="hljs ebnf"><span class="hljs-attribute">hit_set_period</span></code></pre><ul><li><p>描述</p><p>参见<a href="https://docs.ceph.com/en/latest/rados/operations/pools/#hit-set-period">hit_set_period</a></p></li><li><p>类型</p><p>整数</p></li></ul><pre><code class="hljs ebnf"><span class="hljs-attribute">hit_set_fpp</span></code></pre><ul><li><p>描述</p><p>见<a href="https://docs.ceph.com/en/latest/rados/operations/pools/#hit-set-fpp">hit_set_fpp</a></p></li><li><p>类型</p><p>双</p></li></ul><pre><code class="hljs ebnf"><span class="hljs-attribute">cache_target_dirty_ratio</span></code></pre><ul><li><p>描述</p><p>参见<a href="https://docs.ceph.com/en/latest/rados/operations/pools/#cache-target-dirty-ratio">cache_target_dirty_ratio</a></p></li><li><p>类型</p><p>双</p></li></ul><pre><code class="hljs ebnf"><span class="hljs-attribute">cache_target_dirty_high_ratio</span></code></pre><ul><li><p>描述</p><p>参见<a href="https://docs.ceph.com/en/latest/rados/operations/pools/#cache-target-dirty-high-ratio">cache_target_dirty_high_ratio</a></p></li><li><p>类型</p><p>双</p></li></ul><pre><code class="hljs ebnf"><span class="hljs-attribute">cache_target_full_ratio</span></code></pre><ul><li><p>描述</p><p>请参阅<a href="https://docs.ceph.com/en/latest/rados/operations/pools/#cache-target-full-ratio">cache_target_full_ratio</a></p></li><li><p>类型</p><p>双</p></li></ul><pre><code class="hljs ebnf"><span class="hljs-attribute">target_max_bytes</span></code></pre><ul><li><p>描述</p><p>参见<a href="https://docs.ceph.com/en/latest/rados/operations/pools/#target-max-bytes">target_max_bytes</a></p></li><li><p>类型</p><p>整数</p></li></ul><pre><code class="hljs ebnf"><span class="hljs-attribute">target_max_objects</span></code></pre><ul><li><p>描述</p><p>参见<a href="https://docs.ceph.com/en/latest/rados/operations/pools/#target-max-objects">target_max_objects</a></p></li><li><p>类型</p><p>整数</p></li></ul><pre><code class="hljs ebnf"><span class="hljs-attribute">cache_min_flush_age</span></code></pre><ul><li><p>描述</p><p>参见<a href="https://docs.ceph.com/en/latest/rados/operations/pools/#cache-min-flush-age">cache_min_flush_age</a></p></li><li><p>类型</p><p>整数</p></li></ul><pre><code class="hljs ebnf"><span class="hljs-attribute">cache_min_evict_age</span></code></pre><ul><li><p>描述</p><p>参见<a href="https://docs.ceph.com/en/latest/rados/operations/pools/#cache-min-evict-age">cache_min_evict_age</a></p></li><li><p>类型</p><p>整数</p></li></ul><pre><code class="hljs ebnf"><span class="hljs-attribute">fast_read</span></code></pre><ul><li><p>描述</p><p>见<a href="https://docs.ceph.com/en/latest/rados/operations/pools/#fast-read">fast_read</a></p></li><li><p>类型</p><p>布尔型</p></li></ul><pre><code class="hljs ebnf"><span class="hljs-attribute">scrub_min_interval</span></code></pre><ul><li><p>描述</p><p>参见<a href="https://docs.ceph.com/en/latest/rados/operations/pools/#scrub-min-interval">scrub_min_interval</a></p></li><li><p>类型</p><p>双</p></li></ul><pre><code class="hljs ebnf"><span class="hljs-attribute">scrub_max_interval</span></code></pre><ul><li><p>描述</p><p>参见<a href="https://docs.ceph.com/en/latest/rados/operations/pools/#scrub-max-interval">scrub_max_interval</a></p></li><li><p>类型</p><p>双</p></li></ul><pre><code class="hljs ebnf"><span class="hljs-attribute">deep_scrub_interval</span></code></pre><ul><li><p>描述</p><p>见<a href="https://docs.ceph.com/en/latest/rados/operations/pools/#deep-scrub-interval">deep_scrub_interval</a></p></li><li><p>类型</p><p>双</p></li></ul><pre><code class="hljs ebnf"><span class="hljs-attribute">allow_ec_overwrites</span></code></pre><ul><li><p>描述</p><p>参见<a href="https://docs.ceph.com/en/latest/rados/operations/pools/#allow-ec-overwrites">allow_ec_overwrites</a></p></li><li><p>类型</p><p>布尔型</p></li></ul><pre><code class="hljs ebnf"><span class="hljs-attribute">recovery_priority</span></code></pre><ul><li><p>描述</p><p>见<a href="https://docs.ceph.com/en/latest/rados/operations/pools/#recovery-priority">recovery_priority</a></p></li><li><p>类型</p><p>整数</p></li></ul><pre><code class="hljs ebnf"><span class="hljs-attribute">recovery_op_priority</span></code></pre><ul><li><p>描述</p><p>见<a href="https://docs.ceph.com/en/latest/rados/operations/pools/#recovery-op-priority">recovery_op_priority</a></p></li><li><p>类型</p><p>整数</p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;div class=&quot;note note-primary&quot;&gt;
            &lt;p&gt;本期主要介绍Ceph pool的操作，需要您事先部署成功Ceph集群！&lt;/p&gt;&lt;p&gt;主要参考&lt;a href=&quot;https://docs.ceph.com/en/latest/rados/</summary>
      
    
    
    
    <category term="ceph" scheme="http://durantthorvalds.top/categories/ceph/"/>
    
    
    <category term="Ceph实践" scheme="http://durantthorvalds.top/tags/Ceph%E5%AE%9E%E8%B7%B5/"/>
    
  </entry>
  
  <entry>
    <title>「研究向」Redis中跳表实现原理</title>
    <link href="http://durantthorvalds.top/2020/12/01/%E3%80%90%E7%A0%94%E7%A9%B6%E5%90%91%E3%80%91Redis%E4%B8%AD%E8%B7%B3%E8%A1%A8%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/"/>
    <id>http://durantthorvalds.top/2020/12/01/%E3%80%90%E7%A0%94%E7%A9%B6%E5%90%91%E3%80%91Redis%E4%B8%AD%E8%B7%B3%E8%A1%A8%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/</id>
    <published>2020-11-30T16:00:00.000Z</published>
    <updated>2021-01-01T08:14:38.825Z</updated>
    
    <content type="html"><![CDATA[<h1 id="跳表-SkipList的实现原理"><a href="#跳表-SkipList的实现原理" class="headerlink" title="跳表 SkipList的实现原理"></a>跳表 SkipList的实现原理</h1><h2 id="XII-跳跃表（Skip-list）实现排序"><a href="#XII-跳跃表（Skip-list）实现排序" class="headerlink" title="XII 跳跃表（Skip list）实现排序"></a>XII 跳跃表（Skip list）实现排序</h2><blockquote><h4 id="1206-设计跳表-这一题可以帮助我们快速理解跳表原理。"><a href="#1206-设计跳表-这一题可以帮助我们快速理解跳表原理。" class="headerlink" title="1206. 设计跳表  这一题可以帮助我们快速理解跳表原理。"></a><a href="https://leetcode-cn.com/problems/design-skiplist/">1206. 设计跳表  </a>这一题可以帮助我们快速理解跳表原理。</h4><p>这种数据结构是由<a href="https://en.wikipedia.org/wiki/William_Pugh">William Pugh</a>发明的，最早出现于他在1990年发表的论文《<a href="ftp://ftp.cs.umd.edu/pub/skipLists/skiplists.pdf">Skip Lists: A Probabilistic Alternative to Balanced Trees</a>》</p></blockquote><p>跳跃表是一种<strong>随机化</strong>的数据结构，基于<strong>并联</strong>的链表，其效率相当于二叉查找树，查找和删除均为$O(logn)$。有序的链表加上附加的前进链接。</p><p><img src="/img/1506_skiplist.gif" alt="img"></p><p>为避免插入节点时保持上下节点的节点数之比为2：1的麻烦，跳表不要求上下相邻两层的节点数有严格的对应关系，而是为每个节点随机选择一个层数（level），比如一个节点随机出层数为3，那么就把它链入第1层到第3层这三层链表中。</p><p><img src="http://zhangtielei.com/assets/photos_redis/skiplist/skiplist_insertions.png" alt="skiplist插入形成过程"></p><p>我们可以看到，每一个节点的层数是随机的，而且插入一个节点不会影响其它节点的层数。因此插入操作只需要修改插入节点前后的指针，而不需要对很多节点进行调整。这就降低了插入操作的时间复杂度。并且在性能上优于平衡树。</p><p>下面我们介绍插入操作计算随机层数的过程：</p><ul><li>首先，每一个节点肯定有第1层的指针</li><li>如果一个节点有第i层指针(i≥1)指针，即节点已在1层到第i层链表中，那么它有第（i+1）层指针的概率为p。</li><li>节点的最大层数不允许超过一个最大值，记为<code>MAX_LEVEL</code></li></ul><p>这个计算随机层数的伪码如下所示：</p><pre><code class="hljs pgsql">randomLevel()    <span class="hljs-keyword">level</span> := <span class="hljs-number">1</span>    // random()返回一个[<span class="hljs-number">0.</span>.<span class="hljs-number">.1</span>)的随机数    <span class="hljs-keyword">while</span> random() &lt; p <span class="hljs-keyword">and</span> <span class="hljs-keyword">level</span> &lt; MaxLevel <span class="hljs-keyword">do</span>        <span class="hljs-keyword">level</span> := <span class="hljs-keyword">level</span> + <span class="hljs-number">1</span>    <span class="hljs-keyword">return</span> <span class="hljs-keyword">level</span></code></pre><p>randomLevel()的伪码中包含两个参数，一个是p，一个是MaxLevel。在Redis的skiplist实现中，这两个参数的取值为：</p><pre><code class="hljs ini"><span class="hljs-attr">p</span> = <span class="hljs-number">1</span>/<span class="hljs-number">4</span><span class="hljs-attr">MaxLevel</span> = <span class="hljs-number">32</span></code></pre><p>根据前面randomLevel()的伪码，我们很容易看出，产生越高的节点层数，概率越低。定量的分析如下：</p><ul><li>节点层数至少为1。而大于1的节点层数，满足一个概率分布。</li><li>节点层数恰好等于1的概率为1-p。</li><li>节点层数大于等于2的概率为p，而节点层数恰好等于2的概率为p(1-p)。</li><li>节点层数大于等于3的概率为p2，而节点层数恰好等于3的概率为p2(1-p)。</li><li>节点层数大于等于4的概率为p3，而节点层数恰好等于4的概率为p3(1-p)。</li><li>……</li></ul><p>因此，一个节点的平均层数（也即包含的平均指针数目），计算如下：</p><script type="math/tex; mode=display">(1-p)+2p(1-p)+3p^2(1-p)+.... = (1-p)\sum\limits_{k=1}^{+\infin}=(1-p)\frac{1}{(1-p)^2}=\frac1{1-p}</script><p>现在很容易计算出：</p><ul><li>当p=1/2时，每个节点所包含的平均指针数目为2；</li><li>当p=1/4时，每个节点所包含的平均指针数目为1.33。这也是Redis里的skiplist实现在空间上的开销。</li><li></li></ul><hr>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;跳表-SkipList的实现原理&quot;&gt;&lt;a href=&quot;#跳表-SkipList的实现原理&quot; class=&quot;headerlink&quot; title=&quot;跳表 SkipList的实现原理&quot;&gt;&lt;/a&gt;跳表 SkipList的实现原理&lt;/h1&gt;&lt;h2 id=&quot;XII-跳跃表（S</summary>
      
    
    
    
    <category term="算法" scheme="http://durantthorvalds.top/categories/%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="数据结构" scheme="http://durantthorvalds.top/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>「核心」Ceph学习三部曲之二:CEPH 纠删码操作&amp;API</title>
    <link href="http://durantthorvalds.top/2020/11/26/ceph%E7%BA%A0%E5%88%A0%E7%A0%81%E9%83%A8%E7%BD%B2/"/>
    <id>http://durantthorvalds.top/2020/11/26/ceph%E7%BA%A0%E5%88%A0%E7%A0%81%E9%83%A8%E7%BD%B2/</id>
    <published>2020-11-25T16:00:00.000Z</published>
    <updated>2021-01-23T12:18:53.190Z</updated>
    
    <content type="html"><![CDATA[<div class="note note-primary">            <p>本blog包括理论和实践两个部分，力求深入浅出，实践部分需要您事先部署成功Ceph集群！</p><p>参考《Ceph设计与实现》谢型果等，第三章。以及<a href="https://docs.ceph.com/en/latest/rados/operations/erasure-code/">官方纠删码教程</a>。</p>          </div><h1 id="Part-I"><a href="#Part-I" class="headerlink" title="Part I"></a>Part I</h1><h2 id="Ceph-纠删码操作"><a href="#Ceph-纠删码操作" class="headerlink" title="Ceph 纠删码操作"></a>Ceph 纠删码操作</h2><hr><h2 id="术语"><a href="#术语" class="headerlink" title="术语"></a>术语</h2><p>K ——数据块数。</p><p>M——编码块数。</p><p>N——条带中块的个数，$N=K+M$。</p><p>块（chunk）——将对象基于纠删码进行编码时，每次编码将产生若干大小的块（要求是有序的），Ceph通过数量相等的PG将这些块分别存储至不同的OSD之中。每次编码时，序号相同的块总是由同一个PG负责存储。</p><p>条带（stripe）——如果待编码的对象太大，无法一次性完成，那么可以分成多次进行，每次完成编码的部分称为一个条带。其大小为k*块大小。</p><p>分片（shard）——同一个对象所有序号相同的块位于同一个PG之上，它们组成的对象的一个分片。</p><p>rate——空间利用率，即$K/N$</p><h2 id="1-Ceph纠删码库"><a href="#1-Ceph纠删码库" class="headerlink" title="1 Ceph纠删码库"></a>1 Ceph纠删码库</h2><p>Ceph的默认纠删码库是Jerasure，即Jerasure库；除此之外还有 Clay, ISA-L, LRC, Shec(Octopus版本15.2.5).<br>当管理员创建一个erasure-coded后端时，可以指定数据块和代码块参数。Jerasure库是第三方提供的中间件。Ceph环境安装时，已经默认安装了Jerasure库。</p><h2 id="1-1-一个简单的纠删码池样例"><a href="#1-1-一个简单的纠删码池样例" class="headerlink" title="1.1 一个简单的纠删码池样例"></a>1.1 一个简单的纠删码池样例</h2><p>最简单的erasure pool等效于RAID5，至少需要3个主机：【更多关于池的操作可以参考《Ceph pool》博文】</p><pre><code class="hljs routeros">$ ceph osd<span class="hljs-built_in"> pool </span>create ecpool erasurepool <span class="hljs-string">&#x27;ecpool&#x27;</span> created$ echo ABCDEFGHI | rados --pool ecpool put NYAN -$ rados --pool ecpool <span class="hljs-builtin-name">get</span> NYAN -ABCDEFGHI</code></pre><h2 id="1-2-获取纠删码配置文件"><a href="#1-2-获取纠删码配置文件" class="headerlink" title="1.2 获取纠删码配置文件"></a>1.2 获取纠删码配置文件</h2><p>最简单的 k = 2, m = 2, 允许两个节点同时失效。相当于三副本，但是空间节省了。</p><pre><code class="hljs routeros">$ ceph osd erasure-code-profile <span class="hljs-builtin-name">get</span> default<span class="hljs-attribute">k</span>=2<span class="hljs-attribute">m</span>=2<span class="hljs-attribute">plugin</span>=jerasure<span class="hljs-attribute">crush-failure-domain</span>=host<span class="hljs-attribute">technique</span>=reed_sol_van</code></pre><p>选择正确的配置文件很重要，因为在创建池之后无法对其进行修改：需要创建具有不同配置文件的新池，并且将先前池中的所有对象都移到新的池中。</p><p>概要文件的最重要参数是<em>K</em>，<em>M</em>和 <em>rush-failure域，</em>因为它们定义了存储开销和数据持久性。例如，如果所需的架构必须承受两个机架的损失，而存储开销(m/k*100%)为开销的67％，则可以定义以下配置文件：</p><pre><code class="hljs routeros">$ ceph osd erasure-code-profile <span class="hljs-builtin-name">set</span> myprofile \   <span class="hljs-attribute">k</span>=3 \   <span class="hljs-attribute">m</span>=2 \   <span class="hljs-attribute">crush-failure-domain</span>=rack$ ceph osd<span class="hljs-built_in"> pool </span>create ecpool 128 erasure myprofile #128在这类是PG的数量$ echo ABCDEFGHI | rados --pool ecpool put NYAN -$ rados --pool ecpool <span class="hljs-builtin-name">get</span> NYAN -ABCDEFGHI</code></pre><p>该<em>NYAN</em>对象将在三个（被划分<em>K = 3</em>）和两个附加 <em>的块</em>将被创建（<em>M = 2</em>）。<em>M</em>的值定义了在不丢失任何数据的情况下可以同时丢失多少个OSD。所述<code>crush-failure-domain=rack</code>将创建一个CRUSH规则，以确保没有两个<code>chunks</code>被存储在同一个机架。</p><pre><code class="hljs vim">ceph osd erasure-code-<span class="hljs-keyword">profile</span> <span class="hljs-keyword">ls</span> #显示所有<span class="hljs-keyword">profile</span>ceph osd erasure-code-<span class="hljs-keyword">profile</span> rm &#123;profilr&#125; #删除特定<span class="hljs-keyword">profile</span></code></pre><p>读写文件test.txt</p><pre><code class="hljs cmake">rados -p ecpool put <span class="hljs-keyword">test</span> <span class="hljs-keyword">test</span>.txtrados -p ecpool get <span class="hljs-keyword">test</span> <span class="hljs-keyword">file</span>.txt</code></pre><p>更多信息在 <a href="https://docs.ceph.com/en/latest/rados/operations/erasure-code-profile">erasure code profiles</a></p><p><img src="/img/nyan.png" alt="img"></p><p>上图展示的是一种最简单的情况，我们称之为“满条带写”，向k=3，m=2的纠删码存储池写入NYAN对象。针对同一个逻辑PG，将对象分片并写入不同的PG实例。每个PG实例都认为字节保存的是一个完整而独立的对象。因此其保存的内容在逻辑上是连续的，以块大小为单位。5个OSD最终都向名为“NYAN”的对象写入三个字节，它们在对象内的逻辑地址都为[0,2]。</p><h2 id="1-3-写覆盖"><a href="#1-3-写覆盖" class="headerlink" title="1.3 写覆盖"></a>1.3 写覆盖</h2><p>默认情况下，纠删码池仅适用于执行完整对象写入和追加的RGW之类的用途。</p><p>自从luminous版本，每个池设置启用对纠删码池的<strong>部分写</strong>入。这使RBD和CephFS将其数据存储在纠删码池中：</p><pre><code class="hljs routeros">ceph osd<span class="hljs-built_in"> pool </span><span class="hljs-builtin-name">set</span> ec_pool allow_ec_overwrites <span class="hljs-literal">true</span></code></pre><p>这是针对bluestore的osd，这是因为bluestore的校验和用于检测deep-scrub 期间的bitrot和其它损坏。除了不安全之外，overwrite还将降低性能。</p><p>纠删码不支持<code>omap</code>,因此需要和RBD和CephFS一起使用，必须指示它们将数据存储在ec池中，并将元数据存储在复制池中。对于RBD，这意味着<code>--data-pool</code>在图像创建过程中使用纠删码池：</p><pre><code class="hljs brainfuck"><span class="hljs-comment">rbd</span> <span class="hljs-comment">create</span> --<span class="hljs-comment">size</span> <span class="hljs-comment">1G</span> --<span class="hljs-comment">data</span><span class="hljs-literal">-</span><span class="hljs-comment">pool</span> <span class="hljs-comment">ec_pool</span> <span class="hljs-comment">replicated_pool/image_name</span><span class="hljs-comment"></span></code></pre><p>对于CephFS，可以在文件系统创建过程中或通过<a href="https://docs.ceph.com/en/latest/cephfs/file-layouts">文件布局</a>将纠删码池设置为默认数据池。</p><h2 id="1-4-缓存层"><a href="#1-4-缓存层" class="headerlink" title="1.4 缓存层"></a>1.4 缓存层</h2><p>纠删码比副本需要更多资源，并且缺失<code>omap</code>这样的功能。为了克服这些限制，需要设置一个<a href="https://docs.ceph.com/en/latest/rados/operations/cache-tiering">缓存层</a></p><pre><code class="hljs dsconfig">$ <span class="hljs-string">ceph </span><span class="hljs-string">osd </span><span class="hljs-string">tier </span><span class="hljs-string">add </span><span class="hljs-string">ecpool </span><span class="hljs-string">hot-storage</span><span class="hljs-string">$</span> <span class="hljs-string">ceph </span><span class="hljs-string">osd </span><span class="hljs-string">tier </span><span class="hljs-string">cache-mode </span><span class="hljs-string">hot-storage </span><span class="hljs-string">writeback</span><span class="hljs-string">$</span> <span class="hljs-string">ceph </span><span class="hljs-string">osd </span><span class="hljs-string">tier </span><span class="hljs-built_in">set-overlay</span> <span class="hljs-string">ecpool </span><span class="hljs-string">hot-storage</span></code></pre><p>将放置热存储池的ecpool 在<em>写回</em> 模式。提供灵活性和速度。</p><h2 id="1-5-恢复"><a href="#1-5-恢复" class="headerlink" title="1.5 恢复"></a>1.5 恢复</h2><p>如果纠删码池丢失了一些碎片，则必须从其他碎片中恢复它们。通常，这涉及读取其余分片，重建数据并将其写入新对等方。在Octopus中，只要至少有<em>K个</em>碎片可用，擦除编码池就可以恢复。（使用少于<em>K个分</em>片，您实际上已经丢失了数据！）</p><p>在使用Octopus之前，即使<em>min_size</em>大于<em>K</em>，擦除编码池也至少需要<em>min_size分</em>片可用。（我们通常建议min_size为<em>K + 2</em>或更大，以防止写入和数据丢失。）这种保守的决定是在设计新的池模式时出于谨慎考虑而做出的，但是这也意味着丢失OSD但没有数据丢失的池无法进行操作恢复并开始活动，而无需手动干预来更改<em>min_size</em>。</p><h2 id="2-OSD-erasure-code-profile-参数"><a href="#2-OSD-erasure-code-profile-参数" class="headerlink" title="2 OSD erasure-code-profile 参数"></a>2 OSD erasure-code-profile 参数</h2><p>通用</p><pre><code class="hljs routeros">ceph osd erasure-code-profile <span class="hljs-builtin-name">set</span> &#123;name&#125; \     [&#123;<span class="hljs-attribute">directory</span>=directory&#125;] \     [&#123;<span class="hljs-attribute">plugin</span>=plugin&#125;] \     [&#123;<span class="hljs-attribute">stripe_unit</span>=stripe_unit&#125;] \     [&#123;<span class="hljs-attribute">key</span>=value&#125; <span class="hljs-built_in">..</span>.] \     [--force]</code></pre><ul><li><p><code>&#123;directory&#125;:string</code></p><p>设置从中加载擦除代码插件的<strong>目录</strong>名称. 默认<code>/ usr / lib / ceph / erasure-code</code></p></li><li><p><code>crush-failure-domain=&#123;bucket-type&#125;</code></p><p>确保一个桶两个数据块没有相同的容灾域. 它被用于创建 CRUSH 规则 <strong>step chooseleaf host</strong>. 默认host。</p></li><li><p><code>crush-device-class=&#123;device-class&#125;</code></p><p>使用CRUSH映射中的Crush设备类名称，将布局限制为特定类（例如 <code>ssd</code>或<code>hdd</code>）的设备。</p></li><li><p><code>&#123;plugin&#125;:string</code></p><p>默认: <code>jerasure</code> ,  可选<code>isa\ lrc \shec\clay</code></p></li></ul><h2 id="2-1-jerasure"><a href="#2-1-jerasure" class="headerlink" title="2.1 jerasure"></a>2.1 jerasure</h2><pre><code class="hljs sql">ceph osd erasure-code-profile <span class="hljs-keyword">set</span> &#123;<span class="hljs-keyword">name</span>&#125; \     <span class="hljs-keyword">plugin</span>=jerasure \     k=&#123;<span class="hljs-keyword">data</span>-chunks&#125; \     m=&#123;coding-chunks&#125; \     technique=&#123;reed_sol_van|reed_sol_r6_op|cauchy_orig|cauchy_good|liberation|blaum_roth|liber8tion&#125; \     [crush-root=&#123;root&#125;] \     [crush-<span class="hljs-keyword">failure</span>-<span class="hljs-keyword">domain</span>=&#123;<span class="hljs-keyword">bucket</span>-<span class="hljs-keyword">type</span>&#125;] \     [crush-device-<span class="hljs-keyword">class</span>=&#123;device-<span class="hljs-keyword">class</span>&#125;] \     [<span class="hljs-keyword">directory</span>=&#123;<span class="hljs-keyword">directory</span>&#125;] \     [<span class="hljs-comment">--force]</span></code></pre><p>我们可以选择具体技术<code>technique</code>。更灵活的技术是<em>reed_sol_van</em>：足以设置<em>k</em>和<em>m</em>。该<em>cauchy_good</em>技术可以更快，但你需要选择的<em>PACKETSIZE</em> 小心。从只能使用<em>m = 2</em>进行配置的意义上来说，所有<em>reed_sol_r6_op</em>，<em>liberation</em>， <em>blaum_roth</em>，<em>liber8tion</em>都是<em>RAID6</em>等效项。</p><h2 id="2-2-ISA"><a href="#2-2-ISA" class="headerlink" title="2.2 ISA"></a>2.2 ISA</h2><pre><code class="hljs sql">ceph osd erasure-code-profile <span class="hljs-keyword">set</span> &#123;<span class="hljs-keyword">name</span>&#125; \     <span class="hljs-keyword">plugin</span>=isa \     technique=&#123;reed_sol_van|cauchy&#125; \     [k=&#123;<span class="hljs-keyword">data</span>-chunks&#125;] \     [m=&#123;coding-chunks&#125;] \     [crush-root=&#123;root&#125;] \     [crush-<span class="hljs-keyword">failure</span>-<span class="hljs-keyword">domain</span>=&#123;<span class="hljs-keyword">bucket</span>-<span class="hljs-keyword">type</span>&#125;] \     [crush-device-<span class="hljs-keyword">class</span>=&#123;device-<span class="hljs-keyword">class</span>&#125;] \     [<span class="hljs-keyword">directory</span>=&#123;<span class="hljs-keyword">directory</span>&#125;] \     [<span class="hljs-comment">--force]</span></code></pre><h2 id="2-3-LRC"><a href="#2-3-LRC" class="headerlink" title="2.3 LRC"></a>2.3 LRC</h2><pre><code class="hljs sql">ceph osd erasure-code-profile <span class="hljs-keyword">set</span> &#123;<span class="hljs-keyword">name</span>&#125; \     <span class="hljs-keyword">plugin</span>=lrc \     k=&#123;<span class="hljs-keyword">data</span>-chunks&#125; \     m=&#123;coding-chunks&#125; \     l=&#123;locality&#125; \     [crush-root=&#123;root&#125;] \     [crush-locality=&#123;<span class="hljs-keyword">bucket</span>-<span class="hljs-keyword">type</span>&#125;] \     [crush-<span class="hljs-keyword">failure</span>-<span class="hljs-keyword">domain</span>=&#123;<span class="hljs-keyword">bucket</span>-<span class="hljs-keyword">type</span>&#125;] \     [crush-device-<span class="hljs-keyword">class</span>=&#123;device-<span class="hljs-keyword">class</span>&#125;] \     [<span class="hljs-keyword">directory</span>=&#123;<span class="hljs-keyword">directory</span>&#125;] \     [<span class="hljs-comment">--force]</span></code></pre><p><em>LRC</em>创建本地校验块，使用更少的存活OSD。例如，如果<em>lrc</em>配置为 <em>k = 8</em>，<em>m = 4</em>和<em>l = 4</em>，它将为每4个OSD创建一个额外的奇偶校验块。当1个OSD丢失时，只能使用4个OSD（而不是8个）来恢复它。</p><h2 id="2-4-SHEC"><a href="#2-4-SHEC" class="headerlink" title="2.4 SHEC"></a>2.4 SHEC</h2><pre><code class="hljs sql">ceph osd erasure-code-profile <span class="hljs-keyword">set</span> &#123;<span class="hljs-keyword">name</span>&#125; \     <span class="hljs-keyword">plugin</span>=shec \     [k=&#123;<span class="hljs-keyword">data</span>-chunks&#125;] \     [m=&#123;coding-chunks&#125;] \     [c=&#123;durability-estimator&#125;] \     [crush-root=&#123;root&#125;] \     [crush-<span class="hljs-keyword">failure</span>-<span class="hljs-keyword">domain</span>=&#123;<span class="hljs-keyword">bucket</span>-<span class="hljs-keyword">type</span>&#125;] \     [crush-device-<span class="hljs-keyword">class</span>=&#123;device-<span class="hljs-keyword">class</span>&#125;] \     [<span class="hljs-keyword">directory</span>=&#123;<span class="hljs-keyword">directory</span>&#125;] \     [<span class="hljs-comment">--force]</span></code></pre><ul><li><p><code>c=&#123;durability-estimator&#125;:int</code></p><p>奇偶校验块的数量，每个奇偶校验块包括其计算范围内的每个数据块。该数字用作<strong>耐久性估算器</strong>。例如，如果c = 2，则2个OSD可以关闭而不会丢失数据。默认为2.</p></li></ul><h2 id="2-5-CLAY"><a href="#2-5-CLAY" class="headerlink" title="2.5 CLAY"></a>2.5 CLAY</h2><p>全称是coupled-layer.  此编码目标是在修复时减少网络带宽和磁盘IO。</p><p>令d为修复时沟通的OSD数量。比如Jerasure中k=8，m=4，修复1GiB数据</p><p>需要下载8GiB数据。</p><p>在clay中允许设置d， $k+1\le d\le k+m-1$。默认情况下$d=k+m-1$，这将最大化节省网络带宽和磁盘IO。比如 k = 8, m = 4, d = 11. <em>则</em>当单个OSD发生故障时，将沟通d = 11 osds并从每个插件中下载250MiB，导致总下载量为11 X 250MiB = 2.75GiB。下面提供了更多常规参数。当对存储量达到TB级的信息的机架进行维修时，好处是巨大的。</p><div class="table-container"><table><thead><tr><th style="text-align:left">Plugin</th><th style="text-align:left">磁盘IO总量</th></tr></thead><tbody><tr><td style="text-align:left">Jeraure</td><td style="text-align:left">$kS$</td></tr><tr><td style="text-align:left">Clay</td><td style="text-align:left">$dS/(d−k+1)=(k+m−1)S/m$</td></tr></tbody></table></div><p>其中<em>S</em>是正在修复的单个OSD上存储的数据量。在上表中，我们使用了<em>d</em>的最大可能值，因为这将导致从OSD故障恢复所需的最小数据下载量。</p><pre><code class="hljs sql">ceph osd erasure-code-profile <span class="hljs-keyword">set</span> &#123;<span class="hljs-keyword">name</span>&#125; \     <span class="hljs-keyword">plugin</span>=clay \     k=&#123;<span class="hljs-keyword">data</span>-chunks&#125; \     m=&#123;coding-chunks&#125; \     [d=&#123;helper-chunks&#125;] \     [scalar_mds=&#123;<span class="hljs-keyword">plugin</span>-<span class="hljs-keyword">name</span>&#125;] \     [technique=&#123;technique-<span class="hljs-keyword">name</span>&#125;] \     [crush-<span class="hljs-keyword">failure</span>-<span class="hljs-keyword">domain</span>=&#123;<span class="hljs-keyword">bucket</span>-<span class="hljs-keyword">type</span>&#125;] \     [<span class="hljs-keyword">directory</span>=&#123;<span class="hljs-keyword">directory</span>&#125;] \     [<span class="hljs-comment">--force]</span></code></pre><ul><li><p><code>d=&#123;helper-chunks&#125;</code></p><p>恢复单个块期间请求发送数据的OSD数量。需要选择<em>d</em>，以使k + 1 &lt;= d &lt;= k + m-1。在较大的<em>d</em>，节省越多。默认 k + m -1.</p></li><li><p><code>scalar_mds=&#123;jerasure|isa|shec&#125;</code></p><p><strong>scalar_mds</strong>指定在分层构造中用作构建块的插件。可以是<em>jerasure</em>，<em>isa</em>，<em>shec之一</em></p></li><li><p><code>technique=&#123;technique&#125;</code></p><p><strong>technique</strong>指定将在指定的“ scalar_mds”插件中采用的技术。支持的技术是’reed_sol_van’，’reed_sol_r6_op’，’cauchy_orig’，’cauchy_good’，’liber8tion’用于jerasure，’reed_sol_van’，’cauchy’用于isa和’single’，’multiple’用于shec。</p><p>默认reed_sol_van (for jerasure, isa), single (for shec)</p></li></ul><blockquote><h2 id="MORE"><a href="#MORE" class="headerlink" title="MORE"></a>MORE</h2><p>Clay代码是矢量代码，因此能够节省磁盘IO和网络带宽，并且能够以称为子块的更精细的粒度查看和操作块中的数据。Clay代码的块中子块的数量由下式给出：</p><blockquote><p>子块计数= $q^{(k+m)/q}$， $q=d−k+1$</p></blockquote><p>在OSD修复期间，从可用OSD请求的帮助者信息只是块的一小部分。实际上，修复期间访问的块内子块的数量由下式给出：</p><blockquote><p>修复子块计数= $\frac{sub—-chunk \: count}{q}$</p></blockquote><h3 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h3><ol><li>对于<em>k = 4</em>，<em>m = 2</em>，<em>d = 5的配置</em>，子块计数为8，修复子块计数为4。因此，在修复期间仅读取一半的块。</li><li>当<em>k = 8</em>，<em>m = 4</em>，<em>d = 11时</em>，子块计数为64，修复子块计数为16。从可用OSD中读取四分之一的块以修复故障块。</li></ol><h2 id="如何在给定工作量的情况下选择配置"><a href="#如何在给定工作量的情况下选择配置" class="headerlink" title="如何在给定工作量的情况下选择配置"></a>如何在给定工作量的情况下选择配置</h2><p>块中所有子块中只有几个子块被读取。这些子块不必连续存储在块中。为了获得最佳的磁盘IO性能，读取连续的数据很有帮助。因此，建议您选择条带大小，以使子块大小足够大。</p><p>对于给定的条带大小（这是基于固定的工作负载），选择<code>k</code>，<code>m</code>，<code>d</code>使得：</p><blockquote><p>子块大小= $\frac{stripe-size}{k sub-chunk count}$ = 4KB，8KB，12KB…</p></blockquote><ol><li>对于条带大小较大的大型工作负载，很容易选择k，m，d。例如，考虑大小为64MB的条带大小，选择<em>k = 16</em>，<em>m = 4</em>和<em>d = 19</em>将导致子块计数为1024，子块大小为4KB。</li><li>对于较小的工作负载，<em>k = 4</em>，<em>m = 2</em>是一个很好的配置，可同时带来网络和磁盘IO的好处。</li></ol><h2 id="与LRC的比较"><a href="#与LRC的比较" class="headerlink" title="与LRC的比较"></a>与LRC的比较</h2><p>还设计了本地可恢复代码（LRC），以便在网络带宽方面节省单个OSD恢复期间的磁盘IO。但是，LRC的重点是使修复（d）期间接触的OSD数量保持最少，但这是以存储开销为代价的。clay代码有一个存储开销 m/k。在<em>lrc</em>的情况下，除奇偶校验外，它还存储（k + m）/ d个奇偶<code>m</code>校验，从而导致存储开销（m +（k + m）/ d）/ k。两个<em>粘土</em>和<em>LRC</em> 可以从任何的故障中恢复<code>m</code>的OSD。</p><blockquote><div class="table-container"><table><thead><tr><th style="text-align:left">参量</th><th style="text-align:left">磁盘IO，存储开销（LRC）</th><th style="text-align:left">磁盘IO，存储开销（CLAY）</th></tr></thead><tbody><tr><td style="text-align:left">（k = 10，m = 4）</td><td style="text-align:left">7 * S，0.6（d = 7）</td><td style="text-align:left">3.25 * S，0.4（d = 13）</td></tr><tr><td style="text-align:left">（k = 16，m = 4）</td><td style="text-align:left">4 * S，0.5625（d = 4）</td><td style="text-align:left">4.75 * S，0.25（d = 19）</td></tr></tbody></table></div></blockquote><p><code>S</code>是恢复单个OSD的存储数据量。</p></blockquote><h2 id="覆盖写思考"><a href="#覆盖写思考" class="headerlink" title="覆盖写思考"></a>覆盖写思考</h2><p>因为数据更新必须以条带为单位进行，如果覆盖写的起始或者结束位置没有进行条带对齐，那么不足一个完整条带的部分，其写入只能通过“读取完整条带→修改数据→基于条带重新计算校验数据→写入（被修改部分和校验和）”。这个过程被称为RMW。</p><p>整个RMW过程补齐读阶段最耗时，由两种解决思路：1. 减少RMW次数，2.如果RMW不可避免，那么尽量减少补齐读的数据量。一种常见的做法是引入写缓存。将驻留于缓存的写操作进行合并。 另外是尽可能减少读的次数，基于被修改写的数据范围预先计算出需要执行补齐读的块，而不是每次都执行满条带写。</p><h2 id="Scrub的问题"><a href="#Scrub的问题" class="headerlink" title="Scrub的问题"></a>Scrub的问题</h2><p>Scrub指数据扫描，通过读取对象数据并重新计算校验和，再与之前存储在对象属性的校验和进行比对，以判断有无静默错误（磁盘自身无法感知的错误）。目前Ceph纠删码没有自动修复功能。其中Scrub只扫描元数据，而Deep Scrub对对象整体进行扫描。</p><p>例如对象大小为4MB，那么每4KB原始数据采用CRC32生成固定四个字节的校验和，则整个对象的校验和最大只能是4KB，这显然无法直接使用对象扩展属性存储，而只能使用对象的omap存储(kv pairs)，但是纠删码目前不支持omap! </p><p>Ceph中纠删码一直未达到商业水平，无外乎以下几个原因：</p><ul><li>相较于多副本，纠删码实现更复杂</li><li>相较于多副本，纠删码性能较差，尤其是读性能。其最适合的场景一般是追加写或者删除。</li></ul><p>这也是笔者的研究方向，路漫漫其修远兮，吾将上下而求索。</p><hr><h1 id="Part-II"><a href="#Part-II" class="headerlink" title="Part II"></a>Part II</h1><h2 id="纠删码库介绍"><a href="#纠删码库介绍" class="headerlink" title="纠删码库介绍"></a>纠删码库介绍</h2><h2 id="1-Jerasure"><a href="#1-Jerasure" class="headerlink" title="1 Jerasure"></a>1 Jerasure</h2><p>Jerasure支持水平的纠删码模式，我</p><p>2007 James给出了RS-RAID一个开源实现。<a href="http://jerasure.org/jerasure/jerasure">official</a> ，<a href="https://github.com/tsuraan/Jerasure">github</a></p><p>Jerasure库包括以下5个模块：</p><ul><li><code>galois.h/.c</code>提供伽罗华域的算术运算。</li><li><code>jerasure.h/.c</code>提供了绝大部分的核心函数。包括矩阵的编解码，位矩阵变换，矩阵转置和位矩阵转置。</li><li><code>reedsol.h/.c</code>支持RS编解码和优化和的RS码。</li><li><code>caucy.h/.c</code> 支持Caucy RS编解码和最优Caucy编码。</li><li><code>caucy_best_r6.h/.c</code> 基于Caucy矩阵的RAID-6优化。</li><li><code>liberation.h/.c</code> 支持Liberartion RAID-6编码, Blaum-Roth 编码，和Liber8tion RAID-6编码。 Liberation是一种低密度MDS。这三种编码采用位矩阵来实现，其性能远优于现有的RS和EVENNODD，在某种情况下也优于RDP编码。</li></ul><p>下标汇总了Jerasure-2.0库常见的参数和其含义：</p><div class="table-container"><table><thead><tr><th style="text-align:center">参数名</th><th style="text-align:center">含义</th></tr></thead><tbody><tr><td style="text-align:center">k</td><td style="text-align:center">数据盘个数</td></tr><tr><td style="text-align:center">m</td><td style="text-align:center">校验盘个数</td></tr><tr><td style="text-align:center">w</td><td style="text-align:center">字长</td></tr><tr><td style="text-align:center">packetsize</td><td style="text-align:center">包大小</td></tr><tr><td style="text-align:center">size</td><td style="text-align:center">每个盘待编码或者解码的字节数</td></tr><tr><td style="text-align:center">matrix</td><td style="text-align:center">编码矩阵</td></tr><tr><td style="text-align:center">bitmatrix</td><td style="text-align:center">位矩阵</td></tr><tr><td style="text-align:center">dataptrs</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">erasures</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">erased</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">schdule</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">row k ones</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">dm ids</td></tr></tbody></table></div><p>针对RAID-6，Jerasure作了两类优化：</p><ul><li><p>对乘2运算进行优化</p></li><li><p>直接对编码矩阵进行改造，得到最小密度RAID6. 这类矩阵很稀疏，计算量较少。目前有三种</p><ul><li>Liberation: 要求W必须是素数</li><li>Blaum-Roth: 要求W+1必须是素数</li><li>Lber8tion: 要求W必须等于8</li></ul><p>这三种编码效率相当。</p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;div class=&quot;note note-primary&quot;&gt;
            &lt;p&gt;本blog包括理论和实践两个部分，力求深入浅出，实践部分需要您事先部署成功Ceph集群！&lt;/p&gt;&lt;p&gt;参考《Ceph设计与实现》谢型果等，第三章。以及&lt;a href=&quot;https://d</summary>
      
    
    
    
    <category term="ceph" scheme="http://durantthorvalds.top/categories/ceph/"/>
    
    <category term="纠删码" scheme="http://durantthorvalds.top/categories/ceph/%E7%BA%A0%E5%88%A0%E7%A0%81/"/>
    
    
    <category term="ceph" scheme="http://durantthorvalds.top/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title>「核心」Ceph学习三部曲之一:A First Glance At CRUSH</title>
    <link href="http://durantthorvalds.top/2020/11/25/A%20first%20glance%20at%20CRUSH/"/>
    <id>http://durantthorvalds.top/2020/11/25/A%20first%20glance%20at%20CRUSH/</id>
    <published>2020-11-25T10:00:00.000Z</published>
    <updated>2021-01-09T03:28:02.837Z</updated>
    
    <content type="html"><![CDATA[<p>TODO: last update: 2020/12/28</p><div class="note note-primary">            <p>本blog包括理论和实践两个部分，实践部分需要您事先部署成功Ceph集群！</p><p>参考《Ceph设计与实现》谢型果等，第一章。以及<a href="https://docs.ceph.com/en/latest/rados/operations/crush-map/?s">官方CRUSH教程</a>。</p>          </div><h1 id="浅析CRUSH算法"><a href="#浅析CRUSH算法" class="headerlink" title="浅析CRUSH算法"></a>浅析CRUSH算法</h1><blockquote><p>CRUSH论文地址：<a href="https://ceph.com/wp-content/uploads/2016/08/weil-crush-sc06.pdf">https://ceph.com/wp-content/uploads/2016/08/weil-crush-sc06.pdf</a></p><p>一个通过增加额外时间维度来提升性能的方案，MapX（FAST2020‘）：<a href="https://www.usenix.org/system/files/fast20-wang_li.pdf">https://www.usenix.org/system/files/fast20-wang_li.pdf</a></p></blockquote><h1 id="1-CRUSH背景"><a href="#1-CRUSH背景" class="headerlink" title="1. CRUSH背景"></a>1. CRUSH背景</h1><p>大部分存储系统将数据写入到存储设备之后，数据很少在设备之间相互移动，这会导致一个潜在问题，即使是一个数据分布趋于完美的系统，随着时间的迁移，新的空闲设备不断加入，老设备不断退出，数据变得不均匀。<br>一种可行方案是将数据以足够小的粒度打散，均匀分布于整个存储系统，这样做有两个问题:</p><ul><li>如果设备数量发生变化， 如何最小化迁移量使得整个系统尽快恢复平衡。</li><li>在大型（PB以上）分布式存储系统，为保证数据可靠性一般采用多副本或者纠删码，如何合理分布它们。。</li></ul><p>CRUSH: Controlled Replication Under Scalable Hashing<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="WEIL, S. A., BRANDT, S. A., MILLER, E. L., AND MALTZAHN,C. Crush: Controlled, scalable, decentralized placement of replicateddata. In SC’06: Proceedings of the 2006 ACM/IEEE Conference onSupercomputing (2006), IEEE, pp. 31–31.">[1]</span></a></sup> 基于可扩展哈希的受控副本分布策略。<br>它基于伪随机数哈希算法，以数据唯一标识符、当前存储集群拓扑以及数据备份策略作为输入，可以随时随地通过计算获取数据所在的底层存储设备的位置并之间与其通信，从而避免查表，失效去中心化和高度并发。</p><p>按照Sage Weil系统讲解Ceph论文的说法，CRUSH其实解决了两个问题：一个是我应该把数据存在哪？另一个是我把数据存在哪了？[5.1节]</p><h2 id="1-1-故障恢复"><a href="#1-1-故障恢复" class="headerlink" title="1.1 故障恢复"></a>1.1 故障恢复</h2><p>OSD群集映射将因OSD故障，恢复和显式群集更改（例如，部署新存储）而更改。 为了促进快速恢复，OSD主要为每个对象保留一个版本号和每个PG的最近更改日志（已更新或删除的对象的名称和版本）。</p><p>   当活动的OSD收到更新的群集映射时，它将遍历所有本地存储的放置组，并计算CRUSH映射以确定它负责哪个映射（作为主副本或副本副本）。 如果PG的成员资格已更改，或者OSD刚刚启动，则OSD必须与PG的其他OSD进行$peer$。</p><p>   对于复制的PG，OSD向主数据库提供其当前PG版本号。 如果OSD是PG的主要版本，则OSD会收集当前（和以前）副本的PG版本。 如果主数据库缺少最新的PG状态，它将从PG中的当前OSD或先前的OSD检索最近PG更改的日志（或完整的内容摘要），以确定正确的（最新）PG内容。 然后主数据库向每个副本发送一次重要的日志更新（或完整的内容摘要），以便所有各方都知道PG的内容，即便本地存储的对象并不匹配。只有在主服务器确定正确的PG状态并与任何副本共享它之后，才允许对PG中的对象进行I / O。 然后，OSD将独立负责从其对等方检索丢失或过时的对象。 如果OSD收到对陈旧或丢失对象的请求，它将延迟处理并将该对象移到恢复队列的最前面。</p><p>   例如，假设osd1崩溃并被标记为down，而osd2接替pgA作为主要对象。 如果osd1恢复，它将在启动时请求最新的映射，并且监视器将其标记为已启动。 当osd2收到结果映射更新时，它将意识到它不再是pgA的主要版本，并将pgA版本号发送给osd1。</p><p>   osd1将从osd2检索最近的pgA日志条目，告诉osd2其内容是最新的，然后开始处理请求，同时在后台恢复任何更新的对象。<br>   由于故障恢复完全由单独的OSD驱动，因此受故障OSD影响的每个PG将与不同的替换OSD并行恢复。</p><h2 id="1-2-EBOFS"><a href="#1-2-EBOFS" class="headerlink" title="1.2 EBOFS"></a>1.2 EBOFS</h2><pre><code> POSIX接口不能支持原子数据和元数据（例如，属性）更新事务，这对于保持RADOS的一致性很重要。</code></pre><p>​    每个Ceph OSD都使用<strong>EBOFS</strong>（基于范围和B树的对象文件系统）来管理其本地对象存储。 完全在用户空间中实现EBOFS并直接与原始块设备进行交互，使我们能够定义自己的低级对象存储接口和更新语义，从而将更新序列化（用于同步）与磁盘提交（出于安全性）分开。</p><p>   EBOFS支持原子事务（例如，在多个对象上进行写和属性更新），并且当提供内存中的提交的异步通知时，更新功能在存储器中的高速缓存被更新时返回。</p><ul><li>避免了与Linux VFS和页面缓存的繁琐交互，这两者都是针对不同的界面和工作负载而设计的。</li><li>更容易地最终确定工作负载的优先级（例如，客户端I / O与恢复）或提供服务质量保证。</li><li>EBOFS可以在磁盘上的写入位置或相关数据附近快速定位可用空间，同时还可以限制长期碎片。</li><li>EBOFS积极地执行写时复制：除超级块(superblock)更新外，数据始终写入磁盘的未分配区域。</li></ul><h1 id="2-Straw选择算法"><a href="#2-Straw选择算法" class="headerlink" title="2. Straw选择算法"></a>2. Straw选择算法</h1><p>网络中不同层级具有不同容忍灾难的能力，称之为容灾域。</p><p><img src="/img/image-20201128210418154.png" alt="image-20201128210418154"></p><p>Sage weil一共设计了四种选择算法，并按照添加删除数据的性能进行比较。结论是：考虑到存储空间需求爆炸式增长，在大型分布式存储系统中某些部件故障是常态，以及数据性可靠性要求，Straw将是不错的选择。我们重点分析。</p><ul><li>straw算法将所有元素（设备）比作吸管，为每个元素随机计算一个长度，最后从中选择长度最长的那个元素作为结果输出，这个过程被形象地称为抽签（draw）。</li><li>CRUSH引入了权重（weight）来区分不同容量的设备。大容量设备理应获得更大的权重。设输入为x，元素编号为i，权重为w，随机数种子为r。每根“吸管”的长度是根据权重决定的,i.e.$f(w_i)$。</li></ul><script type="math/tex; mode=display">C(r,x) = max_i(f(w_i)hash(x,r,i))</script><blockquote><p>其实这里的x是输入PGID。<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="Wang L, Zhang Y, Xu J, et al. {MAPX}: Controlled Data Migration in the Expansion of Decentralized Object-Based Storage Systems[C]//18th {USENIX} Conference on File and Storage Technologies ({FAST} 20). 2020: 1-11.">[2]</span></a></sup>[2.1]节</p></blockquote><ul><li>当添加一个元素，straw会随机将一些原有元素中的数据随机映射至新加入的元素中；当删除一个元素x，straw会将全部数据重新映射到除x以外所有元素。</li></ul><p>当然straw也存在问题</p><ul><li>Straw算法将所有元素按权重逆序排列后逐个计算每个元素的Item_straw，会导致最终选择结果不断取决于每个元素自身权重还与集合当助其他元素强相关。因而会引起不相干的数据迁移。因而Sage Weil进行修正：在计算straw长度时仅使用元素自身的权重。从而得到straw改进算法straw2。</li></ul><p>原Straw算法：</p><pre><code class="hljs cpp">max_x = <span class="hljs-number">-1</span>max_item = <span class="hljs-number">-1</span><span class="hljs-keyword">for</span> each item:x = hash(input,r)x = x*item_straw<span class="hljs-keyword">if</span> x &gt; max_x:max_x = xmax_item = item<span class="hljs-keyword">return</span> max_item</code></pre><p>Straw2算法：</p><pre><code class="hljs properties"><span class="hljs-attr">max_x</span> = <span class="hljs-string">-1</span><span class="hljs-attr">max_item</span> = <span class="hljs-string">-1</span><span class="hljs-attr">for</span> <span class="hljs-string">each item:</span><span class="hljs-attr">x</span> = <span class="hljs-string">hash(input,r)</span><span class="hljs-attr">x</span> = <span class="hljs-string">ln(x/65536)/weight</span><span class="hljs-attr">if</span> <span class="hljs-string">x &gt; max_x:</span><span class="hljs-attr">max_x</span> = <span class="hljs-string">x</span><span class="hljs-attr">max_item</span> = <span class="hljs-string">item</span><span class="hljs-attr">return</span> <span class="hljs-string">max_item</span></code></pre><p>上述逻辑中，针对输入input和随机因子r执行哈希后，结果落在[0,65536]之间，x/65536必然小于1，取其自然对数ln(x/65536)后结果为负值，将其除以自身权重后，表现为权重越大，x越大，从而体现了我们所期望的每个元素对于抽签结果的正反馈作用。</p><h1 id="3-CRUSH算法"><a href="#3-CRUSH算法" class="headerlink" title="3 . CRUSH算法"></a>3 . CRUSH算法</h1><ul><li>针对特定输入x，CRUSH将输出一个包含n个不同存储对象的集合。我们称集群的拓扑为 Cluster Map , 不同数据分布是通过制定不同的placement rule实现的，它实际是一组包括最大副本数或纠删码策略、容灾级别的自定义约束条件。</li><li>x和cluster map和placement rule是CRUSH的哈希函数输入参数。因为使用伪随机哈希函数，CRUSH选择每个目标存储对象概率是相对独立的。</li></ul><h2 id="3-1-Cluster-Map"><a href="#3-1-Cluster-Map" class="headerlink" title="3.1 Cluster Map"></a>3.1 Cluster Map</h2><p><img src="/img/cluster_map.png" alt="image-20201128210954633"></p><p>实现上cluster map具有诸如 “数据中心→机架→主机→磁盘”这样的树状层次关系。每个叶子节点都是真实的物理设备（比如磁盘）称为device；所有的中间节点称为bucket；根节点称为root，是整个集群的入口。每个节点都用于唯一的数字ID和类型，但是只有叶子节点采用与非负ID。父节点的权重是所有孩子节点权重之和。</p><p>CRUSH放置策略可在故障域中分布对象副本，同时保持所需的分布。例如，为了解决并发故障的可能性，可能需要确保数据副本位于使用不同架子，机架，电源，控制器和/或物理位置的设备上。CRUSH算法通过（i）将故障域的信息（如共享电源或网络）编码到群集图中，并（ii）让管理员定义用于指定副本放置方式的放置规则，从而支持可靠约束副本放置的灵活约束。 通过递归选择存储桶项目。</p><p>常见的节点层级</p><ul><li><code>osd</code> (or <code>device</code>)</li><li><code>host</code></li><li><code>chassis</code></li><li><code>rack</code></li><li><code>row</code></li><li><code>pdu</code> 电源分配单元</li><li><code>pod</code></li><li><code>room</code></li><li><code>datacenter</code></li><li><code>zone</code></li><li><code>region</code></li><li><code>root</code></li></ul><p><img src="/img/cluster_mapdemo.png" alt="image-20201128211039048"></p><h2 id="3-2-数据分布策略——Placement-Rule"><a href="#3-2-数据分布策略——Placement-Rule" class="headerlink" title="3.2 数据分布策略——Placement Rule"></a>3.2 数据分布策略——Placement Rule</h2><p>CRUSH算法的核心包括三个步骤：TAKE，SELECT和EMIT。<br>$TAKE(a)$：从cluster_map选择指定编号的bucket并放入工作向量作为下一级SELECT的输入。系统默认采用root作为输入。</p><p><img src="/img/crush1.png" style="zoom:67%;" /></p><p>$SELECT(n,t)$:从bucket随机选择指定类型和数量的item。n: number，t: type。type可以设为 容灾域类型，比如rack 或host。Ceph当前支持两种备份策略——多副本和纠删码，相应的有两种选择方法first n 和 indep. 主要区别是纠删码要求结果是有序的。</p><ul><li><p>$f$在这里表示失败的尝试数，初始设为0.</p></li><li><p>$r$表示副本编号，它的范围是[1,n]。</p></li><li>CRUSH采用深度优先搜索方式遍历所有副本。</li></ul><p><img src="/img/crush2.png" style="zoom:67%;" /></p><p>我们下面再看一下选择算法。</p><p><img src="/img/crush4.png" alt="image-20201129194851670"></p><ul><li>（左图）first n：比如 n = 6， select(6, disk), 当第二个item被拒绝，其余节点会填充空位。尝试次数f 将更新 副本编号r。</li><li>（右图）每一个队列有概率上独立的顺序，这儿$f_r = 1, r’=r+f_rn=8$，对应device:h，因此会用h进行“填充”。</li></ul><p>因为在容灾域模式下会产生递归调用，所以还需要限制产生递归调用时作为下一级输入的全局尝试次数（<code>choose_total_tries</code>），因为这个限制会导致递归调用时全局尝试次数成倍增长，按照递归的概念，多次递归后这个全局尝试次数应该成指数增长，但是实际上至多调用一次，所以这里是将原始尝试次数放大N倍后作为下一级输入的全局尝试数，实现上采用一个布尔变量（<code>chooseleaf_descent_once</code>,i.e. “first n”）进行控制，如果为真，则在产生递归调用至多重试一次，否则则不进行重试，由调用者自身进行重试。N由<code>chooseleaf_vary_r</code>进行决定。</p><p><img src="/img/crush5.png" alt="image-20201129195221719" style="zoom:67%;" /></p><p>这儿的$b.c(r’,x)$即为第二节谈到的 Bucket Choose ， 我们采用Straw2算法。<br>如果得到的结果不是目标类型，则继续向下递归。并设置重试标记$retry_bucket$为true.</p><p><img src="/img/crush6.png" alt="image-20201129195537731" style="zoom:67%;" /></p><p>当输出已经在输出条目中或者发生冲突或过载时，如果$f_r\ge 3$会执行29行。冲突（Collision）： 选中的条目已经存在于输出条目列表之中。<br>OSD过载或失效：</p><ol><li>由于集群规模较小，导致集群PG总数有限，CRUSH输入不够。</li><li>CRUSH本身缺陷，每次选择是单个条目被选中的独立概率，但是CRUSH所要求的副本策略使得针对同一个输入、多个副本直接的选择变成了条件概率。</li></ol><p>在老的CRUSH实现，为了避免每次回到初始输入的bucket下重试，可以在当前的bucket下直接进行重试。此时同样需要对局部尝试次数进行限制，称为(<code>choose_local_retries</code>)。</p><p>$Overload(o,x)$</p><p>除了由容量计算得到的真实权重之外，Ceph还设置了可以人工调整的权重（reweight）。算法正常选中一个OSD之后，最后还基于此reweight进行一次过载测试，如果测试失败，则将仍然拒绝该item。<br>我们可以通过设置reweight介于[0,0x10000]之间，如果为0就不能通过测试，为0x10000就是一定会通过测试。在实际应用中通过降低过载OSD或者增加空闲reweight都可以触发数据在OSD之间重新分布。并且可以区分暂时失效的OSD和永久失效的OSD。</p><p><img src="/img/reweight.png" alt="reweight" style="zoom:67%;" /></p><p>$EMIT$:输出最终选择结果给上级调用者并返回。</p><p><img src="/img/crush8.png" alt="image-20201129195731168" style="zoom:67%;" /></p><p>总结</p><p>我们以firstn为例展示从指定bucket查找指定数量item的过程。</p><p><img src="/img/crush9.png" alt="image-20201129195815305"></p><hr><h1 id="4-操作CRUSH"><a href="#4-操作CRUSH" class="headerlink" title="4 操作CRUSH"></a>4 操作CRUSH</h1><p>1 查看osd tree （含权重）</p><pre><code class="hljs dos">sudo ceph osd <span class="hljs-built_in">tree</span></code></pre><pre><code class="hljs lsl">ID  CLASS  WEIGHT   TYPE NAME       STATUS  REWEIGHT  PRI-AFF<span class="hljs-number">-1</span>         <span class="hljs-number">0.05846</span>  root <span class="hljs-section">default</span>                             <span class="hljs-number">-5</span>         <span class="hljs-number">0.01949</span>      host node3                            <span class="hljs-number">1</span>    ssd  <span class="hljs-number">0.01949</span>          osd<span class="hljs-number">.1</span>       up   <span class="hljs-number">1.00000</span>  <span class="hljs-number">1.00000</span><span class="hljs-number">-3</span>         <span class="hljs-number">0.01949</span>      host node4                            <span class="hljs-number">0</span>    ssd  <span class="hljs-number">0.01949</span>          osd<span class="hljs-number">.0</span>       up   <span class="hljs-number">1.00000</span>  <span class="hljs-number">1.00000</span><span class="hljs-number">-7</span>         <span class="hljs-number">0.01949</span>      host node5                            <span class="hljs-number">2</span>    ssd  <span class="hljs-number">0.01949</span>          osd<span class="hljs-number">.2</span>       up   <span class="hljs-number">1.00000</span>  <span class="hljs-number">1.00000</span></code></pre><p><code>ID</code>: 每个节点在集群唯一ID。<code>class</code>：每个节点的类别。<code>weight</code>:每个节点的权重。</p><p>2 查看整个集群空间利用率</p><pre><code class="hljs dos">sudo ceph osd df <span class="hljs-built_in">tree</span></code></pre><pre><code class="hljs angelscript">ID  CLASS  WEIGHT   REWEIGHT  SIZE    RAW USE  DATA     OMAP    META      AVAIL   %USE  VAR   PGS  STATUS  TYPE NAME     <span class="hljs-number">-1</span>         <span class="hljs-number">0.05998</span>         -  <span class="hljs-number">60</span> GiB  <span class="hljs-number">3.0</span> GiB  <span class="hljs-number">2.4</span> MiB  <span class="hljs-number">59</span> KiB   <span class="hljs-number">3.0</span> GiB  <span class="hljs-number">57</span> GiB  <span class="hljs-number">5.01</span>  <span class="hljs-number">1.00</span>    -          root <span class="hljs-keyword">default</span>  <span class="hljs-number">-5</span>         <span class="hljs-number">0.00999</span>         -  <span class="hljs-number">20</span> GiB  <span class="hljs-number">1.0</span> GiB  <span class="hljs-number">824</span> KiB  <span class="hljs-number">18</span> KiB  <span class="hljs-number">1024</span> MiB  <span class="hljs-number">19</span> GiB  <span class="hljs-number">5.01</span>  <span class="hljs-number">1.00</span>    -              host node3 <span class="hljs-number">1</span>    ssd  <span class="hljs-number">0.00999</span>   <span class="hljs-number">1.00000</span>  <span class="hljs-number">20</span> GiB  <span class="hljs-number">1.0</span> GiB  <span class="hljs-number">824</span> KiB  <span class="hljs-number">18</span> KiB  <span class="hljs-number">1024</span> MiB  <span class="hljs-number">19</span> GiB  <span class="hljs-number">5.01</span>  <span class="hljs-number">1.00</span>   <span class="hljs-number">81</span>      up          osd<span class="hljs-number">.1</span> <span class="hljs-number">-3</span>         <span class="hljs-number">0.01999</span>         -  <span class="hljs-number">20</span> GiB  <span class="hljs-number">1.0</span> GiB  <span class="hljs-number">824</span> KiB  <span class="hljs-number">17</span> KiB  <span class="hljs-number">1024</span> MiB  <span class="hljs-number">19</span> GiB  <span class="hljs-number">5.01</span>  <span class="hljs-number">1.00</span>    -              host node4 <span class="hljs-number">0</span>    ssd  <span class="hljs-number">0.01999</span>   <span class="hljs-number">1.00000</span>  <span class="hljs-number">20</span> GiB  <span class="hljs-number">1.0</span> GiB  <span class="hljs-number">824</span> KiB  <span class="hljs-number">17</span> KiB  <span class="hljs-number">1024</span> MiB  <span class="hljs-number">19</span> GiB  <span class="hljs-number">5.01</span>  <span class="hljs-number">1.00</span>   <span class="hljs-number">81</span>      up          osd<span class="hljs-number">.0</span> <span class="hljs-number">-7</span>         <span class="hljs-number">0.03000</span>         -  <span class="hljs-number">20</span> GiB  <span class="hljs-number">1.0</span> GiB  <span class="hljs-number">828</span> KiB  <span class="hljs-number">24</span> KiB  <span class="hljs-number">1024</span> MiB  <span class="hljs-number">19</span> GiB  <span class="hljs-number">5.01</span>  <span class="hljs-number">1.00</span>    -              host node5 <span class="hljs-number">2</span>    ssd  <span class="hljs-number">0.03000</span>   <span class="hljs-number">1.00000</span>  <span class="hljs-number">20</span> GiB  <span class="hljs-number">1.0</span> GiB  <span class="hljs-number">828</span> KiB  <span class="hljs-number">24</span> KiB  <span class="hljs-number">1024</span> MiB  <span class="hljs-number">19</span> GiB  <span class="hljs-number">5.01</span>  <span class="hljs-number">1.00</span>   <span class="hljs-number">81</span>      up          osd<span class="hljs-number">.2</span>                        TOTAL  <span class="hljs-number">60</span> GiB  <span class="hljs-number">3.0</span> GiB  <span class="hljs-number">2.4</span> MiB  <span class="hljs-number">61</span> KiB   <span class="hljs-number">3.0</span> GiB  <span class="hljs-number">57</span> GiB  <span class="hljs-number">5.01</span>                                   MIN/MAX VAR: <span class="hljs-number">1.00</span>/<span class="hljs-number">1.00</span>  STDDEV: <span class="hljs-number">0</span></code></pre><h2 id="4-1-Rules"><a href="#4-1-Rules" class="headerlink" title="4.1 Rules"></a>4.1 Rules</h2><p>查看集群的rules</p><pre><code class="hljs crmsh">sudo ceph osd crush <span class="hljs-keyword">rule</span> ls</code></pre><pre><code class="hljs ebnf"><span class="hljs-attribute">replicated_rule</span><span class="hljs-attribute">ecpool</span></code></pre><p>你也能打印出rules的细节</p><pre><code class="hljs crmsh">sudo ceph osd crush <span class="hljs-keyword">rule</span> dump</code></pre><pre><code class="hljs json">[    &#123;        <span class="hljs-attr">&quot;rule_id&quot;</span>: <span class="hljs-number">0</span>,        <span class="hljs-attr">&quot;rule_name&quot;</span>: <span class="hljs-string">&quot;replicated_rule&quot;</span>,        <span class="hljs-attr">&quot;ruleset&quot;</span>: <span class="hljs-number">0</span>,        <span class="hljs-attr">&quot;type&quot;</span>: <span class="hljs-number">1</span>,        <span class="hljs-attr">&quot;min_size&quot;</span>: <span class="hljs-number">1</span>,        <span class="hljs-attr">&quot;max_size&quot;</span>: <span class="hljs-number">10</span>,        <span class="hljs-attr">&quot;steps&quot;</span>: [            &#123;                <span class="hljs-attr">&quot;op&quot;</span>: <span class="hljs-string">&quot;take&quot;</span>,                <span class="hljs-attr">&quot;item&quot;</span>: <span class="hljs-number">-1</span>,                <span class="hljs-attr">&quot;item_name&quot;</span>: <span class="hljs-string">&quot;default&quot;</span>            &#125;,            &#123;                <span class="hljs-attr">&quot;op&quot;</span>: <span class="hljs-string">&quot;chooseleaf_firstn&quot;</span>,                <span class="hljs-attr">&quot;num&quot;</span>: <span class="hljs-number">0</span>,                <span class="hljs-attr">&quot;type&quot;</span>: <span class="hljs-string">&quot;host&quot;</span>            &#125;,            &#123;                <span class="hljs-attr">&quot;op&quot;</span>: <span class="hljs-string">&quot;emit&quot;</span>            &#125;        ]    &#125;,    &#123;        <span class="hljs-attr">&quot;rule_id&quot;</span>: <span class="hljs-number">1</span>,        <span class="hljs-attr">&quot;rule_name&quot;</span>: <span class="hljs-string">&quot;ecpool&quot;</span>,        <span class="hljs-attr">&quot;ruleset&quot;</span>: <span class="hljs-number">1</span>,        <span class="hljs-attr">&quot;type&quot;</span>: <span class="hljs-number">3</span>,        <span class="hljs-attr">&quot;min_size&quot;</span>: <span class="hljs-number">3</span>,        <span class="hljs-attr">&quot;max_size&quot;</span>: <span class="hljs-number">3</span>,        <span class="hljs-attr">&quot;steps&quot;</span>: [            &#123;                <span class="hljs-attr">&quot;op&quot;</span>: <span class="hljs-string">&quot;set_chooseleaf_tries&quot;</span>,                <span class="hljs-attr">&quot;num&quot;</span>: <span class="hljs-number">5</span>            &#125;,            &#123;                <span class="hljs-attr">&quot;op&quot;</span>: <span class="hljs-string">&quot;set_choose_tries&quot;</span>,                <span class="hljs-attr">&quot;num&quot;</span>: <span class="hljs-number">100</span>            &#125;,            &#123;                <span class="hljs-attr">&quot;op&quot;</span>: <span class="hljs-string">&quot;take&quot;</span>,                <span class="hljs-attr">&quot;item&quot;</span>: <span class="hljs-number">-1</span>,                <span class="hljs-attr">&quot;item_name&quot;</span>: <span class="hljs-string">&quot;default&quot;</span>            &#125;,            &#123;                <span class="hljs-attr">&quot;op&quot;</span>: <span class="hljs-string">&quot;chooseleaf_indep&quot;</span>,                <span class="hljs-attr">&quot;num&quot;</span>: <span class="hljs-number">0</span>,                <span class="hljs-attr">&quot;type&quot;</span>: <span class="hljs-string">&quot;host&quot;</span>            &#125;,            &#123;                <span class="hljs-attr">&quot;op&quot;</span>: <span class="hljs-string">&quot;emit&quot;</span>            &#125;        ]    &#125;]</code></pre><h2 id="4-2-Device-class"><a href="#4-2-Device-class" class="headerlink" title="4.2 Device class"></a>4.2 Device class</h2><p>可以通过以下命令设置device的类别（<code>hdd</code>, <code>ssd</code>, or <code>nvme</code>）</p><pre><code class="hljs dsconfig"><span class="hljs-string">sudo </span><span class="hljs-string">ceph </span><span class="hljs-string">osd </span><span class="hljs-string">crush </span><span class="hljs-built_in">set-device-class</span> &lt;<span class="hljs-string">class&gt;</span> &lt;<span class="hljs-string">osd-name&gt;</span> [...]</code></pre><p>可以通过以下命令更改为其它类别</p><pre><code class="hljs ruby">sudo ceph osd crush rm-device-<span class="hljs-class"><span class="hljs-keyword">class</span> &lt;osd-<span class="hljs-title">name</span>&gt; [...]</span></code></pre><p>如果我们想创建新的placement rule</p><pre><code class="hljs dsconfig"><span class="hljs-string">sudo </span><span class="hljs-string">ceph </span><span class="hljs-string">osd </span><span class="hljs-string">crush </span><span class="hljs-string">rule </span><span class="hljs-built_in">create-replicated</span> &lt;<span class="hljs-string">rule-name&gt;</span> &lt;<span class="hljs-string">root&gt;</span> &lt;<span class="hljs-string">failure-domain&gt;</span> &lt;<span class="hljs-string">class&gt;</span></code></pre><p>对于pool而言就是</p><pre><code class="hljs routeros">sudo ceph osd<span class="hljs-built_in"> pool </span><span class="hljs-builtin-name">set</span> &lt;pool-name&gt; crush_rule &lt;rule-name&gt;</code></pre><p>通过为使用中的每个仅包含该类设备的设备类创建一个“影子” CRUSH层次结构来实现设备类。然后，CRUSH规则可以在影子层次结构上分布数据。</p><pre><code class="hljs ada">sudo ceph osd crush tree <span class="hljs-comment">--show-shadow</span></code></pre><pre><code class="hljs lsl">ID  CLASS  WEIGHT   TYPE NAME         <span class="hljs-number">-2</span>    ssd  <span class="hljs-number">0.05846</span>  root <span class="hljs-section">default</span>~ssd  <span class="hljs-number">-6</span>    ssd  <span class="hljs-number">0.01949</span>      host node3~ssd <span class="hljs-number">1</span>    ssd  <span class="hljs-number">0.01949</span>          osd<span class="hljs-number">.1</span>     <span class="hljs-number">-4</span>    ssd  <span class="hljs-number">0.01949</span>      host node4~ssd <span class="hljs-number">0</span>    ssd  <span class="hljs-number">0.01949</span>          osd<span class="hljs-number">.0</span>     <span class="hljs-number">-8</span>    ssd  <span class="hljs-number">0.01949</span>      host node5~ssd <span class="hljs-number">2</span>    ssd  <span class="hljs-number">0.01949</span>          osd<span class="hljs-number">.2</span>     <span class="hljs-number">-1</span>         <span class="hljs-number">0.05846</span>  root <span class="hljs-section">default</span>      <span class="hljs-number">-5</span>         <span class="hljs-number">0.01949</span>      host node3     <span class="hljs-number">1</span>    ssd  <span class="hljs-number">0.01949</span>          osd<span class="hljs-number">.1</span>     <span class="hljs-number">-3</span>         <span class="hljs-number">0.01949</span>      host node4     <span class="hljs-number">0</span>    ssd  <span class="hljs-number">0.01949</span>          osd<span class="hljs-number">.0</span>     <span class="hljs-number">-7</span>         <span class="hljs-number">0.01949</span>      host node5     <span class="hljs-number">2</span>    ssd  <span class="hljs-number">0.01949</span>          osd<span class="hljs-number">.2</span></code></pre><h2 id="4-3-权重集"><a href="#4-3-权重集" class="headerlink" title="4.3 权重集"></a>4.3 权重集</h2><p>权重集使群集可以根据群集的详细信息（层次结构，池等）执行数值优化，以实现平衡分配。</p><p>支持两种类型的权重集，目前支持两种类型的weight set：</p><ul><li>Compat权重集，针对集群每个节点而设计的权重，具有良好的向后兼容性。</li><li>Per-pool权重集，针对数据池的权重集。</li></ul><h2 id="4-4-修改CRUSH-Map"><a href="#4-4-修改CRUSH-Map" class="headerlink" title="4.4 修改CRUSH Map"></a>4.4 修改CRUSH Map</h2><p>要在正在运行的群集的CRUSH映射中添加或移动OSD，请执行以下操作：</p><pre><code class="hljs sql">sudo ceph osd crush <span class="hljs-keyword">set</span> &#123;<span class="hljs-keyword">name</span>&#125; &#123;weight&#125; root=&#123;root&#125; [&#123;<span class="hljs-keyword">bucket</span>-<span class="hljs-keyword">type</span>&#125;=&#123;<span class="hljs-keyword">bucket</span>-<span class="hljs-keyword">name</span>&#125; ...]</code></pre><p>{name}指osd名称</p><h2 id="4-5-调整OSD权重¶"><a href="#4-5-调整OSD权重¶" class="headerlink" title="4.5 调整OSD权重¶"></a>4.5 调整OSD权重<a href="https://docs.ceph.com/en/latest/rados/operations/crush-map/#adjust-osd-weight">¶</a></h2><p>要在正在运行的群集的CRUSH映射中调整OSD的CRUSH权重，请执行以下操作：</p><pre><code class="hljs dust"><span class="xml">sudo ceph osd crush reweight </span><span class="hljs-template-variable">&#123;name&#125;</span><span class="xml"> </span><span class="hljs-template-variable">&#123;weight&#125;</span></code></pre><p>要从正在运行的群集的CRUSH映射中删除OSD，请执行以下操作：</p><pre><code class="hljs routeros">sudo ceph osd crush <span class="hljs-builtin-name">remove</span> &#123;name&#125;</code></pre><p>要在正在运行的集群的CRUSH映射中添加存储桶，请执行以下 命令：<code>sudo ceph osd crush add-bucket</code></p><pre><code class="hljs smali">sudo ceph osd crush<span class="hljs-built_in"> add-bucket </span>&#123;bucket-name&#125; &#123;bucket-type&#125;</code></pre><p>要将存储桶移动到CRUSH地图层次结构中的其他位置或位置，请执行以下操作：</p><pre><code class="hljs sqf">sudo ceph osd crush <span class="hljs-built_in">move</span> &#123;bucket-<span class="hljs-built_in">name</span>&#125; &#123;bucket-<span class="hljs-built_in">type</span>&#125;=&#123;bucket-<span class="hljs-built_in">name</span>&#125;, [...]</code></pre><p>要从CRUSH层次结构中删除存储桶，请执行以下操作：</p><pre><code class="hljs routeros">sudo ceph osd crush <span class="hljs-builtin-name">remove</span> &#123;bucket-name&#125;</code></pre><h2 id="4-6-创建一个Compat权重集"><a href="#4-6-创建一个Compat权重集" class="headerlink" title="4.6 创建一个Compat权重集"></a>4.6 创建一个Compat权重集</h2><p>要创建<em>兼容的权重集</em>：</p><pre><code class="hljs livecodeserver">sudo ceph osd crush weight-<span class="hljs-built_in">set</span> <span class="hljs-built_in">create</span>-compat</code></pre><p>兼容重量组的重量可以通过以下方式调整：</p><pre><code class="hljs applescript">sudo ceph osd crush weight-<span class="hljs-keyword">set</span> reweight-compat &#123;<span class="hljs-built_in">name</span>&#125; &#123;weight&#125;</code></pre><p>可以用以下方法删除：</p><pre><code class="hljs routeros">sudo ceph osd crush weight-<span class="hljs-builtin-name">set</span> rm-compat</code></pre><h2 id="4-7-创建per-pool的权重集"><a href="#4-7-创建per-pool的权重集" class="headerlink" title="4.7 创建per-pool的权重集"></a>4.7 创建per-pool的权重集</h2><p>要为特定池创建权重集，请执行以下操作：</p><pre><code class="hljs sql">sudo ceph osd crush weight-<span class="hljs-keyword">set</span> <span class="hljs-keyword">create</span> &#123;pool-<span class="hljs-keyword">name</span>&#125; &#123;<span class="hljs-keyword">mode</span>&#125;</code></pre><p>调整权重：</p><pre><code class="hljs applescript">sudo ceph osd crush weight-<span class="hljs-keyword">set</span> reweight &#123;pool-<span class="hljs-built_in">name</span>&#125; &#123;<span class="hljs-built_in">item</span>-<span class="hljs-built_in">name</span>&#125; &#123;weight [...]&#125;</code></pre><p>要列出现有的权重集，请执行以下操作：</p><pre><code class="hljs routeros">sudo ceph osd crush weight-<span class="hljs-builtin-name">set</span> ls</code></pre><p>要删除，请执行以下操作：</p><pre><code class="hljs applescript">sudo ceph osd crush weight-<span class="hljs-keyword">set</span> rm &#123;pool-<span class="hljs-built_in">name</span>&#125;</code></pre><h2 id="4-8-为副本创建规则"><a href="#4-8-为副本创建规则" class="headerlink" title="4.8 为副本创建规则"></a>4.8 为副本创建规则</h2><pre><code class="hljs dsconfig"><span class="hljs-string">sudo </span><span class="hljs-string">ceph </span><span class="hljs-string">osd </span><span class="hljs-string">crush </span><span class="hljs-string">rule </span><span class="hljs-built_in">create-replicated</span> &#123;<span class="hljs-string">name&#125;</span> &#123;<span class="hljs-string">root&#125;</span> &#123;<span class="hljs-string">failure-domain-</span><span class="hljs-string">type&#125;</span> [&#123;<span class="hljs-string">class&#125;</span>]</code></pre><h2 id="4-9-为纠删码创建规则"><a href="#4-9-为纠删码创建规则" class="headerlink" title="4.9 为纠删码创建规则"></a>4.9 为纠删码创建规则</h2><p>对于纠删码（EC）池，需要做出相同的基本决策：故障域是什么，层次结构中的哪个节点将数据放置在（通常为<code>default</code>）下，并且放置位置将限制为特定的设备类。但是，纠删码池的创建方式略有不同，因为需要根据所使用的删除代码仔细构建它们。因此，您必须在<em>纠删码配置文件中</em>包含此信息。使用配置文件创建池时，将根据该规则显式或自动创建CRUSH规则。</p><p>纠删码配置文件可以列出：</p><pre><code class="hljs vim">sudo ceph osd erasure-code-<span class="hljs-keyword">profile</span> <span class="hljs-keyword">ls</span></code></pre><p>查看某个特定配置</p><pre><code class="hljs vim">sudo ceph osd erasure-code-<span class="hljs-keyword">profile</span> <span class="hljs-built_in">get</span> &#123;<span class="hljs-keyword">profile</span>-name&#125;</code></pre><p>通常，绝对不要修改配置文件。而是在创建新池或为现有池创建新规则时创建并使用新配置文件。</p><p>感兴趣的纠删码配置文件属性为：</p><blockquote><ul><li><strong>rush-root</strong>：要在其下放置数据的CRUSH节点的名称[默认值：<code>default</code>]。</li><li><strong>rush-failure-domain</strong>：在其上分配擦除编码分片的CRUSH存储桶类型[默认值：<code>host</code>]。</li><li><strong>rush-device-class</strong>：放置数据的设备类[默认：无，表示使用了所有设备]。</li><li><strong>k</strong>和<strong>m</strong>（对于<code>lrc</code>插件，为<strong>l</strong>）：它们确定擦除代码分片的数量，影响最终的CRUSH规则。</li></ul></blockquote><p>定义配置文件后，您可以使用以下方法创建CRUSH规则：</p><pre><code class="hljs dsconfig"><span class="hljs-string">sudo </span><span class="hljs-string">ceph </span><span class="hljs-string">osd </span><span class="hljs-string">crush </span><span class="hljs-string">rule </span><span class="hljs-built_in">create-erasure</span> &#123;<span class="hljs-string">name&#125;</span> &#123;<span class="hljs-string">profile-name&#125;</span><span class="hljs-comment">#&#123;name&#125;为规则名称</span></code></pre><p>可以通过以下方式删除池未使用的规则：</p><pre><code class="hljs pgsql">sudo ceph osd crush <span class="hljs-keyword">rule</span> rm &#123;<span class="hljs-keyword">rule</span>-<span class="hljs-type">name</span>&#125;</code></pre><h2 id="4-10-自定义CRUSH规则♥"><a href="#4-10-自定义CRUSH规则♥" class="headerlink" title="4.10 自定义CRUSH规则♥"></a>4.10 自定义CRUSH规则♥</h2><blockquote><p><a href="https://docs.ceph.com/en/latest/rados/operations/crush-map-edits/">https://docs.ceph.com/en/latest/rados/operations/crush-map-edits/</a></p></blockquote><p>我们可以通过CLI来很方便的修改CRUSH各项配置，但是如果修改项目过多，而集群较多，直接编辑CRUSH map 会是更好的选择。在一些特殊情况下，比如集群HDD和SSD和NVME混合则必须单独配置CRUSH rules。</p><h3 id="（1）获取CRUSH-Map"><a href="#（1）获取CRUSH-Map" class="headerlink" title="（1）获取CRUSH Map"></a>（1）获取CRUSH Map</h3><pre><code class="hljs dts">sudo ceph osd getcrushmap -<span class="hljs-class">o </span>&#123;compilefilename&#125;</code></pre><p>之后我们将其解码为txt</p><pre><code class="hljs dts">crushtool -<span class="hljs-class">d </span>&#123;compilefilename&#125; -<span class="hljs-class">o </span>&#123;outputfilename&#125;.txt</code></pre><p>例如 </p><pre><code class="hljs css"><span class="hljs-selector-tag">crushtool</span> <span class="hljs-selector-tag">-d</span> <span class="hljs-selector-tag">mycrushmap</span> <span class="hljs-selector-tag">-o</span> <span class="hljs-selector-tag">mycrushmap</span><span class="hljs-selector-class">.txt</span></code></pre><p>典型的crush map如下:</p><p>它包含6个部分：</p><ol><li><strong>可调项：</strong> tunable</li><li><strong>设备：</strong>设备是存储数据的单个OSD。</li><li><strong>types</strong>：存储桶<code>types</code>定义在CRUSH层次结构中使用的存储桶的类型。存储桶由存储位置（例如，行，机架，机箱，主机等）及其分配的权重的分层聚合组成。</li><li><strong>存储桶：</strong>定义存储桶类型后，必须定义层次结构中的每个节点，其类型以及它包含的设备或其他节点。</li><li><strong>规则：</strong>规则定义有关数据如何在层次结构中的各个设备之间分配的策略。</li><li>choice_args <strong>：</strong> Choose_args是与层次结构关联的替代权重，这些权重已进行调整以优化数据放置。单个choose_args映射可以用于整个集群，也可以为每个单独的池创建一个映射。</li></ol><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> begin crush map</span>tunable choose_local_tries 0tunable choose_local_fallback_tries 0tunable choose_total_tries 50tunable chooseleaf_descend_once 1tunable chooseleaf_vary_r 1tunable chooseleaf_stable 1tunable straw_calc_version 1tunable allowed_bucket_algs 54<span class="hljs-meta">#</span><span class="bash"> devices</span>device 0 osd.0 class ssddevice 1 osd.1 class ssddevice 2 osd.2 class ssd<span class="hljs-meta">#</span><span class="bash"> types</span>type 0 osdtype 1 hosttype 2 chassistype 3 racktype 4 rowtype 5 pdutype 6 podtype 7 roomtype 8 datacentertype 9 zonetype 10 regiontype 11 root<span class="hljs-meta">#</span><span class="bash"> buckets</span>host node4 &#123;        id -3           # do not change unnecessarily        id -4 class ssd         # do not change unnecessarily        # weight 0.020        alg straw2        hash 0  # rjenkins1        item osd.0 weight 0.020&#125;...root default &#123;        id -1           # do not change unnecessarily        id -2 class ssd         # do not change unnecessarily        # weight 0.060        alg straw2        hash 0  # rjenkins1        item node4 weight 0.020        item node3 weight 0.010        item node5 weight 0.030&#125;<span class="hljs-meta">#</span><span class="bash">rules</span>rule replicated_rule &#123;        id 0        type replicated        min_size 1        max_size 10        step take default        step chooseleaf firstn 0 type host        step emit&#125;rule ecpool &#123;        id 1        type erasure        min_size 3        max_size 3        step set_chooseleaf_tries 5        step set_choose_tries 100        step take default        step chooseleaf indep 0 type host        step emit&#125;...<span class="hljs-meta">#</span><span class="bash"> choose_args</span>choose_args 4 &#123;  &#123;    bucket_id -1    weight_set [      [ 0.020 0.010 0.030 ]    ]  &#125;&#125;...<span class="hljs-meta">#</span><span class="bash"> end crush map</span></code></pre><p>我们对ruleset中参数稍作解释</p><p><code>step</code> 包括三个部分 take, chooseleaf, emit</p><ul><li>chooseleaf, 容灾域模式，可以替换为choose，即非容灾域模式。</li><li>firstn, 两种选择算法之一，参看前面理论部分，可以替换为indep.</li><li>0, 表示由具体的调用者指定输出的副本数，例如不同的pool可以使用同一套ruleset（拥有相同的备份策略），但是可以拥有不同的副本数。</li><li>type，对应chooseleaf操作，指示输出必须是分布在由本选项指定类型的、不同的bucket之下的叶子节点；对应choose操作，指示输出类型。</li></ul><p><code>set_chooseleaf_tries</code>: 容灾域下产生递归调用时的尝试次数。</p><p><code>set_choose_tries</code>: 非容灾域下产生递归调用的尝试次数。</p><p><code>min_size</code>: 如果池中的副本数少于此数量，则CRUSH将 <strong>不会</strong>选择此规则。</p><p><code>max_size</code>: 如果池中的副本数量超过此数量，则CRUSH将 <strong>不会</strong>选择此规则。</p><blockquote><p><code>firstn</code> 与 <code>indep</code></p><ul><li>描述</li></ul><p>控制在CRUSH映射中标记了项目（OSD）时CRUSH使用的替换策略。<strong>如果此规则将用于复制池，则应使用<code>firstn</code>；如果是擦除编码池，则应使用<code>indep</code></strong>。原因与先前选择的设备发生故障时它们的行为有关。假设您有一个PG存储在OSD 1、2、3、4、5上。然后3下降。在“ firstn”模式下，CRUSH只需将其计算调整为选择1和2，然后选择3，但发现它已关闭，因此它重试并选择4和5，然后继续选择一个新的OSD6。因此最终的CRUSH映射更改为1、2、3、4、5-&gt; 1、2、4、5、6。但是，如果要存储EC池，则意味着您只需更改映射到OSD 4、5和6的数据！因此，“独立”模式试图不这样做。相反，您可以期望它在选择失败的OSD 3时再次尝试并选择6，以进行以下最终转换：1，2，3，4，5-&gt; 1，2，6，4，5</p></blockquote><div class="note note-danger">            <p>重要 :给定的CRUSH规则可以分配给多个池，但是单个池不可能具有多个CRUSH规则。</p>          </div><h3 id="（2）修改并编译CRUSH-Map"><a href="#（2）修改并编译CRUSH-Map" class="headerlink" title="（2）修改并编译CRUSH Map"></a>（2）修改并编译CRUSH Map</h3><p>我们需要进行编译才能被ceph识别</p><pre><code class="hljs dts">crushtool - <span class="hljs-class">c </span>&#123;decompiled_filename&#125; -<span class="hljs-class">o </span>&#123;compiled_filename&#125;</code></pre><h3 id="（3）测试"><a href="#（3）测试" class="headerlink" title="（3）测试"></a>（3）测试</h3><p>例如我们打印出输入范围为[0,9]、副本数3，采用编号为0的ruleset映射的结果。</p><pre><code class="hljs brainfuck"><span class="hljs-comment">sudo</span> <span class="hljs-comment">crushtool</span> <span class="hljs-literal">-</span><span class="hljs-comment">i</span> <span class="hljs-comment">&#123;compiled_filename&#125;</span> --<span class="hljs-comment">test</span> --<span class="hljs-comment">min</span><span class="hljs-literal">-</span><span class="hljs-comment">x</span> <span class="hljs-comment">0</span> --<span class="hljs-comment">max</span><span class="hljs-literal">-</span><span class="hljs-comment">x</span> <span class="hljs-comment">9</span> --<span class="hljs-comment">num</span><span class="hljs-literal">-</span><span class="hljs-comment">rep</span> <span class="hljs-comment">3</span> --<span class="hljs-comment">ruleset</span> <span class="hljs-comment">0\</span><span class="hljs-comment"></span> --<span class="hljs-comment">show_mappings</span></code></pre><pre><code class="hljs angelscript">CRUSH rule <span class="hljs-number">0</span> x <span class="hljs-number">0</span> [<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">2</span>]CRUSH rule <span class="hljs-number">0</span> x <span class="hljs-number">1</span> [<span class="hljs-number">2</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]CRUSH rule <span class="hljs-number">0</span> x <span class="hljs-number">2</span> [<span class="hljs-number">2</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]CRUSH rule <span class="hljs-number">0</span> x <span class="hljs-number">3</span> [<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>]CRUSH rule <span class="hljs-number">0</span> x <span class="hljs-number">4</span> [<span class="hljs-number">2</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>]CRUSH rule <span class="hljs-number">0</span> x <span class="hljs-number">5</span> [<span class="hljs-number">0</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>]CRUSH rule <span class="hljs-number">0</span> x <span class="hljs-number">6</span> [<span class="hljs-number">2</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]CRUSH rule <span class="hljs-number">0</span> x <span class="hljs-number">7</span> [<span class="hljs-number">2</span>,<span class="hljs-number">1</span>,<span class="hljs-number">0</span>]CRUSH rule <span class="hljs-number">0</span> x <span class="hljs-number">8</span> [<span class="hljs-number">2</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]CRUSH rule <span class="hljs-number">0</span> x <span class="hljs-number">9</span> [<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">0</span>]</code></pre><p>也可以仅统计结果分布情况，输入变为[0,100000]</p><pre><code class="hljs brainfuck"><span class="hljs-comment">sudo</span> <span class="hljs-comment">crushtool</span> <span class="hljs-literal">-</span><span class="hljs-comment">i</span> <span class="hljs-comment">mycrushmap</span> --<span class="hljs-comment">test</span> --<span class="hljs-comment">min</span><span class="hljs-literal">-</span><span class="hljs-comment">x</span> <span class="hljs-comment">0</span> --<span class="hljs-comment">max</span><span class="hljs-literal">-</span><span class="hljs-comment">x</span> <span class="hljs-comment">100000</span> --<span class="hljs-comment">num</span><span class="hljs-literal">-</span><span class="hljs-comment">rep</span> <span class="hljs-comment">3\</span><span class="hljs-comment"></span> --<span class="hljs-comment">ruleset</span> <span class="hljs-comment">0</span> --<span class="hljs-comment">show_utilization</span></code></pre><pre><code class="hljs yaml"><span class="hljs-string">rule</span> <span class="hljs-number">0</span> <span class="hljs-string">(replicated_rule),</span> <span class="hljs-string">x</span> <span class="hljs-string">=</span> <span class="hljs-number">0</span><span class="hljs-string">..100000,</span> <span class="hljs-string">numrep</span> <span class="hljs-string">=</span> <span class="hljs-number">3</span><span class="hljs-string">..3</span><span class="hljs-string">rule</span> <span class="hljs-number">0</span> <span class="hljs-string">(replicated_rule)</span> <span class="hljs-string">num_rep</span> <span class="hljs-number">3</span> <span class="hljs-string">result</span> <span class="hljs-string">size</span> <span class="hljs-string">==</span> <span class="hljs-attr">2:</span><span class="hljs-number">3</span><span class="hljs-string">/100001</span><span class="hljs-string">rule</span> <span class="hljs-number">0</span> <span class="hljs-string">(replicated_rule)</span> <span class="hljs-string">num_rep</span> <span class="hljs-number">3</span> <span class="hljs-string">result</span> <span class="hljs-string">size</span> <span class="hljs-string">==</span> <span class="hljs-attr">3:</span><span class="hljs-number">99998</span><span class="hljs-string">/100001</span>  <span class="hljs-attr">device 0:</span> <span class="hljs-attr">stored :</span> <span class="hljs-number">100001</span> <span class="hljs-attr">expected :</span> <span class="hljs-number">100001</span>  <span class="hljs-attr">device 1:</span> <span class="hljs-attr">stored :</span> <span class="hljs-number">99998</span> <span class="hljs-attr">expected :</span> <span class="hljs-number">100001</span>  <span class="hljs-attr">device 2:</span> <span class="hljs-attr">stored :</span> <span class="hljs-number">100001</span> <span class="hljs-attr">expected :</span> <span class="hljs-number">100001</span></code></pre><p>需要注意的是，除了叶子节点，其它的层级均为虚拟的，例如下面这个例子，我们让所有副本都必须分布在编号为0,1,2者三个特定的osd上。</p><p>可以通过如下语句声明存储桶：</p><pre><code class="hljs clojure">[bucket-type] [bucket-name] &#123;        id [a unique negative numeric ID]        weight [the relative capacity/capability of the item(<span class="hljs-name">s</span>)]        alg [the bucket type: uniform | list | tree | straw | straw2 ]        hash [the hash type: <span class="hljs-number">0</span> by default]        item [item-name] weight [weight]&#125;</code></pre><pre><code class="hljs routeros">vim mycrushmap.txthost virtualhost&#123;id -14#weight 3.00alg straw2hash 0 # rejenkins1item osd.0 weight 1.00item osd.1 weight 1.00item osd.2 weight 1.00&#125;<span class="hljs-comment">#rules</span>rule customized_ruleset&#123;ruleset 1<span class="hljs-built_in">type </span>replicated min_size 1max_size 10<span class="hljs-keyword">step</span> take virtualhost<span class="hljs-keyword">step</span> chooseleaf firstn 0<span class="hljs-built_in"> type </span>osd<span class="hljs-keyword">step</span> emit&#125;</code></pre><h3 id="（4）注入集群"><a href="#（4）注入集群" class="headerlink" title="（4）注入集群"></a>（4）注入集群</h3><pre><code class="hljs dts">sudo ceph osd setcrushmap -<span class="hljs-class">i </span>&#123;compiledfilename&#125;</code></pre><h2 id="4-11数据重平衡"><a href="#4-11数据重平衡" class="headerlink" title="4.11数据重平衡"></a>4.11数据重平衡</h2><p>找到空间利用率比较高的osd然后执行</p><pre><code class="hljs \">sudo ceph osd reweight &#123;osd_num_id&#125; &#123;reweight&#125;</code></pre><p>当然也可以批量调整：目前有两种模式</p><ul><li>按照OSD当前的空间利用率(<code>reweight-by-utilization</code>) ;</li><li>按照PG在OSD之间的分布(<code>reweight-by-pg</code>)。</li></ul><p>为防止影响前端业务，可以先进行测试，这会触发PG进行迁移量统计。</p><p>例如：</p><pre><code class="hljs dust"><span class="xml">sudo ceph osd test-reweight-by-utilization </span><span class="hljs-template-variable">&#123;oload&#125;</span><span class="xml"> </span><span class="hljs-template-variable">&#123;max_change&#125;</span><span class="xml">\</span><span class="hljs-template-variable">&#123;max_osds&#125;</span><span class="xml"> </span><span class="hljs-template-variable">&#123;--no-increasing&#125;</span></code></pre><div class="table-container"><table><thead><tr><th style="text-align:center">参数</th><th style="text-align:center">含义</th></tr></thead><tbody><tr><td style="text-align:center">oload</td><td style="text-align:center">可选；整型，≥100，默认值120；当且仅当某个OSD的空间利用率大于等于集群瓶颈空间利用率的overload/100时，调整其reweight</td></tr><tr><td style="text-align:center">max_change</td><td style="text-align:center">可选，浮点数，[0,1]；默认受<code>mon_reweight_max_change</code>控制，目前为0.05.每次调整reweight的最大幅度，即调整上限。实际每个osd调整幅度取决于自身空间利用率与集群平均空间利用率的偏离程度——偏离越多调整越大</td></tr><tr><td style="text-align:center">max_osds</td><td style="text-align:center">可选，整型，默认受<code>mon_reweight_max_osds</code>控制，目前为4.每次至多调整的osd数目。</td></tr><tr><td style="text-align:center">—no-increasing</td><td style="text-align:center">可选,字符类型，如果携带，则从不将reweight进行上调（上调指将当前的underload的OSD权重调大，让其分担更多PG）；如果不携带，至多将OSD的reweight调整至1.0/</td></tr></tbody></table></div><p>使用以下参数进行确认</p><pre><code class="hljs dust"><span class="xml">sudo ceph osd reweight-by-utilization </span><span class="hljs-template-variable">&#123;oload&#125;</span><span class="xml"> </span><span class="hljs-template-variable">&#123;max_change&#125;</span><span class="xml">\</span><span class="hljs-template-variable">&#123;max_osds&#125;</span><span class="xml"> </span><span class="hljs-template-variable">&#123;--no-increasing&#125;</span></code></pre><blockquote><p>合理设置weights</p><p>Ceph将weights表示为两倍，从而可以进行精细调整。 weight是设备容量之间的相对差。 我们建议将1.00用作1TB存储设备的相对重量。 在这种情况下，权重为0.5代表大约500GB，权重为3.00代表大约3TB。 较高级别的存储桶的权重是该存储桶聚合的所有叶子项的总和。</p><p>桶项目weight是一维的，但您也可以计算项目重量以反映存储驱动器的性能。 例如，如果您有许多1TB驱动器，其中一些具有较低的数据传输速率，而另一些具有较高的数据传输速率，则即使它们具有相同的容量（例如，硬盘的权重为0.80），也可以对其进行不同的加权。 第一组总吞吐量较低的驱动器，以及1.20第二组总吞吐量较高的驱动器）。</p></blockquote><hr><p>2020/12/22更新</p><h2 id="MapX-lt-span-class-”hint—top-hint—rounded”-aria-label-”Wang-L-Zhang-Y-Xu-J-et-al-MAPX-Controlled-Data-Migration-in-the-Expansion-of-Decentralized-Object-Based-Storage-Systems-C-18th-USENIX-Conference-on-File-and-Storage-Technologies-FAST-20-2020-1-11"><a href="#MapX-lt-span-class-”hint—top-hint—rounded”-aria-label-”Wang-L-Zhang-Y-Xu-J-et-al-MAPX-Controlled-Data-Migration-in-the-Expansion-of-Decentralized-Object-Based-Storage-Systems-C-18th-USENIX-Conference-on-File-and-Storage-Technologies-FAST-20-2020-1-11" class="headerlink" title="MapX&lt;span class=”hint—top hint—rounded” aria-label=”Wang L, Zhang Y, Xu J, et al. {MAPX}: Controlled Data Migration in the Expansion of Decentralized Object-Based Storage Systems[C]//18th {USENIX} Conference on File and Storage Technologies ({FAST} 20). 2020: 1-11."></a>MapX<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote">&lt;span class=”hint—top hint—rounded” aria-label=”Wang L, Zhang Y, Xu J, et al. {MAPX}: Controlled Data Migration in the Expansion of Decentralized Object-Based Storage Systems[C]//18th {USENIX} Conference on File and Storage Technologies ({FAST} 20). 2020: 1-11.</h2><p>“&gt;[2]&lt;/span&gt;&lt;/a&gt;&lt;/sup&gt; (FAST20’) 中对CRUSH算法的改进</p><p>传统的CRUSH架构如下图（图1）</p><p><img src="/img/mapx1.png" alt="image-20201224164010122" style="zoom:80%;" /></p><p>传统的CRUSH算法在增加节点时会导致大量的数据迁移，导致系统性能下降。下图（图2）模拟了两个模拟CRUSH集群在扩展时发生的数据迁移。</p><p><img src="/img/mapx2.png" alt="image-20201224162115959" style="zoom:80%;" /></p><p>定性的表示，数据迁移量可以高达hΔw/W，其中h是层次结构中的级别数，而Δw和W是扩展权重以及 所有OSD的总重量.</p><p>存储系统通常在集群扩展时会避免数据迁移，这会导致数据暂时不平衡。比如，Haystack和HDFS采用中心目录来避免已有的对象受到影响。因此本文思考通过加入适当的中心化成分来优化CRUSH。</p><p><img src="/img/mapx3.png" alt="image-20201224163557319" style="zoom:67%;" /></p><p>上图(3)表示，MAPX对每次扩展记录为“一层”。MAPX 会加入select到placement rule当中。可与传统CRUSH对比理解。为了支持时间维度的映射并且对CRUSH改动最小，我们在root下插入一个虚拟层，每一个虚拟节点表示一个扩展层。 虚拟层使MAPX能够通过在将新对象映射到新层之前对CRUSH算法进行进一步处理来实现无迁移扩展。 由于新层不会影响旧层的权重，因此旧对象在旧层中的放置不会改变。</p><h3 id="映射对象到PG"><a href="#映射对象到PG" class="headerlink" title="映射对象到PG"></a>映射对象到PG</h3><p>在每次扩展中，都会为新层分配一定数量的新创建的PG，每个PG的时间戳（$t_{pgs}$）等于该层的扩展时间（$t_l$）。 写入/读取对象O（带有创建时间戳记$t_0$）时，我们首先通过以下方法计算O的PG的ID（pgid）：</p><script type="math/tex; mode=display">pgid =Hash(name) \text{ mod INIT_PG_NUM[j]} + \sum\limits_{i=0}^{j-1}\text{INIT_PG_NUM[i]}</script><p>这里name表示对象名，<code>INT_PG_NUM[i]</code>表示PG第i层的初始值，并且第j层拥有最小的时间标签$t_l\le t_O$，PG可能会重新映射到其他层，例如进行负载平衡（第3.2节），<code>INIT_PG_NUM</code>是层的常量，因此从对象到PG的映射是不可变的。<br>   因此，每个对象在创建过程中都映射到负责的PG，所有PG中的最新时间戳$t_{pgs}≤t_O$。 例如，图3（b）中的三个RBD1，RBD2和RBD3是分别在layer0，layer1和layer2扩展之后创建的。  RBD1，RBD2和RBD3的对象将使用三层的<code>INIT_PG_NUM</code>分别计算其在layer0，layer1和layer2内的PG。</p><h3 id="将PG映射到OSD"><a href="#将PG映射到OSD" class="headerlink" title="将PG映射到OSD"></a>将PG映射到OSD</h3><p><img src="/img/mapx4.png" alt="image-20201224170254087" style="zoom:67%;" /></p><p>下面我们看下这篇文章具体是如何改进CRUSH的。</p><ul><li><p>如果type不是“layer”那么就等同于原始CRUSH算法（2~4行）。</p></li><li><p>否则，我们将初始化一个图层数组，该数组按图层时间戳的升序（第5行）存储当前正在处理的存储桶（通常是根目录）下的所有图层。 我们还在第6到8行初始化num层（层数），pg（放置组）和〜o（输出列表）。然后循环（第9-21行）在层阵列中添加数字层 到输出列表〜o。 在大多数情况下，层数为 1，PG可以在一层中映射到OSD，也有number很大的情况，比如在两个扩展层进行镜像。</p></li><li>请注意，对象的副本不一定都放置在最新层上。 例如，假设最后一个扩展（第2层）在图3（a）中仅添加了两个机柜（即m = 2），但是第二个<code>select()</code>函数（<code>select(3,cabinet)</code>）需要三个机柜。 这将导致第一个<code>select()</code>函数<code>(select(1,layer)</code> 被调用两次，以满足遵循CRUSH回溯机制的规则：当<code>select()</code>函数无法在“ layer”存储桶下选择足够的项目时，MAPX 将保留（而不是放弃）选定的项目，并回溯到根以选择上一层下面的缺少的项目。 第12至14行检查先前的<code>select()</code>是否选择了图层，如果是，我们将继续进行下一个循环，以避免执行回溯时重复的图层选择。 仔细检查可确保算法1正确处理此情况，并分别为第一个和第二个<code>select()</code>函数返回layer2和layer1。</li></ul><h3 id="迁移控制"><a href="#迁移控制" class="headerlink" title="迁移控制"></a>迁移控制</h3><p>由于原始CRUSH的随机性和均匀性，基于MAPX的免迁移放置算法可在每层内提供（统计）负载平衡，当当前层的负载增加到与先前层相同的水平时，通过适时扩展群集来实现不同层之间的近似负载平衡。但是，层的负载可能由于例如对象的移除，OSD的故障或不可预测的工作负载变化而改变。 例如，在图3中，当第一个扩展（第1层）的负载与原始集群（第0层）的负载一样高时，该集群可能会执行第二个扩展（第2层），但随后会有大量 删除第1层的对象，因此前两层的负载可能会变得不平衡。<br>  为了解决潜在的负载不平衡问题，我们设计了三种灵活的策略来动态管理MAPX中的负载，即放置组重新映射，群集收缩和层合并。</p><p><strong>PG重新映射</strong>。  MAPX支持通过动态重新映射PG来控制对象数据的迁移。 每个PG都有两个时间戳，即等于PG初始层扩展时间的静态时间戳（$t_{pgs}$）和可以设置为任何层的扩展时间的动态时间戳（$t_{pgd}$）。 与使用静态时间戳的从对象到PG的映射不同，从PG到图层的映射是通过将PG的动态时间戳与图层的时间戳进行比较来进行的（算法1中的第11行）。 因此，可以通过操纵动态时间戳（如图3（b）所示）将PG轻松地重新映射到任何层，该时间戳将通过增量映射更新通知所有OSD和客户端。  PG的时间戳存储开销适中。 例如，如果我们为每个PG时间戳使用一个字节索引（指向相应层的时间戳），该索引最多支持$2^8$层= 256层），并且假设一台机器有20个OSD，每个OSD负责200个PG，则 1000个机器集群的时间戳的内存开销为1000×20×200×2×1B = 8MB。<br>   <strong>集群收缩</strong>。 当层的负载低于阈值时，MAPX会通过从集群中删除该层的设备（例如OSD，机器和机架）来收缩集群，这是集群扩展的逆向操作。<br>   给定要从群集中删除的层Ω，我们首先将Ω中的所有PG根据其总权重分配给其余层（为简单起见，重新分配不考虑层的实际负载），然后将PG迁移到 通过重新映射确定目标层（如上所述）。 缩小后，逻辑上保留了Ω层（没有物理设备或PG），并且它的<code>INIT_PG_NUM</code>不会更改，以免影响从对象到PG的映射（根据等式（2））。<br>   <strong>层合并</strong>。  MAPX通过层合并来平衡两层（Ω和Ω’）的负载，这可以通过将一层（Ω’）的扩展时间设置为与另一层（Ω）相同的扩展时间来轻松实现。</p><p>我们通过将物理设备ID和该层的时间戳连接起来，为特定层（即，特定虚拟节点下方）的内部设备分配了虚拟设备ID。 <u>我们使用虚拟节点的权重字段来记录图层的时间戳，并将其与PG的动态时间戳进行比较以进行图层选择</u>。<br>   MAPX不适合用于一般对象存储，主要是因为维护和检索任意对象的时间戳很重要。 按对象时间戳维护的开销与维护中央目录的开销类似，因此在诸如CRUSH和MAPX的集中式放置方法中应避免这种开销。<br>   但是，MAPX适用于各种基于对象的存储系统，例如块存储（Ceph-RBD ）和文件存储（Ceph-FS），其中对象时间戳可以保持为更高级别 元数据。</p><h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><p><strong>Ceph-RBD</strong>    </p><p>Ceph-RBD。 我们已经为Ceph-RBD（RADOS块设备）实现了基于元数据的时间戳检索机制。  Ceph将RBD的元数据（例如数据对象名称的前缀以及卷，快照，条带等信息）存储在其<code>rbd_header</code>结构中，当客户端通过<code>rbd_open</code>挂载RBD时将检索该元数据。 由于RBD的对象可以在任何扩展之后创建，因此我们继承当前层的时间戳（创建对象时）作为对象的时间戳。 因此，我们在<code>rbd_header</code>结构中添加了一个每个对象的索引（称为对象时间戳<code>object_timestamp</code>），该索引指向每一层的扩展时间。 额外元数据的存储开销适中。 例如，如果我们为每个对象索引使用一个字节，而每个对象为4MB，则4TB RBD的对象时间戳数组的存储开销最多为4TB/4MB×1B = 1MB。</p><p><strong>CephFS</strong></p><p> 我们还（部分）为CephFS（Ceph文件系统）实现了时间戳检索机制。<br>   Ceph将文件元数据（包括文件创建时间）存储在inode结构中。 客户端在打开文件时读取inode并获取文件创建时间。 当前，我们让文件的所有对象继承文件的时间戳，以便我们可以按文件的粒度控制时维映射。 我们还计划支持更精细的对象时间戳维护。 如果文件大小超过阈值T（例如T = 100 MB），我们可以将其划分为每个小于100 MB的子文件。 文件的元数据既维护了从文件到其子文件的映射，又维护了每个子文件的创建时间戳，因此我们可以以子文件的粒度控制时间维度映射。</p><h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p><img src="/img/mapx6.png" alt="image-20201224174039878" style="zoom:150%;" /></p><blockquote><p>​                                        左图4，第99百分位 I/O延迟对比；右图5 ，IOPS对比</p></blockquote><p>我们使用Ceph所有参数的默认值，但<code>OSD_max_backfills</code>除外。 如第1节所述，Ceph通过实现级优化减轻了CRUSH的迁移问题。 它使用参数<code>OSD_max_back_fill≥1</code>在数据迁移导致的性能下降的严重性和持续时间之间进行权衡。默认情况下，Ceph将参数<code>OSD_max_backfills</code>设置为1，这使迁移具有最低优先级，因此PG中的对象可能以极低的速度迁移。 尽管部分缓解了降级问题，但将<code>OSD_max_backfills</code>设置为1会大大延长迁移时间，并在迁移完成之前大大增加写入负载：等待迁移的PG写入将首先对原始OSD执行，然后异步进行 迁移到目标OSD。<br>   显然，这使Ceph遭受的性能下降的幅度较小，但时间较长。 我们设置<code>OSD_max_backfills=10</code>，在此实验中更合理，因此可以优先考虑迁移，以证明MAPX和CRUSH在算法级别上的差异。 </p><p>​    图4显示了第99个百分位尾延迟的评估结果。 请注意，云存储方案通常关心的是（第99、99.9或99.99个百分位）尾部延迟，而不是平均延迟或中值延迟，以确保SLA（服务等级保障协议）。  MAPX的性能比CRUSH高出4.25倍，这主要是因为CRUSH中的迁移与正常的I / O请求严重竞争。 在此实验中，MAPX始终使用初始群集的六个OSD来满足I / O请求，因为它不会将现有RBD迁移到新OSD。 相比之下，CRUSH分别使用六个，九个和十二个OSD，但是CRUSH引起的数据迁移会严重降低性能，这对于延迟敏感的应用程序是不可接受的。<br>   图5分别显示了MAPX和CRUSH中IOPS的评估结果。 每个结果均为20次运行的平均值，我们省略了误差线，因为与平均值的方差相对较小（小于5％）。 与延迟测试类似，在IOPS测试中，MAPX的性能明显优于CRUSH，最高可达到74.3％，这是因为CRUSH的数据迁移可以应付正常的I / O请求。</p><p><img src="/img/mapx7.png" alt="image-20201224200625534" style="zoom:80%;" /></p><blockquote><p>图7：MAPX和CRUSH的第99个百分点的I / O延迟（在群集收缩期间）。</p></blockquote><p><img src="/img/mapx8.png" alt="image-20201224200711635" style="zoom:80%;" /></p><blockquote><p>图8：在MAPX中合并的层中受影响的PG的数量（四个扩展之后）。 由于CRUSH不支持合并，因此我们在每次CRUSH扩展后测量受影响的PG的数量以供参考。</p></blockquote><p>我们使用CrushTool模拟MAPX中的图层合并。<br>   我们采用三向复制，其中每个对象在三个OSD上存储三个副本。 最初，存储集群由5个机架组成，每个机架有20台计算机。 一台机器有20个OSD。 总共有100台机器和2000个OSD，可存储20万个PG。 我们将集群扩展了四倍。 在每个扩展中，我们将一个机架的新层（包含20台计算机和400个OSD）添加到一个新层，并在新层中添加40,000个新PG。 显然，MAPX将所有新PG映射到新添加的OSD上，因此不会发生迁移。 四个扩展之后，总共有9个机架，180台机器和3600个OSD，可存储360,000个PG。 然后，我们合并第一扩展和第二扩展的40台机器，并测量MAPX中的合并影响了多少个PG。<br>   结果如图8所示，其中MAPX中的图层合并影响了两个合并图层的所有80,000 PG中的70,910 PG。  MAPX的层合并中受影响的PG的相对较高比例取决于作为参考，我们还模拟了CRUSH中的四个扩展，其中让群集最初具有360,000个PG，并且在扩展期间不添加新PG，因为否则CRUSH会将映射从对象更改为PG，从而导致更多PG迁移。<br>  图8还显示了CRUSH的每次扩展会影响多少PG。 例如，当机器数量从160增加到180时，几乎90％的PG在第四次扩展中都受到影响。</p><hr><h2 id="引用和参考文献"><a href="#引用和参考文献" class="headerlink" title="引用和参考文献"></a>引用和参考文献</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>WEIL, S. A., BRANDT, S. A., MILLER, E. L., AND MALTZAHN,C. Crush: Controlled, scalable, decentralized placement of replicateddata. In <em>SC</em>’06: Proceedings of the 2006 ACM/IEEE Conference on<em>Supercomputing</em> (2006), IEEE, pp. 31–31.<a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>Wang L, Zhang Y, Xu J, et al. {MAPX}: Controlled Data Migration in the Expansion of Decentralized Object-Based Storage Systems[C]//18th {USENIX} Conference on File and Storage Technologies ({FAST} 20). 2020: 1-11.<a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;TODO: last update: 2020/12/28&lt;/p&gt;
&lt;div class=&quot;note note-primary&quot;&gt;
            &lt;p&gt;本blog包括理论和实践两个部分，实践部分需要您事先部署成功Ceph集群！&lt;/p&gt;&lt;p&gt;参考《Ceph设计与实现</summary>
      
    
    
    
    <category term="ceph" scheme="http://durantthorvalds.top/categories/ceph/"/>
    
    <category term="分布式存储" scheme="http://durantthorvalds.top/categories/ceph/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/"/>
    
    
    <category term="ceph" scheme="http://durantthorvalds.top/tags/ceph/"/>
    
    <category term="理论" scheme="http://durantthorvalds.top/tags/%E7%90%86%E8%AE%BA/"/>
    
    <category term="crush" scheme="http://durantthorvalds.top/tags/crush/"/>
    
  </entry>
  
  <entry>
    <title>「入门部署」Ceph-ansible部署集群</title>
    <link href="http://durantthorvalds.top/2020/11/24/Ceph-ansible%E9%83%A8%E7%BD%B2%E8%B8%A9%E5%9D%91%E6%97%A5%E8%AE%B0/"/>
    <id>http://durantthorvalds.top/2020/11/24/Ceph-ansible%E9%83%A8%E7%BD%B2%E8%B8%A9%E5%9D%91%E6%97%A5%E8%AE%B0/</id>
    <published>2020-11-24T10:00:00.000Z</published>
    <updated>2020-12-16T08:34:15.044Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Ceph-ansible部署踩坑日记"><a href="#Ceph-ansible部署踩坑日记" class="headerlink" title="Ceph-ansible部署踩坑日记"></a>Ceph-ansible部署踩坑日记</h1><blockquote><p>官方文档：<a href="https://docs.ceph.com/projects/ceph-ansible/en/latest/">https://docs.ceph.com/projects/ceph-ansible/en/latest/</a></p></blockquote><h1 id="Quick-Deployment-快速部署"><a href="#Quick-Deployment-快速部署" class="headerlink" title="Quick Deployment:快速部署"></a>Quick Deployment:快速部署</h1><div class="note note-danger">            <p>Important:</p><p>ceph-deploy 不再经常性维护， 并且在高于Nautilus的版本没有进行过测试。不推荐使用! </p>          </div><p><strong>Cephadm</strong>：Cephadm完美支持新的编排API、CLI和仪表盘特性，可用于快速部署Octopus及更新版本的Ceph集群。Cephadm更加简单，且不依赖其它自动化部署工具，但该工具不支持部署旧版本的Ceph（如14的Nautilus），所有Ceph进程也是运行在容器中的，意味着在修改源码的情况下还需要制作新的镜像，比较麻烦。<strong>官方文档中指出，Cephadm暂时不推荐用于生产环境。</strong></p><p><strong>Rook</strong>：可以把Ceph部署在Kubernetes集群中运行。与1类似的是，Ceph也是运行在容器中的。</p><blockquote><p>上述的两种方法都是容器部署</p></blockquote><p>第三方部署工具</p><p>【推荐👍】 <strong>ceph-ansible</strong>. 它被广泛使用。ceph-ansible未与Nautlius和Octopus中引入的新的Orchestrator API集成在一起，这意味着更新的管理功能和仪表板集成不可用。</p><p><a href="https://github.com/SUSE/DeepSea">DeepSea</a>使用Salt安装Ceph。</p><p><a href="https://jaas.ai/ceph-mon">jaas.ai/ceph-mon</a>使用Juju安装Ceph。</p><p><a href="https://github.com/openstack/puppet-ceph">github.com/openstack/puppet-ceph</a> 通过Puppet安装Ceph。</p><p>官方的最低要求是 <strong>3</strong> Monitors + <strong>3</strong> Managers + <strong>3</strong> OSDs 。 我们尝试用虚拟机搭建最小集群。</p><p>在运行ceph-mon守护程序的每个节点上，还应该设置一个ceph-mgr守护程序。</p><div class="table-container"><table><thead><tr><th style="text-align:center">虚拟机节点名称</th><th style="text-align:center">职责</th><th style="text-align:center">IP地址</th></tr></thead><tbody><tr><td style="text-align:center">ceph-master (w/ source-code compiled)</td><td style="text-align:center">mon0 + mgr0+osd0(部署节点)</td><td style="text-align:center">192.168.161.134</td></tr><tr><td style="text-align:center">ceph-node1</td><td style="text-align:center">mon1+mgr1</td><td style="text-align:center">192.168.161.130</td></tr><tr><td style="text-align:center">ceph-node2</td><td style="text-align:center">mon2+mgr2</td><td style="text-align:center">192.168.161.131</td></tr><tr><td style="text-align:center">ceph-osd1</td><td style="text-align:center">osd1</td><td style="text-align:center">192.168.161.132</td></tr><tr><td style="text-align:center">ceph-osd2</td><td style="text-align:center">osd2</td><td style="text-align:center">192.168.161.133</td></tr></tbody></table></div><div class="note note-primary">            <p>小技巧：我们可以先在1台虚拟机上配置好通用的预备环境，再克隆出另外的2台，随后调整部分配置后即可（如IP、主机名等）。克隆后记得重新生成网卡MAC地址</p>          </div><h3 id="安装前准备"><a href="#安装前准备" class="headerlink" title="安装前准备"></a>安装前准备</h3><p>关闭firewalld，设置selinux为disabled状态，设置ssh免密登录，配置时钟同步服务。</p><pre><code class="hljs nginx"><span class="hljs-attribute">sudo</span> ufw status<span class="hljs-comment">#查看防火墙状态 active:开启 inactive：关闭</span></code></pre><pre><code class="hljs puppet"><span class="hljs-comment"># mon作为控制节点</span>sudo apt install openssh-serverssh-keygen -t rsa ssh-copy-<span class="hljs-keyword">id</span> &#123;<span class="hljs-built_in">hostname</span>&#125;@&#123;ip&#125;<span class="hljs-comment">#复制密钥到各节点</span><span class="hljs-keyword">ssh</span> &#123;<span class="hljs-built_in">hostname</span>&#125;@&#123;ip&#125;</code></pre><p>mon节点作为时钟同步节点</p><pre><code class="hljs stylus">sudo apt install ntpsudo vim /etc/ntp.conf#在/etc/ntp.conf添加<span class="hljs-selector-id">#server</span> <span class="hljs-number">127.127</span>.<span class="hljs-number">1.0</span><span class="hljs-selector-id">#fudge</span> <span class="hljs-number">127.127</span>.<span class="hljs-number">1.0</span> stratum <span class="hljs-number">10</span>#并注释下列四行<span class="hljs-selector-id">#pool</span> <span class="hljs-number">0</span><span class="hljs-selector-class">.ubuntu</span><span class="hljs-selector-class">.pool</span><span class="hljs-selector-class">.ntp</span><span class="hljs-selector-class">.org</span> iburst<span class="hljs-selector-id">#pool</span> <span class="hljs-number">1</span><span class="hljs-selector-class">.ubuntu</span><span class="hljs-selector-class">.pool</span><span class="hljs-selector-class">.ntp</span><span class="hljs-selector-class">.org</span> iburst<span class="hljs-selector-id">#pool</span> <span class="hljs-number">2</span><span class="hljs-selector-class">.ubuntu</span><span class="hljs-selector-class">.pool</span><span class="hljs-selector-class">.ntp</span><span class="hljs-selector-class">.org</span> iburst<span class="hljs-selector-id">#pool</span> <span class="hljs-number">3</span><span class="hljs-selector-class">.ubuntu</span><span class="hljs-selector-class">.pool</span><span class="hljs-selector-class">.ntp</span><span class="hljs-selector-class">.org</span> iburst</code></pre><p>在其它节点</p><pre><code class="hljs routeros">sudo apt install ntpdate openssh-server --assume-yessudo /etc/init.d<span class="hljs-built_in">/ntp </span>restart sudo systemctl <span class="hljs-builtin-name">enable</span> ntp.service #启动ntpsudo ntpdate &#123;mon-hostname&#125;</code></pre><p>会得到矫正的时间</p><pre><code class="hljs routeros">Nov 06:31:02 ntpdate[8540]: adjust time<span class="hljs-built_in"> server </span>192.168.161.129 offset 0.097653 sec</code></pre><p>有可能出现 name or server not known 错误</p><p>在每个节点 \etc\hosts 加入所有其它节点映射，比如</p><pre><code class="hljs accesslog"><span class="hljs-number">192.168.161.129</span>  ceph-master</code></pre><p>再ping 或者 nslookup测试一下就行。</p><div class="note note-warning">            <p>不要在同一个节点同时安装ntp和ntpdate</p>          </div><p>另一种方法是直接和aliyun服务器同步，在ntp.conf末尾加上<code>server ntp.aliyun.com</code></p><p>再重启</p><pre><code class="hljs routeros">sudo /etc/init.d<span class="hljs-built_in">/ntp </span>restart</code></pre><h2 id="ceph-ansible-😁"><a href="#ceph-ansible-😁" class="headerlink" title="ceph-ansible 😁"></a>ceph-ansible <a href="https://docs.ceph.com/projects/ceph-ansible/en/latest/index.html#configuration-and-usage">😁</a></h2><blockquote><p>参考：<a href="https://www.cnblogs.com/zyxnhr/p/10543814.html">https://www.cnblogs.com/zyxnhr/p/10543814.html</a> 写的非常细致。</p><p><a href="https://blog.csdn.net/liuzhupeng/article/details/106767126">https://blog.csdn.net/liuzhupeng/article/details/106767126</a></p></blockquote><p> 三要素：inventory file, playbook and configuration。</p><p>Dependency: <code>python2</code>所有节点</p><p>首先测试能否连通其它节点</p><pre><code class="hljs routeros">ansible all -m<span class="hljs-built_in"> ping </span>#注意这儿不能加sudo，否则会permission denied</code></pre><p>如果没有问题</p><pre><code class="hljs arcade">node1 | <span class="hljs-function"><span class="hljs-params">SUCCESS</span> =&gt;</span> &#123;    <span class="hljs-string">&quot;ansible_facts&quot;</span>: &#123;        <span class="hljs-string">&quot;discovered_interpreter_python&quot;</span>: <span class="hljs-string">&quot;/usr/bin/python&quot;</span>    &#125;,    <span class="hljs-string">&quot;changed&quot;</span>: <span class="hljs-literal">false</span>,    <span class="hljs-string">&quot;ping&quot;</span>: <span class="hljs-string">&quot;pong&quot;</span>&#125;node2 | <span class="hljs-function"><span class="hljs-params">SUCCESS</span> =&gt;</span> &#123;    <span class="hljs-string">&quot;ansible_facts&quot;</span>: &#123;        <span class="hljs-string">&quot;discovered_interpreter_python&quot;</span>: <span class="hljs-string">&quot;/usr/bin/python3&quot;</span>    &#125;,    <span class="hljs-string">&quot;changed&quot;</span>: <span class="hljs-literal">false</span>,    <span class="hljs-string">&quot;ping&quot;</span>: <span class="hljs-string">&quot;pong&quot;</span>&#125;node3 | <span class="hljs-function"><span class="hljs-params">SUCCESS</span> =&gt;</span> &#123;    <span class="hljs-string">&quot;ansible_facts&quot;</span>: &#123;        <span class="hljs-string">&quot;discovered_interpreter_python&quot;</span>: <span class="hljs-string">&quot;/usr/bin/python3&quot;</span>    &#125;,    <span class="hljs-string">&quot;changed&quot;</span>: <span class="hljs-literal">false</span>,    <span class="hljs-string">&quot;ping&quot;</span>: <span class="hljs-string">&quot;pong&quot;</span>&#125;</code></pre><ol><li><h3 id="我们首先安装ceph-ansble"><a href="#我们首先安装ceph-ansble" class="headerlink" title="我们首先安装ceph-ansble."></a>我们首先安装ceph-ansble.</h3></li></ol><pre><code class="hljs awk">git clone https:<span class="hljs-regexp">//gi</span>thub.com<span class="hljs-regexp">/ceph/</span>ceph-ansible.gitgit checkout <span class="hljs-variable">$branch</span> <span class="hljs-comment">#默认master分支</span>pip install -r requirements.txt sudo apt install ansible</code></pre><ol><li><h3 id="再配置inventory"><a href="#再配置inventory" class="headerlink" title="再配置inventory"></a>再配置inventory</h3></li></ol><p>在 /etc/ansible/下创建<code>hosts</code>文件，clients节点不应该与mon osd等重叠。在最新版本，还需要设置<code>[monitoring]</code>. 个人感觉和mon节点设成一样就行了。</p><pre><code class="hljs csharp">[<span class="hljs-meta">mons</span>]node1node2node3[<span class="hljs-meta">osds</span>]node1node2node3[<span class="hljs-meta">rgws</span>]node1node2node3[<span class="hljs-meta">clients</span>]node4node5[<span class="hljs-meta">mgrs</span>]node1node2node3[<span class="hljs-meta">monitoring</span>]node1node2node3</code></pre><ol><li><strong>然后在下载下来的ansible目录下拷贝文件，根据节点的角色拷贝具体的文件</strong></li></ol><pre><code class="hljs stylus">#除了site<span class="hljs-selector-class">.yml</span>.sample，all<span class="hljs-selector-class">.yml</span>.sample是必须要修改的之外，其他文件根据要安装的角色自行修改cp site<span class="hljs-selector-class">.yml</span><span class="hljs-selector-class">.sample</span> site.ymlcp group_vars/osds<span class="hljs-selector-class">.yml</span><span class="hljs-selector-class">.sample</span> group_vars/osds.ymlcp group_vars/clients<span class="hljs-selector-class">.yml</span><span class="hljs-selector-class">.sample</span> group_vars/clients.ymlcp group_vars/mons<span class="hljs-selector-class">.yml</span><span class="hljs-selector-class">.sample</span> group_vars/mons.ymlcp group_vars/mgrs<span class="hljs-selector-class">.yml</span><span class="hljs-selector-class">.sample</span> group_vars/mgrs.ymlcp group_vars/all<span class="hljs-selector-class">.yml</span><span class="hljs-selector-class">.sample</span> group_vars/all.yml</code></pre><ol><li><h3 id="配置all-yml"><a href="#配置all-yml" class="headerlink" title="配置all.yml"></a>配置all.yml</h3></li></ol><p>我们需要修改osds.yml部分：</p><pre><code class="hljs yaml"><span class="hljs-string">--</span><span class="hljs-attr">dummy:</span><span class="hljs-attr">ceph_release_num:</span>  <span class="hljs-attr">octopus:</span> <span class="hljs-number">15</span><span class="hljs-attr">cluster:</span> <span class="hljs-string">ceph-test</span><span class="hljs-attr">ceph_origin:</span> <span class="hljs-string">repository</span><span class="hljs-attr">ceph_repository:</span> <span class="hljs-string">community</span><span class="hljs-attr">ceph_mirror:</span> <span class="hljs-string">https://mirrors.aliyun.com/ceph/</span><span class="hljs-attr">ceph_stable_key:</span> <span class="hljs-string">https://mirrors.aliyun.com/ceph/keys/release.asc</span><span class="hljs-attr">ceph_stable_release:</span> <span class="hljs-string">octopus</span><span class="hljs-attr">monitor_interface:</span> <span class="hljs-string">ens33</span><span class="hljs-attr">monitor_address:</span> <span class="hljs-number">192.168</span><span class="hljs-number">.161</span><span class="hljs-number">.135</span><span class="hljs-attr">ip_version:</span> <span class="hljs-string">ipv4</span><span class="hljs-attr">public_network:</span> <span class="hljs-number">192.168</span><span class="hljs-number">.161</span><span class="hljs-number">.0</span><span class="hljs-string">/24</span><span class="hljs-attr">cluster_network:</span> <span class="hljs-string">&quot;<span class="hljs-template-variable">&#123;&#123; public_network | regex_replace(&#x27; &#x27;, &#x27;&#x27;) &#125;&#125;</span>&quot;</span><span class="hljs-attr">osd_mkfs_type:</span> <span class="hljs-string">xfs</span><span class="hljs-attr">osd_mkfs_options_xfs:</span> <span class="hljs-string">-f</span> <span class="hljs-string">-i</span> <span class="hljs-string">size=2048</span><span class="hljs-attr">osd_mount_options_xfs:</span> <span class="hljs-string">noatime,largeio,inode64,swalloc</span><span class="hljs-attr">osd_objectstore:</span> <span class="hljs-string">bluestore</span><span class="hljs-attr">dashboard_enabled:</span> <span class="hljs-literal">False</span></code></pre><p>devices：指定osd使用的硬盘</p><p>osd_scenario：collocated启用并置journal  【笔者没有找到】</p><p>下面是一些trick</p><pre><code class="hljs elixir"><span class="hljs-variable">$ </span>sudo fdisk -l <span class="hljs-comment">#查看磁盘信息</span></code></pre><p>查看你的更改</p><pre><code class="hljs vim">$<span class="hljs-keyword">grep</span> -v <span class="hljs-string">&#x27;^#&#x27;</span> <span class="hljs-keyword">all</span>.yml | <span class="hljs-keyword">grep</span> -v <span class="hljs-string">&#x27;^&amp;&#x27;</span> <span class="hljs-keyword">all</span>.yml</code></pre><p>或者查看所有修改后文件</p><pre><code class="hljs \">grep -Ev &quot;^$|^\s*#&quot;  *.yml</code></pre><ol><li><h3 id="配置osd"><a href="#配置osd" class="headerlink" title="配置osd"></a>配置osd</h3></li></ol><pre><code class="hljs arcade">devices:   - <span class="hljs-regexp">/dev/</span>vdb   - <span class="hljs-regexp">/dev/</span>vdc   - <span class="hljs-regexp">/dev/</span>vdd</code></pre><ol><li><h3 id="定义ansible的入口文件"><a href="#定义ansible的入口文件" class="headerlink" title="定义ansible的入口文件"></a>定义ansible的入口文件</h3></li></ol><pre><code class="hljs vala">- hosts:  - mons<span class="hljs-meta">#  - agents</span>  - osds<span class="hljs-meta">#  - mdss</span><span class="hljs-meta">#  - rgws</span><span class="hljs-meta">#  - nfss</span><span class="hljs-meta">#  - restapis</span><span class="hljs-meta">#  - rbdmirrors</span>  - clients  - mgrs<span class="hljs-meta">#  - iscsi-gws</span></code></pre><ol><li><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3></li></ol><pre><code class="hljs 1c">ansible-playbook  site.yml <span class="hljs-meta">#注意同样没有sudo</span></code></pre><pre><code class="hljs routeros">PLAY RECAP *********************************************************************node1                      : <span class="hljs-attribute">ok</span>=340  <span class="hljs-attribute">changed</span>=33   <span class="hljs-attribute">unreachable</span>=0    <span class="hljs-attribute">failed</span>=0    <span class="hljs-attribute">skipped</span>=547  <span class="hljs-attribute">rescued</span>=0    <span class="hljs-attribute">ignored</span>=0   node2                      : <span class="hljs-attribute">ok</span>=109  <span class="hljs-attribute">changed</span>=12   <span class="hljs-attribute">unreachable</span>=0    <span class="hljs-attribute">failed</span>=0    <span class="hljs-attribute">skipped</span>=261  <span class="hljs-attribute">rescued</span>=0    <span class="hljs-attribute">ignored</span>=0   node3                      : <span class="hljs-attribute">ok</span>=112  <span class="hljs-attribute">changed</span>=12   <span class="hljs-attribute">unreachable</span>=0    <span class="hljs-attribute">failed</span>=0    <span class="hljs-attribute">skipped</span>=260  <span class="hljs-attribute">rescued</span>=0    <span class="hljs-attribute">ignored</span>=0   INSTALLER STATUS ***************************************************************Install Ceph Monitor           : Complete (0:00:32)Install Ceph Manager           : Complete (0:00:09)Install Ceph OSD               : Complete (0:00:34)Install Ceph<span class="hljs-built_in"> Client </span>           : Complete (0:00:31)</code></pre><p>如果<code>failed=1</code>那就说明安装失败，你需要根据错误进行修改，如果打印的信息太少，你可以使用</p><pre><code class="hljs css"><span class="hljs-selector-tag">ansible-playbook</span> <span class="hljs-selector-tag">-vvv</span> <span class="hljs-selector-tag">site</span><span class="hljs-selector-class">.yml</span></code></pre><p>来打印更多信息。</p><ol><li><h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a><strong>测试</strong></h3></li></ol><p>接下来我们看看集群健康状况 ，在ceph-mon节点执行</p><pre><code class="hljs ebnf"><span class="hljs-attribute">ceph -s</span></code></pre><p>有可能会提示ceph RADOS object not found. 笔者怀疑是 /etc/ceph/下的conf文件名不是ceph所以导致无法识别出集群，后来干脆把集群名字改成<code>ceph</code>。是不是很傻X呢？</p><pre><code class="hljs apache"><span class="hljs-attribute">cluster</span>:  <span class="hljs-attribute">id</span>:     <span class="hljs-number">0</span>d<span class="hljs-number">3</span>c<span class="hljs-number">793</span>b-<span class="hljs-number">1</span>ee<span class="hljs-number">0</span>-<span class="hljs-number">43</span>ca-<span class="hljs-number">8</span>f<span class="hljs-number">28</span>-<span class="hljs-number">6</span>dbf<span class="hljs-number">695578</span>cf  <span class="hljs-attribute">health</span>: HEALTH_OK <span class="hljs-attribute">services</span>:  <span class="hljs-attribute">mon</span>: <span class="hljs-number">1</span> daemons, quorum node<span class="hljs-number">2</span> (age <span class="hljs-number">5</span>m)  <span class="hljs-attribute">mgr</span>: node<span class="hljs-number">2</span>(active, since <span class="hljs-number">5</span>m)  <span class="hljs-attribute">mds</span>: cephfs:<span class="hljs-number">1</span> &#123;<span class="hljs-number">0</span>=node<span class="hljs-number">3</span>=up:active&#125; <span class="hljs-number">2</span> up:standby  <span class="hljs-attribute">osd</span>: <span class="hljs-number">3</span> osds: <span class="hljs-number">3</span> up (since <span class="hljs-number">3</span>m), <span class="hljs-number">3</span> in (since <span class="hljs-number">4</span>m) <span class="hljs-attribute">task</span> status:  <span class="hljs-attribute">scrub</span> status:      <span class="hljs-attribute">mds</span>.node<span class="hljs-number">3</span>: idle <span class="hljs-attribute">data</span>:  <span class="hljs-attribute">pools</span>:   <span class="hljs-number">3</span> pools, <span class="hljs-number">65</span> pgs  <span class="hljs-attribute">objects</span>: <span class="hljs-number">22</span> objects, <span class="hljs-number">2</span>.<span class="hljs-number">2</span> KiB  <span class="hljs-attribute">usage</span>:   <span class="hljs-number">3</span>.<span class="hljs-number">0</span> GiB used, <span class="hljs-number">57</span> GiB / <span class="hljs-number">60</span> GiB avail  <span class="hljs-attribute">pgs</span>:     <span class="hljs-number">65</span> active+clean</code></pre><p>以及<code>sudo fidsk -l</code> 查看osd分配磁盘。</p><p>查看osd 拓扑。这有助于我们理解CRUSH算法。</p><pre><code class="hljs dos">sudo ceph osd <span class="hljs-built_in">tree</span></code></pre><pre><code class="hljs lsl">ID  CLASS  WEIGHT   TYPE NAME       STATUS  REWEIGHT  PRI-AFF<span class="hljs-number">-1</span>         <span class="hljs-number">0.05846</span>  root <span class="hljs-section">default</span>                             <span class="hljs-number">-5</span>         <span class="hljs-number">0.01949</span>      host node3                            <span class="hljs-number">1</span>    ssd  <span class="hljs-number">0.01949</span>          osd<span class="hljs-number">.1</span>       up   <span class="hljs-number">1.00000</span>  <span class="hljs-number">1.00000</span><span class="hljs-number">-3</span>         <span class="hljs-number">0.01949</span>      host node4                            <span class="hljs-number">0</span>    ssd  <span class="hljs-number">0.01949</span>          osd<span class="hljs-number">.0</span>       up   <span class="hljs-number">1.00000</span>  <span class="hljs-number">1.00000</span><span class="hljs-number">-7</span>         <span class="hljs-number">0.01949</span>      host node5                            <span class="hljs-number">2</span>    ssd  <span class="hljs-number">0.01949</span>          osd<span class="hljs-number">.2</span>       up   <span class="hljs-number">1.00000</span>  <span class="hljs-number">1.00000</span></code></pre><p>EOF</p><hr><p>优质博文参考：<a href="https://blog.51cto.com/14210294/2353243">https://blog.51cto.com/14210294/2353243</a></p><p><a href="https://www.cnblogs.com/zyxnhr/p/10543814.html">https://www.cnblogs.com/zyxnhr/p/10543814.html</a></p><hr><h2 id="Trouble-Shooting"><a href="#Trouble-Shooting" class="headerlink" title="Trouble Shooting"></a>Trouble Shooting</h2><ol><li>Devices are not disjoint</li></ol><p>设备存在交集。如果存储方式是bluestore, 那么block，wal和db必须是不同的存储盘。并且：</p><pre><code class="hljs angelscript">$ mount | grep osdtmpfs on /var/lib/ceph/osd/ceph<span class="hljs-number">-0</span> type tmpfs (rw,relatime,seclabel)$ ls -Alh /var/lib/ceph/osd/ceph<span class="hljs-number">-0</span>lrwxrwxrwx. <span class="hljs-number">1</span> ceph ceph <span class="hljs-number">19</span> Apr  <span class="hljs-number">7</span> <span class="hljs-number">21</span>:<span class="hljs-number">36</span> block -&gt; /dev/ceph-pool/osd0lrwxrwxrwx. <span class="hljs-number">1</span> root root <span class="hljs-number">22</span> Apr  <span class="hljs-number">7</span> <span class="hljs-number">21</span>:<span class="hljs-number">36</span> block.db -&gt; /dev/ceph-pool/osd0.dblrwxrwxrwx. <span class="hljs-number">1</span> root root <span class="hljs-number">23</span> Apr  <span class="hljs-number">7</span> <span class="hljs-number">21</span>:<span class="hljs-number">36</span> block.wal -&gt; /dev/ceph-pool/osd0.wal-rw-------. <span class="hljs-number">1</span> ceph ceph <span class="hljs-number">37</span> Apr  <span class="hljs-number">7</span> <span class="hljs-number">21</span>:<span class="hljs-number">36</span> ceph_fsid-rw-------. <span class="hljs-number">1</span> ceph ceph <span class="hljs-number">37</span> Apr  <span class="hljs-number">7</span> <span class="hljs-number">21</span>:<span class="hljs-number">36</span> fsid-rw-------. <span class="hljs-number">1</span> ceph ceph <span class="hljs-number">55</span> Apr  <span class="hljs-number">7</span> <span class="hljs-number">21</span>:<span class="hljs-number">36</span> keyring-rw-------. <span class="hljs-number">1</span> ceph ceph  <span class="hljs-number">6</span> Apr  <span class="hljs-number">7</span> <span class="hljs-number">21</span>:<span class="hljs-number">36</span> ready-rw-------. <span class="hljs-number">1</span> ceph ceph <span class="hljs-number">10</span> Apr  <span class="hljs-number">7</span> <span class="hljs-number">21</span>:<span class="hljs-number">36</span> type-rw-------. <span class="hljs-number">1</span> ceph ceph  <span class="hljs-number">2</span> Apr  <span class="hljs-number">7</span> <span class="hljs-number">21</span>:<span class="hljs-number">36</span> whoami</code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Ceph-ansible部署踩坑日记&quot;&gt;&lt;a href=&quot;#Ceph-ansible部署踩坑日记&quot; class=&quot;headerlink&quot; title=&quot;Ceph-ansible部署踩坑日记&quot;&gt;&lt;/a&gt;Ceph-ansible部署踩坑日记&lt;/h1&gt;&lt;blockquo</summary>
      
    
    
    
    <category term="ceph" scheme="http://durantthorvalds.top/categories/ceph/"/>
    
    <category term="分布式存储" scheme="http://durantthorvalds.top/categories/ceph/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/"/>
    
    
    <category term="系统架构" scheme="http://durantthorvalds.top/tags/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84/"/>
    
    <category term="ceph" scheme="http://durantthorvalds.top/tags/ceph/"/>
    
  </entry>
  
  <entry>
    <title>「基础理论」CEPH 基础介绍</title>
    <link href="http://durantthorvalds.top/2020/11/22/CEPH%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA/"/>
    <id>http://durantthorvalds.top/2020/11/22/CEPH%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA/</id>
    <published>2020-11-21T16:00:00.000Z</published>
    <updated>2021-01-05T12:50:41.075Z</updated>
    
    <content type="html"><![CDATA[<h1 id="CEPH基础理论学习"><a href="#CEPH基础理论学习" class="headerlink" title="CEPH基础理论学习"></a>CEPH基础理论学习</h1><blockquote><p>参考<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="Weil S A, Brandt S A, Miller E L, et al. Ceph: A scalable, high-performance distributed file system[C]//Proceedings of the 7th symposium on Operating systems design and implementation. 2006: 307-320.">[2]</span></a></sup></p></blockquote><hr><h1 id="第一部分"><a href="#第一部分" class="headerlink" title="第一部分"></a>第一部分</h1><h2 id="1-Ceph简介"><a href="#1-Ceph简介" class="headerlink" title="1 Ceph简介"></a><strong>1 Ceph简介</strong></h2><blockquote><p>Ceph是一个统一的分布式存储系统，设计初衷是提供较好的性能、可靠性和可扩展性。它是一个统一的存储系统，既支持传统的块、文件存储协议，例如SAN和NAS，也支持新兴的对象存储协议，如S3和Swift，这使得Ceph理论上可以满足时下一切主流的存储应用的要求。</p></blockquote><p>Ceph项目最早起源于Sage就读博士期间的工作（最早的成果于2004年发表），并随后贡献给开源社区。在经过了数年的发展之后，目前已得到众多云计算厂商的支持并被广泛应用。RedHat及OpenStack都可与Ceph整合以支持虚拟机镜像的后端存储。</p><ul><li>Ceph摒弃了传统的集中式存储元数据的方案，采用CRUSH算法，数据分布均衡，并行度高。</li><li>考虑了容灾区的隔离，能够实现各类负载的副本放置规则，例如跨机房，机架感知。</li><li>能够支持上千个存储节点的规模，支持TB到PB级的数据。</li></ul><p><strong>高可用性</strong></p><ul><li>a. 副本数可以灵活控制。</li><li>b. 支持故障域分隔，数据强一致性。</li><li>c. 多种故障场景自动进行修复自愈。</li><li>d. 没有单点故障，自动管理。</li></ul><p><strong>高可扩展性</strong></p><ul><li>a. 去中心化。</li><li>b. 扩展灵活。</li><li>c. 随着节点增加而线性增长。</li></ul><p><strong>特性丰富</strong></p><p>a. 支持三种存储接口：块存储、文件存储、对象存储。</p><p>b. 支持自定义接口，支持多种语言驱动。</p><p>特点：</p><ul><li><p>高性能</p></li><li><p>高可用性</p></li><li><p>高可扩展性</p></li><li><p>特性丰富</p></li></ul><p><strong>支持三种接口</strong>：</p><ul><li>Object：有原生的API，而且也兼容Swift和S3的API。</li><li>Block：支持精简配置、快照、克隆。</li><li>File：Posix接口，支持快照。</li></ul><p><img src="\img\ceph-st1.jpg" alt="640?wx_fmt=png"></p><ul><li><p>Monitor</p><p><code>ceph-mon</code>，一个Ceph集群需要多个Monitor组成的小集群，它们通过Paxos同步数据，用来保存OSD的元数据。<a href="http://docs.ceph.org.cn/glossary/#term-ceph-monitor"><em>Ceph Monitor</em></a>维护着展示集群状态的各种图表，包括监视器图、 OSD 图、归置组（ PG ）图、和 CRUSH 图。 Ceph 保存着发生在Monitors 、 OSD 和 PG上的每一次状态变更的历史信息（称为 epoch ）。通常至少需要三个监视器才能实现冗余和高可用性。</p></li><li><p>Manager</p><p><a href="https://docs.ceph.com/en/latest/glossary/#term-Ceph-Manager">Ceph Manager</a>守护进程（<code>ceph-mgr</code>）负责跟踪运行时指标和Ceph集群的当前状态，包括存储利用率，当前性能指标和系统负载。Ceph Manager守护进程还托管基于python的模块，以管理和公开Ceph集群信息，包括基于Web的<a href="https://docs.ceph.com/en/latest/mgr/dashboard/#mgr-dashboard">Ceph仪表板</a>和 <a href="https://docs.ceph.com/en/latest/mgr/restful">REST API</a>。通常，至少需要两个管理器才能实现高可用性。</p></li><li><p>OSD</p><p>OSD全称Object Storage Daemon（<code>ceph-osd</code>），也就是负责响应客户端请求返回具体数据的进程。一个Ceph集群一般都有很多个OSD。<a href="http://docs.ceph.org.cn/glossary/#term-56"><em>Ceph OSD 守护进程</em></a>（ Ceph OSD ）的功能是存储数据，处理数据的复制、恢复、回填、再均衡，并通过检查其他OSD 守护进程的心跳来向 Ceph Monitors 提供一些监控信息。当 Ceph 存储集群设定为有2个副本时，至少需要2个 OSD 守护进程，集群才能达到 <code>active+clean</code> 状态（ Ceph 默认有3个副本，但你可以调整副本数）</p></li></ul><ul><li><p>MDS</p><p>MDS全称Ceph Metadata Server（<code>ceph-mds</code>），是CephFS服务依赖的元数据服务。<a href="http://docs.ceph.org.cn/glossary/#term-63"><em>Ceph 元数据服务器</em></a>（ MDS ）为 <a href="http://docs.ceph.org.cn/glossary/#term-45"><em>Ceph 文件系统</em></a>存储元数据（也就是说，Ceph 块设备和 Ceph 对象存储不使用MDS ）。元数据服务器使得 POSIX 文件系统的用户们，可以在不对 Ceph 存储集群造成负担的前提下，执行诸如 <code>ls</code>、<code>find</code> 等基本命令。</p></li></ul><ul><li><p>Object</p><p>Ceph最底层的存储单元是Object对象，每个Object包含元数据和原始数据。</p></li></ul><ul><li><p>PG</p><p>PG全称Placement Groups归置组，是一个逻辑的概念，一个PG包含多个OSD。引入PG这一层其实是为了更好的分配数据和定位数据。</p></li></ul><ul><li><p>RADOS</p><p>RADOS全称Reliable Autonomic Distributed Object Store （可靠自治的分布式对象存储），是Ceph集群的<strong>精华</strong>，用户实现数据分配、Failover等集群操作。具有自愈，自管理能力的智能存储节点构建的高可靠，自治，分布式对象存储系统。</p></li></ul><ul><li><p>Libradio</p><p>Librados是Rados提供库，因为RADOS是协议很难直接访问，因此上层的RBD、RGW和CephFS都是通过librados访问的，目前提供PHP、Ruby、Java、Python、C和C++支持。</p></li></ul><ul><li><p>CRUSH<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="  Weil S A Brandt S A , Miller E L , et al. CRUSH: Controlled, Scalable, Decentralized Placement of Replicated Data[C]// IEEE Sc Conference. ACM, 2006.">[1]</span></a></sup></p><p>CRUSH是Ceph使用的数据分布算法，类似一致性哈希，让数据分配到预期的地方。</p></li></ul><ul><li><p>RBD</p><p>RBD全称RADOS block device，是Ceph对外提供的块设备服务。采用全分布式，可靠的块设备访问接口，同时提供Linux内核态和用户态客户端访问支持，以及QEMU/KVM驱动。</p></li></ul><ul><li><p>RGW</p><p>RGW全称RADOS gateway，基于Bucket的REST网关，是Ceph对外提供的对象存储服务，接口与S3和Swift兼容。</p></li></ul><ul><li><p>CephFS</p><p>CephFS全称Ceph File System，是Ceph对外提供的文件系统服务。与POSIX兼容，同时提供Linux内核态用户端和FUSE访问支持。</p></li></ul><blockquote><p>基于 RADOS 的 Ceph 对象存储集群包括两类守护进程：对象存储守护进程（ OSD ）把存储节点上的数据存储为对象； Ceph 监视器（ MON ）维护集群运行图的主拷贝。一个 Ceph 集群可以包含数千个存储节点，最简系统至少需要一个监视器和两个 OSD 才能做到数据复制。</p></blockquote><h3 id="1-1-Ceph架构"><a href="#1-1-Ceph架构" class="headerlink" title="1.1 Ceph架构"></a>1.1 Ceph架构</h3><p><img src="CEPH基础理论/image-20201231160559413.png" alt="image-20201231160559413"></p><p>系统架构。 客户端通过直接与OSD通信来执行文件I / O。 每个进程可以直接链接到客户端实例，也可以与已安装的文件系统进行交互。</p><h3 id="1-2-ceph读写流程"><a href="#1-2-ceph读写流程" class="headerlink" title="1.2 ceph读写流程"></a>1.2 ceph读写流程</h3><p>Read：</p><ul><li>Client app 发送读请求，RADOS将请求发送给Primary OSD。</li><li>主要OSD在本地磁盘读数据并完成读请求。</li></ul><p>Write:</p><ul><li><p>Client App 写数据，RADOS将数据发送给Primary OSD。</p></li><li><p>Primary OSD识别Replica OSDs并且向他们发送数据，由他们写数据到本地磁盘。</p></li><li><p>Replica OSDs 完成写并通知Primary OSD。</p></li><li><p>Primary OSDs 通知client APP 写完成。</p></li><li><p><img src="\img\ceph-b.png" alt="image-20201122180847737"></p></li></ul><h3 id="1-3-三种存储方式"><a href="#1-3-三种存储方式" class="headerlink" title="1.3 三种存储方式"></a>1.3 三种存储方式</h3><h4 id="1-块设备"><a href="#1-块设备" class="headerlink" title="1. 块设备"></a>1. 块设备</h4><p><strong>典型设备：</strong> 磁盘阵列，硬盘</p><p>主要是将裸磁盘空间映射给主机使用的。</p><p><strong>优点：</strong></p><ul><li>通过RAID与LVM（逻辑卷管理）等手段，对数据提供了保护。</li><li>多块廉价的硬盘组合起来，提高容量。</li><li>多块磁盘组合出来的逻辑盘，提升读写效率。</li></ul><p><strong>缺点：</strong></p><ul><li>采用SAN架构组网时，光纤交换机，造价成本高。</li><li>主机之间无法共享数据。</li></ul><p><strong>使用场景：</strong></p><ul><li>docker容器、虚拟机磁盘存储分配。</li><li>日志存储。</li><li>文件存储。</li><li>…</li></ul><h4 id="2-文件存储"><a href="#2-文件存储" class="headerlink" title="2.文件存储"></a>2.文件存储</h4><p><strong>典型设备：</strong> FTP、NFS服务器<br> 为了克服块存储文件无法共享的问题，所以有了文件存储。<br> 在服务器上架设FTP与NFS服务，就是文件存储。</p><p><strong>优点：</strong></p><ul><li>造价低，随便一台机器就可以了。</li><li>方便文件共享。</li></ul><p><strong>缺点：</strong></p><ul><li>读写速率低。</li><li>传输速率慢。</li></ul><p><strong>使用场景：</strong></p><ul><li>日志存储。</li><li>有目录结构的文件存储。</li><li>…</li></ul><h4 id="3-对象存储"><a href="#3-对象存储" class="headerlink" title="3.对象存储"></a>3.对象存储</h4><p><img src="\img\ceph-c.jpg" alt="img"></p><p><strong>典型设备：</strong> 内置大容量硬盘的分布式服务器(swift, s3)<br> 多台服务器内置大容量硬盘，安装上对象存储管理软件，对外提供读写访问功能。</p><p><strong>优点：</strong></p><ul><li>具备块存储的读写高速。</li><li>具备文件存储的共享等特性。</li></ul><p><strong>使用场景：</strong> (适合更新变动较少的数据)</p><ul><li>图片存储。</li><li>视频存储。</li><li>…</li></ul><h3 id=""><a href="#" class="headerlink" title=" "></a> </h3><hr><h2 id="2-Ceph-I-O流程和数据分布"><a href="#2-Ceph-I-O流程和数据分布" class="headerlink" title="2 Ceph I/O流程和数据分布"></a>2 Ceph I/O流程和数据分布</h2><p><img src="\img\ceph-d.png" alt="img"></p><p><img src="\img\ceph_io2.png" alt="img"></p><p><strong>步骤：</strong></p><ol><li>client 创建cluster handler。</li><li>client 读取配置文件。</li><li>client 连接上monitor，获取集群map信息。</li><li>client 读写io 根据crushmap 算法请求对应的主osd数据节点。</li><li>主osd数据节点同时写入另外两个副本节点数据。</li><li>等待主节点以及另外两个副本节点写完数据状态。</li><li>主节点及副本节点写入状态都成功后，返回给client，io写入完成。</li></ol><h3 id="2-1-新主I-O流程图"><a href="#2-1-新主I-O流程图" class="headerlink" title="2.1 新主I/O流程图"></a>2.1 新主I/O流程图</h3><p><img src="\img\ceph-e.jpg" alt="img"></p><p><strong>步骤：</strong></p><ol><li>client连接monitor获取集群map信息。</li><li>同时新主osd1由于没有pg数据会主动上报monitor告知让osd2临时接替为主。</li><li>临时主osd2会把数据全量同步给新主osd1。</li><li>client IO读写直接连接临时主osd2进行读写。</li><li>osd2收到读写io，同时写入另外两副本节点。</li><li>等待osd2以及另外两副本写入成功。</li><li>osd2三份数据都写入成功返回给client, 此时client io读写完毕。</li><li>如果osd1数据同步完毕，临时主osd2会交出主角色。</li><li>osd1成为主节点，osd2变成副本。</li></ol><h3 id="2-2-Ceph-I-O算法流程"><a href="#2-2-Ceph-I-O算法流程" class="headerlink" title="2.2 Ceph I/O算法流程"></a>2.2 Ceph I/O算法流程</h3><p><img src="\img\ceph-arch.png" alt="img"></p><ol><li>File用户需要读写的文件。File-&gt;Object映射：</li></ol><ul><li>a. ino (File的元数据，File的唯一id)。</li><li>b. ono(File切分产生的某个object的序号，默认以4M切分一个块大小)。</li><li>c. oid(object id: ino + ono)。</li></ul><ol><li>Object是RADOS需要的对象。Ceph指定一个静态hash函数计算oid的值，将oid映射成一个近似均匀分布的伪随机值，然后和mask按位相与，得到pgid。Object-&gt;PG映射：</li></ol><ul><li>a. hash(oid) &amp; mask-&gt; pgid 。</li><li>b. mask = PG总数m(m为2的整数幂)-1 。</li></ul><ol><li>PG(Placement Group),用途是对object的存储进行组织和位置映射, (类似于redis cluster里面的slot的概念) 一个PG里面会有很多object。采用CRUSH算法，将pgid代入其中，然后得到一组OSD。PG-&gt;OSD映射：</li></ol><ul><li>a. CRUSH(pgid)-&gt;(osd1,osd2,osd3) 。</li></ul><pre><code class="hljs ini"><span class="hljs-attr">locator</span> = object_name<span class="hljs-attr">obj_hash</span> =  hash(locator)<span class="hljs-attr">pg</span> = obj_hash % num_pg<span class="hljs-attr">osds_for_pg</span> = crush(pg)  <span class="hljs-comment"># returns a list of osds</span><span class="hljs-attr">primary</span> = osds_for_pg[<span class="hljs-number">0</span>]<span class="hljs-attr">replicas</span> = osds_for_pg[<span class="hljs-number">1</span>:]</code></pre><h3 id="2-3-Ceph-RBD-IO流程"><a href="#2-3-Ceph-RBD-IO流程" class="headerlink" title="2.3 Ceph RBD IO流程"></a>2.3 Ceph RBD IO流程</h3><p><img src="/img/ceph-f.png" alt="img"></p><ol><li><p>客户端创建一个pool，需要为这个pool指定pg的数量。</p></li><li><p>创建pool/image rbd设备进行挂载。</p></li><li><p>用户写入的数据进行切块，每个块的大小默认为4M，并且每个块都有一个名字，名字就是object+序号。</p></li><li><p>将每个object通过pg进行副本位置的分配。</p></li><li><p>pg根据cursh算法会寻找3个osd，把这个object分别保存在这三个osd上。</p></li><li><p>osd上实际是把底层的disk进行了格式化操作，一般部署工具会将它格式化为xfs文件系统。</p></li><li><p>object的存储就变成了存储一个文rbd0.object1.file。</p></li></ol><p><img src="/img/ceph-g.png" alt="img"></p><p><strong>客户端写数据osd过程：</strong></p><ol><li>采用的是librbd的形式，使用librbd创建一个块设备，向这个块设备中写入数据。</li><li>在客户端本地同过调用librados接口，然后经过pool，rbd，object、pg进行层层映射,在PG这一层中，可以知道数据保存在哪3个OSD上，这3个OSD分为主从的关系。</li><li>客户端与primay OSD建立SOCKET 通信，将要写入的数据传给primary OSD，由primary OSD再将数据发送给其他replica OSD数据节点。</li></ol><h3 id="2-4-Ceph-Pool和PG分布情况"><a href="#2-4-Ceph-Pool和PG分布情况" class="headerlink" title="2.4 Ceph Pool和PG分布情况"></a>2.4 Ceph Pool和PG分布情况</h3><p><img src="/img/ceph-h.jpg" alt="img"></p><ul><li><p>pool是ceph存储数据时的逻辑分区，它起到namespace的作用。</p></li><li><p>每个pool包含一定数量(可配置)的PG。</p></li><li><p>PG里的对象被映射到不同的Object上。</p></li><li><p>pool是分布到整个集群的。</p></li><li><p>pool可以做故障隔离域，根据不同的用户场景不一进行隔离。</p></li></ul><h3 id="2-5-Ceph-数据扩容PG分布"><a href="#2-5-Ceph-数据扩容PG分布" class="headerlink" title="2.5 Ceph 数据扩容PG分布"></a>2.5 Ceph 数据扩容PG分布</h3><p><strong>场景数据迁移流程：</strong></p><ul><li>现状3个OSD, 4个PG</li><li>扩容到4个OSD, 4个PG</li></ul><p><strong>扩容前</strong></p><p><img src="/img/ceph-i.jpg" alt="img"></p><p><strong>扩容后</strong></p><p><img src="/img/ceph-j.png" alt="img"></p><p><strong>说明</strong><br>每个OSD上分布很多PG, 并且每个PG会自动散落在不同的OSD上。如果扩容那么相应的PG会进行迁移到新的OSD上，保证PG数量的均衡。</p><p><img src="/img/image-20210105203533111.png" alt="image-20210105203533111"></p><p>上图：在将写入应用于复制对象的所有OSD上的缓冲区高速缓存后，RADOS会以ack响应。 只有在将其安全地提交到磁盘之后，才将最终提交通知发送到客户端。这确保了数据的安全性。</p><p>   主服务器将更新转发到副本，并在将更新应用到所有OSD的内存缓冲区高速缓存后回复确认，从而允许客户端上的同步POSIX调用返回。 当数据安全地提交到磁盘时，将发送一次最终提交（可能在几秒钟后）。 仅在完全复制更新日期之后，我们才会将确认发送给客户端，以无缝地容忍任何单个OSD的故障，即使这样做会增加客户端的延迟。 默认情况下，客户端还会缓冲写入操作，直到它们承诺避免在放置组中所有OSD同时掉电的情况下避免数据丢失为止。 在这种情况下进行恢复时，RADOS允许在接受新的更新之前，以固定的间隔重播先前已确认（因此有序）的更新。</p><p>详细请见<a href="https://durantthorvalds.top/2020/12/15/%E8%BF%81%E7%A7%BB%E4%B9%8B%E7%BE%8EPG%E8%AF%BB%E5%86%99%E6%B5%81%E7%A8%8B%E4%B8%8E%E7%8A%B6%E6%80%81%E8%BF%81%E7%A7%BB%E8%AF%A6%E8%A7%A3/">PG读写及迁移</a></p><hr><h2 id="3-Ceph心跳机制"><a href="#3-Ceph心跳机制" class="headerlink" title="3 Ceph心跳机制"></a>3 Ceph心跳机制</h2><p>心跳是用于节点间检测对方是否故障的，以便及时发现故障节点进入相应的故障处理流程。</p><p><strong>问题：</strong></p><ul><li>故障检测时间和心跳报文带来的负载之间做权衡。</li><li>心跳频率太高则过多的心跳报文会影响系统性能。</li><li>心跳频率过低则会延长发现故障节点的时间，从而影响系统的可用性。</li></ul><p><strong>故障检测策略应该能够做到：</strong></p><ul><li><strong>及时</strong>：节点发生异常如宕机或网络中断时，集群可以在可接受的时间范围内感知。</li><li><strong>适当的压力</strong>：包括对节点的压力，和对网络的压力。</li><li><strong>容忍网络抖动</strong>：网络偶尔延迟。</li><li><strong>扩散机制</strong>：节点存活状态改变导致的元信息变化需要通过某种机制扩展到整个集群。</li></ul><h3 id="心跳检测"><a href="#心跳检测" class="headerlink" title="心跳检测"></a>心跳检测</h3><p><img src="\img\ceph-webp.png" alt="img"></p><p><strong>OSD节点会监听public、cluster、front和back四个端口</strong></p><ul><li><strong>public端口</strong>：监听来自Monitor和Client的连接。</li><li><strong>cluster端口</strong>：监听来自OSD Peer的连接。</li><li><strong>front端口</strong>：供客户端连接集群使用的网卡, 这里临时给集群内部之间进行心跳。</li><li><strong>back端口</strong>：供客集群内部使用的网卡。集群内部之间进行心跳。</li><li><strong>hbclient</strong>：发送ping心跳的messenger。</li></ul><h3 id="Ceph-OSD之间相互心跳检测"><a href="#Ceph-OSD之间相互心跳检测" class="headerlink" title="Ceph OSD之间相互心跳检测"></a>Ceph OSD之间相互心跳检测</h3><p><img src="/img/ceph-k.jpg" alt="img"></p><ul><li>同一个PG内OSD互相心跳，他们互相发送PING/PONG信息。</li><li>每隔6s检测一次(实际会在这个基础上加一个随机时间来避免峰值)。</li><li>20s没有检测到心跳回复，加入failure队列。</li></ul><h3 id="Ceph-OSD与Mon心跳检测"><a href="#Ceph-OSD与Mon心跳检测" class="headerlink" title="Ceph OSD与Mon心跳检测"></a>Ceph OSD与Mon心跳检测</h3><p><img src="/img/ceph-l.jpg" alt="img"></p><p><strong>OSD报告给Monitor：</strong></p><ul><li>OSD有事件发生时（比如故障、PG变更）。</li><li>自身启动5秒内。</li><li>OSD周期性的上报给Monito<ul><li>OSD检查failure_queue中的伙伴OSD失败信息。</li><li>向Monitor发送失效报告，并将失败信息加入failure_pending队列，然后将其从failure_queue移除。</li><li>收到来自failure_queue或者failure_pending中的OSD的心跳时，将其从两个队列中移除，并告知Monitor取消之前的失效报告。</li><li>当发生与Monitor网络重连时，会将failure_pending中的错误报告加回到failure_queue中，并再次发送给Monitor。</li></ul></li></ul><p>Monitor统计下线OSD</p><ul><li>Monitor收集来自OSD的伙伴失效报告。</li><li>当错误报告指向的OSD失效超过一定阈值，且有足够多的OSD报告其失效时，将该OSD下线。</li></ul><h3 id="Ceph心跳检测总结"><a href="#Ceph心跳检测总结" class="headerlink" title="Ceph心跳检测总结"></a>Ceph心跳检测总结</h3><p>Ceph通过伙伴OSD汇报失效节点和Monitor统计来自OSD的心跳两种方式判定OSD节点失效。</p><ul><li><strong>及时</strong>：伙伴OSD可以在秒级发现节点失效并汇报Monitor，并在几分钟内由Monitor将失效OSD下线。</li><li><strong>适当的压力</strong>：由于有伙伴OSD汇报机制，Monitor与OSD之间的心跳统计更像是一种保险措施，因此OSD向Monitor发送心跳的间隔可以长达600秒，Monitor的检测阈值也可以长达900秒。Ceph实际上是将故障检测过程中中心节点的压力分散到所有的OSD上，以此提高中心节点Monitor的可靠性，进而提高整个集群的可扩展性。</li><li><strong>容忍网络抖动</strong>：Monitor收到OSD对其伙伴OSD的汇报后，并没有马上将目标OSD下线，而是周期性的等待几个条件：<ul><li>目标OSD的失效时间大于通过固定量osd_heartbeat_grace和历史网络条件动态确定的阈值。</li><li>来自不同主机的汇报达到mon_osd_min_down_reporters。</li><li>满足前两个条件前失效汇报没有被源OSD取消。</li></ul></li><li><strong>扩散</strong>：作为中心节点的Monitor并没有在更新OSDMap后尝试广播通知所有的OSD和Client，而是惰性的等待OSD和Client来获取。以此来减少Monitor压力并简化交互逻辑。</li></ul><h2 id="4-Ceph通信框架"><a href="#4-Ceph通信框架" class="headerlink" title="4 Ceph通信框架"></a>4 Ceph通信框架</h2><p><strong>Simple线程模式</strong></p><ul><li><strong>特点</strong>：每一个网络链接，都会创建两个线程，一个用于接收，一个用于发送。</li><li><strong>缺点</strong>：大量的链接会产生大量的线程，会消耗CPU资源，影响性能。</li></ul><p><strong>Async事件的I/O多路复用模式</strong></p><ul><li><strong>特点</strong>：这种是目前网络通信中广泛采用的方式。k版默认已经使用Asnyc了。</li></ul><p><strong>XIO方式使用了开源的网络通信库accelio来实现</strong></p><ul><li><strong>特点</strong>：这种方式需要依赖第三方的库accelio稳定性，目前处于试验阶段。</li></ul><h3 id="Ceph通信框架设计模式"><a href="#Ceph通信框架设计模式" class="headerlink" title="Ceph通信框架设计模式"></a>Ceph通信框架设计模式</h3><p><strong>设计模式(Subscribe/Publish)</strong></p><p>订阅发布模式又名观察者模式，它意图是“定义对象间的一种一对多的依赖关系，<br> 当一个对象的状态发生改变时，所有依赖于它的对象都得到通知并被自动更新”。</p><p><img src="/img/ceph-m.jpg" alt="img"></p><p>Accepter监听peer的请求, 调用 SimpleMessenger::add_accept_pipe() 创建新的 Pipe 到 SimpleMessenger::pipes 来处理该请求。</p><p>Pipe用于消息的读取和发送。该类主要有两个组件，Pipe::Reader，Pipe::Writer用来处理消息读取和发送。</p><p>Messenger作为消息的发布者, 各个 Dispatcher 子类作为消息的订阅者, Messenger 收到消息之后，  通过 Pipe 读取消息，然后转给 Dispatcher 处理。</p><p>Dispatcher调度员是订阅者的基类，具体的订阅后端继承该类,初始化的时候通过 Messenger::add_dispatcher_tail/head 注册到 Messenger::dispatchers. 收到消息后，通知该类处理。</p><p>DispatchQueue该类用来缓存收到的消息, 然后唤醒 DispatchQueue::dispatch_thread 线程找到后端的 Dispatch 处理消息。</p><p><img src="/img/ceph-u.png" alt=""></p><h3 id="通信类框架图"><a href="#通信类框架图" class="headerlink" title="通信类框架图"></a>通信类框架图</h3><p><img src="/img/ceph-n.jpg" alt="img"></p><h3 id="通信数据格式"><a href="#通信数据格式" class="headerlink" title="通信数据格式"></a>通信数据格式</h3><p>通信协议格式需要双方约定数据格式。</p><p><strong>消息的内容主要分为三部分：</strong></p><ul><li>header //消息头类型消息的信封</li><li>user data //需要发送的实际数据<ul><li>payload     //操作保存元数据</li><li>middle      //预留字段</li><li>data       //读写数据</li></ul></li><li>footer       //消息的结束标记</li></ul><pre><code class="hljs cpp"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Message</span> :</span> <span class="hljs-keyword">public</span> RefCountedObject &#123;<span class="hljs-keyword">protected</span>:  ceph_msg_header  header;      <span class="hljs-comment">// 消息头</span>  ceph_msg_footer  footer;      <span class="hljs-comment">// 消息尾</span>  bufferlist       payload;  <span class="hljs-comment">// &quot;front&quot; unaligned blob</span>  bufferlist       middle;   <span class="hljs-comment">// &quot;middle&quot; unaligned blob</span>  bufferlist       data;     <span class="hljs-comment">// data payload (page-alignment will be preserved where possible)</span>  <span class="hljs-comment">/* recv_stamp is set when the Messenger starts reading the</span><span class="hljs-comment">   * Message off the wire */</span>  <span class="hljs-keyword">utime_t</span> recv_stamp;       <span class="hljs-comment">//开始接收数据的时间戳</span>  <span class="hljs-comment">/* dispatch_stamp is set when the Messenger starts calling dispatch() on</span><span class="hljs-comment">   * its endpoints */</span>  <span class="hljs-keyword">utime_t</span> dispatch_stamp;   <span class="hljs-comment">//dispatch 的时间戳</span>  <span class="hljs-comment">/* throttle_stamp is the point at which we got throttle */</span>  <span class="hljs-keyword">utime_t</span> throttle_stamp;   <span class="hljs-comment">//获取throttle 的slot的时间戳</span>  <span class="hljs-comment">/* time at which message was fully read */</span>  <span class="hljs-keyword">utime_t</span> recv_complete_stamp;  <span class="hljs-comment">//接收完成的时间戳</span>  ConnectionRef connection;     <span class="hljs-comment">//网络连接</span>  <span class="hljs-keyword">uint32_t</span> magic = <span class="hljs-number">0</span>;           <span class="hljs-comment">//消息的魔术字</span>  bi::list_member_hook&lt;&gt; dispatch_q;    <span class="hljs-comment">//boost::intrusive 成员字段</span>&#125;;<span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">ceph_msg_header</span> &#123;</span>    __le64 seq;       <span class="hljs-comment">// 当前session内 消息的唯一 序号</span>    __le64 tid;       <span class="hljs-comment">// 消息的全局唯一的 id</span>    __le16 type;      <span class="hljs-comment">// 消息类型</span>    __le16 priority;  <span class="hljs-comment">// 优先级</span>    __le16 version;   <span class="hljs-comment">// 版本号</span>    __le32 front_len; <span class="hljs-comment">// payload 的长度</span>    __le32 middle_len;<span class="hljs-comment">// middle 的长度</span>    __le32 data_len;  <span class="hljs-comment">// data 的 长度</span>    __le16 data_off;  <span class="hljs-comment">// 对象的数据偏移量</span>    <span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">ceph_entity_name</span> <span class="hljs-title">src</span>;</span> <span class="hljs-comment">//消息源</span>    <span class="hljs-comment">/* oldest code we think can decode this.  unknown if zero. */</span>    __le16 compat_version;    __le16 reserved;    __le32 crc;       <span class="hljs-comment">/* header crc32c */</span>&#125; __attribute__ ((packed));<span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">ceph_msg_footer</span> &#123;</span>    __le32 front_crc, middle_crc, data_crc; <span class="hljs-comment">//crc校验码</span>    __le64  sig; <span class="hljs-comment">//消息的64位signature</span>    __u8 flags; <span class="hljs-comment">//结束标志</span>&#125; __attribute__ ((packed));</code></pre><hr><h2 id="5-Ceph-CRUSH算法"><a href="#5-Ceph-CRUSH算法" class="headerlink" title="5 Ceph CRUSH算法"></a>5 Ceph CRUSH算法</h2><blockquote><p>Controlled Replication Under Scalable Hashing, 可扩展哈希下的可控复制。以数据唯一标识符、当前存储集群的拓扑结构以及数据备份策略作为CRUSH输入，可以随时随地的通过计算获取数控所在的底层存储设备的位置并直接与其通信，从而避免查表操作，实现去中心化和高度并发。</p><p>CRUSH是一种伪随机算法，采用<strong>一致性哈希</strong>。</p><p>OSD MAP: 包含当前所有pool的状态，和所有OSD状态。</p><p>CRUSH MAP: 包含当前磁盘、服务器、机架的层次结构。</p><p>CRUSH Rules：数据映射的策略。以便灵活放置Object。</p></blockquote><h3 id="数据分布算法挑战"><a href="#数据分布算法挑战" class="headerlink" title="数据分布算法挑战"></a>数据分布算法挑战</h3><p><strong>数据分布和负载均衡</strong>：</p><ul><li>a. 数据分布均衡，使数据能均匀的分布到各个节点上。</li><li>b. 负载均衡，使数据访问读写操作的负载在各个节点和磁盘的负载均衡。</li></ul><p><strong>灵活应对集群伸缩</strong>：</p><ul><li>a. 系统可以方便的增加或者删除节点设备，并且对节点失效进行处理。</li><li>b. 增加或者删除节点设备后，能自动实现数据的均衡，并且尽可能少的迁移数据。</li></ul><p><strong>支持大规模集群</strong>：</p><ul><li>a. 要求数据分布算法维护的元数据相对较小，并且计算量不能太大。随着集群规模的增 加，数据分布算法开销相对比较小。</li></ul><h3 id="Ceph-CRUSH算法原理"><a href="#Ceph-CRUSH算法原理" class="headerlink" title="Ceph CRUSH算法原理"></a>Ceph CRUSH算法原理</h3><p><strong>CRUSH算法因子：</strong></p><ul><li>层次化的Cluster Map<br> 实际应用中设备具有形如“数据中心 → 机架→主机→磁盘”这样的树状层级，所以Cluster Map采用树来实现，每个叶子节点都是真实的最小物理存储设备，称为devices，而所有中间节点称为root，是整个集群的入口。每个节点都拥有唯一的数字ID和类型，但是只有叶子节点才拥有非负ID，表明它们是终端设备。</li></ul><blockquote><p>​                                                    下表展示了Cluster Map一些常见节点的层级</p></blockquote><ul><li>| 类型ID |  类型名称  |<br> | :——: | :————: |<br>|   0    |    osd     |<br> |   1    |    host    |<br> |   2    |  chassis   |<br> |   3    |    rack    |<br> |   4    |    row     |<br> |   5    |    pdu     |<br> |   6    |    pod     |<br> |   7    |    room    |<br> |   8    | datacenter |<br> |   9    |   region   |<br> |   10   |    root    |</li></ul><p><img src="/img/ceph-o.png" alt=" "></p><ul><li>CRUSH Map是一个树形结构，OSDMap更多记录的是OSDMap的属性(epoch/fsid/pool信息以及osd的ip等等)。</li></ul><p>叶子节点是device（也就是osd），其他的节点称为bucket节点，这些bucket都是虚构的节点，可以根据物理结构进行抽象，当然树形结构只有一个最终的根节点称之为root节点，中间虚拟的bucket节点可以是数据中心抽象、机房抽象、机架抽象、主机抽象等。</p><h3 id="数据分布策略Placement-Rules"><a href="#数据分布策略Placement-Rules" class="headerlink" title="数据分布策略Placement Rules"></a>数据分布策略Placement Rules</h3><p>在完成了使用clustermap建立对应的集群的拓扑结构描述后，可以定义placement rule 来完成<strong>数据映射</strong>.</p><p>这些操作有三种类型：</p><ul><li><p>take</p><p>take从cluster map选择指定编号的bucket ，并以此作为后续步骤的输入。例如系统默认以root节点作为输入。</p></li><li><p>select*</p><p>select从输入的bucket中随机选择指定类型和数量的条目。Ceph支持两种类型的备份策略，多副本和<strong>纠删码</strong>，对应两种算法，firstn和<strong>indep</strong>。以上两种算法都是dfs，无明显区别，唯一区别是纠删码是要求结果是有序的，i.e.总是返回指定长度的结果，如果对应条目不存在，采用空穴进行填充。</p><p>select操作也支持容灾模式，例如设置为rack，select保证所有选出的副本位于不同的机架上，也可以设置为host，即所有选出的副本位于不同的主机的磁盘上。</p></li><li><p>emit</p><p>输出最终的选择结果给上级调用并返回。</p></li></ul><p><strong>数据分布策略Placement Rules主要有特点：</strong></p><ul><li>a. 从CRUSH Map中的哪个节点开始查找</li><li>b. 使用那个节点作为故障隔离域</li><li>c. 定位副本的搜索模式（广度优先 or 深度优先）</li></ul><pre><code class="hljs bash">rule replicated_ruleset  <span class="hljs-comment">#规则集的命名，创建pool时可以指定rule集</span>&#123;    ruleset 0                <span class="hljs-comment">#rules集的编号，顺序编即可   </span>    <span class="hljs-built_in">type</span> replicated          <span class="hljs-comment">#定义pool类型为replicated(还有erasure模式)   </span>    min_size 1                <span class="hljs-comment">#pool中最小指定的副本数量不能小1</span>    max_size 10               <span class="hljs-comment">#pool中最大指定的副本数量不能大于10       </span>    step take default         <span class="hljs-comment">#查找bucket入口点，一般是root类型的bucket    </span>    step chooseleaf  firstn  0  <span class="hljs-built_in">type</span>  host <span class="hljs-comment">#选择一个host,并递归选择叶子节点osd     </span>    step emit        <span class="hljs-comment">#结束</span>&#125;</code></pre><h3 id="CRUSH算法案例"><a href="#CRUSH算法案例" class="headerlink" title="CRUSH算法案例"></a>CRUSH算法案例</h3><p>集群中有部分sas和ssd磁盘，现在有个业务线性能及可用性优先级高于其他业务线，能否让这个高优业务线的数据都存放在ssd磁盘上。</p><p><strong>普通用户：</strong></p><p><img src="/img/ceph-q.jpg" alt="img"></p><p><strong>高优用户</strong></p><p><img src="/img/ceph-r.jpg" alt="img"></p><p>配置规则</p><p>作者：<img src="/img/ceph-t.jpg" alt="img"></p><p>限于篇幅，我们对CRUSH的介绍十分简略，更详细的请看<a href="https://durantthorvalds.top/2020/11/27/A%20first%20glance%20at%20CRUSH/">A First Galance At Crush</a>一文.</p><hr><h2 id="6-定制化Ceph-RBD-QOS"><a href="#6-定制化Ceph-RBD-QOS" class="headerlink" title="6 定制化Ceph RBD QOS"></a>6 定制化Ceph RBD QOS</h2><p>QoS （Quality of Service，服务质量）起源于网络技术，它用来解决网络延迟和阻塞等问题，能够为指定的网络通信提供更好的服务能力。</p><p>我们总的Ceph集群的iIO能力是有限的，比如带宽，IOPS。如何避免用户争取资源，如果保证集群所有用户资源的高可用性，以及如何保证高优用户资源的可用性。所以我们需要把有限的IO能力合理分配。</p><h3 id="Ceph-IO操作类型"><a href="#Ceph-IO操作类型" class="headerlink" title="Ceph IO操作类型"></a>Ceph IO操作类型</h3><ul><li><p><strong>ClientOp</strong>：来自客户端的读写I/O请求。</p></li><li><p><strong>SubOp</strong>：osd之间的I/O请求。主要包括由客户端I/O产生的副本间数据读写请求，以及由数据同步、数据扫描、负载均衡等引起的I/O请求。</p></li><li><p><strong>SnapTrim</strong>：快照数据删除。从客户端发送快照删除命令后，删除相关元数据便直接返回，之后由后台线程删除真实的快照数据。通过控制snaptrim的速率间接控制删除速率。</p></li><li><p><strong>Scrub</strong>：用于发现对象的静默数据错误，扫描元数据的Scrub和对象整体扫描的deep Scrub。</p></li><li><p><strong>Recovery</strong>：数据恢复和迁移。集群扩/缩容、osd失效/从新加入等过程。</p></li></ul><p>详细请见<a href="https://durantthorvalds.top/2020/12/28/%E6%8E%A7%E5%88%B6%E5%85%88%E8%A1%8C-Ceph%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0QoS/">Ceph QoS策略</a></p><p>参考资料</p><p><a href="https://www.jianshu.com/p/cc3ece850433">https://www.jianshu.com/p/cc3ece850433</a></p><hr><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>Weil S A Brandt S A , Miller E L , et al. CRUSH: Controlled, Scalable, Decentralized Placement of Replicated Data[C]// IEEE Sc Conference. ACM, 2006.<a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>Weil S A, Brandt S A, Miller E L, et al. Ceph: A scalable, high-performance distributed file system[C]//Proceedings of the 7th symposium on Operating systems design and implementation. 2006: 307-320.<a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;CEPH基础理论学习&quot;&gt;&lt;a href=&quot;#CEPH基础理论学习&quot; class=&quot;headerlink&quot; title=&quot;CEPH基础理论学习&quot;&gt;&lt;/a&gt;CEPH基础理论学习&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;参考&lt;sup id=&quot;fnref:2&quot; class</summary>
      
    
    
    
    <category term="ceph" scheme="http://durantthorvalds.top/categories/ceph/"/>
    
    <category term="理论" scheme="http://durantthorvalds.top/categories/ceph/%E7%90%86%E8%AE%BA/"/>
    
    
    <category term="ceph" scheme="http://durantthorvalds.top/tags/ceph/"/>
    
  </entry>
  
</feed>
